[
{"url": "https://en.wikipedia.org/wiki/Web_crawler", "title": "Web crawler - Wikipedia", "content": "A Web crawler, sometimes called a spider or spiderbot and often shortened to crawler, is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing (web spidering).[1]\n Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently.\n Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all.\n The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly.\n Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming.\n A web crawler is also known as a spider,[2] an ant, an automatic indexer,[3] or (in the FOAF software context) a Web scutter.[4]\n A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds. As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier. URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'.[5]\n The archive is known as the repository and is designed to store and manage the collection of web pages. The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler.[citation needed]\n The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted.\n The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content. Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content.\n As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\"[6] A crawler must carefully choose at each step which pages to visit next.\n The behavior of a Web crawler is the outcome of a combination of policies:[7]\n Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40–70% of the indexable Web;[8] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999.[9] As a crawler always downloads just a fraction of the Web pages, it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web.\n This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain, or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling.\n Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies.[10] The ordering metrics tested were breadth-first, backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling.[11]\n Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering.[12] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\"\n Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation).[13] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web.\n Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations.[14][15]\n Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies.[16] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one.\n Daneshpajouh et al. designed a community based algorithm for discovering good seeds.[17] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective.\n A crawler may only want to seek out HTML pages and avoid all other MIME types. In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped.\n Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs.\n Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization, also called URL canonicalization, refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component.[18]\n Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl.[19] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling.\n The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers. The concepts of topical and focused crawling were first introduced by Filippo Menczer[20][21] and by Soumen Chakrabarti et al.[22]\n The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton[23] in the first web crawler of the early days of the Web. Diligenti et al.[24] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points.\n An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot, which is the crawler of CiteSeerX search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix, must be customized to filter out other MIME types, or a middleware is used to extract these documents out and import them to the focused crawl database and repository.[25] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers.[26] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads.\n Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes.[27] In addition, ontologies can be automatically updated in the crawling process. Dong et al.[28] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages.\n The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions.\n From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age.[29]\n Freshness: This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as:\n Age: This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as:\n Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler.[30]\n The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are.\n Two simple re-visiting policies were studied by Cho and Garcia-Molina:[31]\n In both cases, the repeated crawling order of pages can be done either in a random or a fixed order.\n Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them.\n To improve freshness, the crawler should penalize the elements that change too often.[32] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\".[30] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes,[32] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution.[33] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy.\n Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers.\n As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community.[34] The costs of using Web crawlers include:\n A partial solution to these problems is the robots exclusion protocol, also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers.[35] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google, Ask Jeeves, MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests.\n The first proposed interval between successive pageloads was 60 seconds.[36] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used.\n Cho uses 10 seconds as an interval for accesses,[31] and the WIRE crawler uses 15 seconds as the default.[37] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10t seconds before downloading the next page.[38] Dill et al. use 1 second.[39]\n For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl.[40]\n Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3–4 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\"[41]\n A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes.\n A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture.\n Shkapenyuk and Suel noted that:[42]\n While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \"search engine spamming\", which prevent major search engines from publishing their ranking algorithms.\n While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines, web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software.\n Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.).\n Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler.\n Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine.\n A vast amount of web pages lie in the deep or invisible web.[43] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai[44] are intended to allow discovery of these deep-Web resources.\n Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a href=\"URL\"> form. In some cases, such as the Googlebot, Web crawling is done on all text contained inside the hypertext content, tags, or text.\n Strategic approaches may be taken to target deep Web content. With a technique called screen scraping, specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers.[45]\n Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index.[46]\n There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data.\n The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs[47]), there is continued growth and investment in this area by investors and end-users.[citation needed]\n The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features:\n The following web crawlers are available, for a price::"},
{"url": "https://en.wikipedia.org/wiki/Unintended_consequences", "title": "Unintended consequences - Wikipedia", "content": "In the social sciences, unintended consequences (sometimes unanticipated consequences or unforeseen consequences, more colloquially called knock-on effects) are outcomes of a purposeful action that are not intended or foreseen. The term was popularized in the 20th century by American sociologist Robert K. Merton.[1]\n Unintended consequences can be grouped into three types:\n The idea of unintended consequences dates back at least to John Locke who discussed the unintended consequences of interest rate regulation in his letter to Sir John Somers, Member of Parliament.[2]\n The idea was also discussed by Adam Smith, the Scottish Enlightenment, and consequentialism (judging by results).[3]\n The invisible hand theorem is an example of the unintended consequences of agents acting in their self-interest. As Andrew S. Skinner puts it:\n \"The individual undertaker (entrepreneur), seeking the most efficient allocation of resources, contributes to overall economic efficiency; the merchant's reaction to price signals helps to ensure that the allocation of resources accurately reflects the structure of consumer preferences; and the drive to better our condition contributes to economic growth.\"[4]\n Influenced by 19th century positivism[5] and Charles Darwin's evolution, for both Friedrich Engels and Karl Marx, the idea of uncertainty and chance in social dynamics (and thus unintended consequences beyond results of perfectly defined laws) was only apparent, (if not rejected) since social actions were directed and produced by deliberate human intention.[6][7]\n While discerning between the forces that generate changes in nature and those that generate changes in history in his discussion of Ludwig Feuerbach, Friedrich Engels touched on the idea of (apparent) unintended consequences:\n In nature [...] there are only blind, unconscious agencies acting upon one another, [...] In the history of society, on the contrary, the actors are all endowed with consciousness, are men acting with deliberation or passion, working towards definite goals; nothing happens without a conscious purpose, without an intended aim. [...] For here, also, on the whole, in spite of the consciously desired aims of all individuals, accident apparently reigns on the surface. That which is willed happens but rarely; in the majority of instances the numerous desired ends cross and conflict with one another, or these ends themselves are from the outset incapable of realization, or the means of attaining them are insufficient. Thus the conflicts of innumerable individual wills and individual actions in the domain of history produce a state of affairs entirely analogous to [...] the realm of unconscious nature. The ends of the actions are intended, but the results which actually follow from these actions are not intended; or when they do seem to correspond to the end intended, they ultimately have consequences quite other than those intended. Historical events thus appear on the whole to be likewise governed by chance. But where on the surface accident holds sway, there actually it is always governed by inner, hidden laws, and it is only a matter of discovering these laws. For his part, for Karl Marx what can be understood as unintended consequences are actually consequences that should be expected but are obtained unconsciously. These consequences (that no one consciously sought) would be (in the same way as it is for Engels[9][10]) product of conflicts that confront actions from countless individuals. The deviation between the original intended goal and the product derived from conflicts would be a marxist equivalent to «unintended consequences.»[11]\n This social conflicts would happen as a result of a competitive society, and also lead society to sabotage itself and prevent historical progress.[12] Thus, historical progress (in Marxist terms) should eliminate these conflicts and make unintended consequences predictable.[13]\n Unintended consequences are a common topic of study and commentary for the Austrian school of economics given its emphasis on methodological individualism. This is to such an extent that unexpected consequences can be considered as a distinctive part of Austrian tenets.[14]\n In \"Principles of Economics\", Austrian school founder Carl Menger (1840 - 1921) noted that the relationships that occur in the economy are so intricate that a change in the condition of a single good can have ramifications beyond that good. Menger wrote:\n If it is established that the existence of human needs capable of satisfaction is a prerequisite of goods-character [...] This principle is valid whether the goods can be placed in direct causal connection with the satisfaction of human needs, or derive their goods-character from a more or less indirect causal connection with the satisfaction of human needs. [...]Thus quinine would cease to be a good if the diseases it serves to cure should disappear, since the only need with the satisfaction of which it is causally connected would no longer exist. But the disappearance of the usefulness of quinine would have the further consequence that a large part of the corresponding goods of higher order would also be deprived of their goods-character. The inhabitants of quinine-producing countries, who currently earn their livings by cutting and peeling cinchona trees, would suddenly find that not only their stocks of cinchona bark, but also, in consequence, their cinchona trees, the tools and appliances applicable only to the production of quinine, and above all the specialized labor services, by means of which they previously earned their livings, would at once lose their goods-character, since all these things would, under the changed circumstances, no longer have any causal relationship with the satisfaction of human needs. Economist and philosopher Friedrich Hayek (1899 – 1992) is another key figure in the Austrian School of Economics who is notable for his comments on unintended consequences.[16]\n In \"The Use of Knowledge in Society\" (1945) Hayek argues that a centrally planned economy cannot reach the level of efficiency of the free market economy because the necessary (and pertinent) information for decision-making is not concentrated but dispersed among a vast number of agents.[17] Then, for Hayek, the price system in the free market allows the members of a society to anonymously coordinate for the most efficient use of resources, for example, in a situation of scarcity of a raw material, the price increase would coordinate the actions of an uncountable amount of individuals \"in the right direction\".[18]\n The development of this system of interactions would allow the progress of society,[19] and individuals would carry it out without knowing all its implications, given the dispersion (or lack of concentration) of information.[20]\n The implication of this is that the social order (which derives from social progress, which in turn derives from the economy),[21] would be result of a spontaneous cooperation and also an unintended consequence,[10] being born from a process of which no individual or group had all the information available or could know all possible outcomes.\n In the Austrian school, this process of social adjustment that generates a social order in an unintendedly way is known as catallactics.[22]\n For Hayek and the Austrian School, the number of individuals involved in the process of creating a social order defines the type of unintended consequence:[23]\n Sociologist Robert K. Merton popularised this concept in the twentieth century.[1][24][25][26]\n In \"The Unanticipated Consequences of Purposive Social Action\" (1936), Merton tried to apply a systematic analysis to the problem of unintended consequences of deliberate acts intended to cause social change. He emphasized that his term purposive action, \"[was exclusively] concerned with 'conduct' as distinct from 'behavior.' That is, with action that involves motives and consequently a choice between various alternatives\".[26] Merton's usage included deviations from what Max Weber defined as rational social action: instrumentally rational and value rational.[27] Merton also stated that \"no blanket statement categorically affirming or denying the practical feasibility of all social planning is warranted.\"[26]\n More recently, the law of unintended consequences has come to be used as an adage or idiomatic warning that an intervention in a complex system tends to create unanticipated and often undesirable outcomes.[28][29][30][31]\n Akin to Murphy's law, it is commonly used as a wry or humorous warning against the hubristic belief that humans can fully control the world around them, not to presuppose a belief in predestination or a lack or a disbelief in that of free will.\n Possible causes of unintended consequences include the world's inherent complexity (parts of a system responding to changes in the environment), perverse incentives, human stupidity, self-deception, failure to account for human nature, or other cognitive or emotional biases. As a sub-component of complexity (in the scientific sense), the chaotic nature of the universe—and especially its quality of having small, apparently insignificant changes with far-reaching effects (e.g., the butterfly effect)—applies.\n In 1936, Robert K. Merton listed five possible causes of unanticipated consequences:[32]\n In addition to Merton's causes, psychologist Stuart Vyse has noted that groupthink, described by Irving Janis, has been blamed for some decisions that result in unintended consequences.[33]\n The creation of \"no-man's lands\" during the Cold War, in places such as the border between Eastern and Western Europe, and the Korean Demilitarized Zone, has led to large natural habitats.[34][35][36]\n The sinking of ships in shallow waters during wartime has created many artificial coral reefs, which can be scientifically valuable and have become an attraction for recreational divers. This led to the deliberate sinking of retired ships for the purpose of replacing coral reefs lost to global warming and other factors.[37][38][39][40][41]\n In medicine, most drugs have unintended consequences ('side effects') associated with their use. However, some are beneficial. For instance, aspirin, a pain reliever, is also an anticoagulant that can help prevent heart attacks and reduce the severity and damage from thrombotic strokes.[42] The existence of beneficial side effects also leads to off-label use—prescription or use of a drug for an unlicensed purpose. Famously, the drug Viagra was developed to lower blood pressure, with its use for treating erectile dysfunction being discovered as a side effect in clinical trials.\n The implementation of a profanity filter by AOL in 1996 had the unintended consequence of blocking residents of Scunthorpe, North Lincolnshire, England, from creating accounts because of a false positive.[43] The accidental censorship of innocent language, known as the Scunthorpe problem, has been repeated and widely documented.[44][45][46]\n In 1990, the Australian state of Victoria made safety helmets mandatory for all bicycle riders. While there was a reduction in the number of head injuries, there was also an unintended reduction in the number of juvenile cyclists—fewer cyclists obviously leads to fewer injuries, all else being equal. The risk of death and serious injury per cyclist seems to have increased, possibly because of risk compensation,[47] or due to invisibilisation of cyclists. (the more a transportation method is uncommonly sighted, the likelier it could be deemed to be accident prone)\n Research by Vulcan, et al. found that the reduction in juvenile cyclists was because the youths considered wearing a bicycle helmet unfashionable.[48] A health-benefit model developed at Macquarie University in Sydney suggests that, while helmet use reduces \"the risk of head or brain injury by approximately two-thirds or more\", the decrease in exercise caused by reduced cycling as a result of helmet laws is counterproductive in terms of net health.[49]\n Prohibition in the 1920s United States, originally enacted to suppress the alcohol trade, drove many small-time alcohol suppliers out of business and consolidated the hold of large-scale organized crime over the illegal alcohol industry. Since alcohol was still popular, criminal organisations producing alcohol were well-funded and hence also increased their other activities. Similarly, the War on Drugs, intended to suppress the illegal drug trade, instead increased the power and profitability of drug cartels who became the primary source of the products.[50][51][52][53]\n In CIA jargon, \"blowback\" describes the unintended, undesirable consequences of covert operations, such as the funding of the Afghan Mujahideen and the destabilization of Afghanistan contributing to the rise of the Taliban and Al-Qaeda.[54][55][56]\n The introduction of exotic animals and plants for food, for decorative purposes, or to control unwanted species often leads to more harm than good done by the introduced species.\n The protection of the steel industry in the United States reduced production of steel in the United States, increased costs to users, and increased unemployment in associated industries.[62][63]\n In 2003, Barbra Streisand unsuccessfully sued Kenneth Adelman and Pictopia.com for posting a photograph of her home online.[64] Before the lawsuit had been filed, only 6 people had downloaded the file, two of them Streisand's attorneys.[65] The lawsuit drew attention to the image, resulting in 420,000 people visiting the site.[66] The Streisand Effect was named after this incident, describing when an attempt to censor or remove a certain piece of information instead draws attention to the material being suppressed, resulting in the material instead becoming widely known, reported on, and distributed.[67]\n Passenger-side airbags in motorcars were intended as a safety feature, but led to an increase in child fatalities in the mid-1990s because small children were being hit by airbags that deployed automatically during collisions. The supposed solution to this problem, moving the child seat to the back of the vehicle, led to an increase in the number of children forgotten in unattended vehicles, some of whom died under extreme temperature conditions.[68]\n Risk compensation, or the Peltzman effect, occurs after implementation of safety measures intended to reduce injury or death (e.g. bike helmets, seatbelts, etc.). People may feel safer than they really are and take additional risks which they would not have taken without the safety measures in place. This may result in no change, or even an increase, in morbidity or mortality, rather than a decrease as intended.\n According to an anecdote, the British government, concerned about the number of venomous cobra snakes in Delhi, offered a bounty for every dead cobra. This was a successful strategy as large numbers of snakes were killed for the reward. Eventually, enterprising people began breeding cobras for the income. When the government became aware of this, they scrapped the reward program, causing the cobra breeders to set the now-worthless snakes free. As a result, the wild cobra population further increased. The apparent solution for the problem made the situation even worse, becoming known as the Cobra effect.\n Theobald Mathew's temperance campaign in 19th-century Ireland resulted in thousands of people vowing never to drink alcohol again. This led to the consumption of diethyl ether, a much more dangerous intoxicant—owing to its high flammability—by those seeking to become intoxicated without breaking the letter of their pledge.[dubious – discuss][69][70]\n It was thought that adding south-facing conservatories to British houses would reduce energy consumption by providing extra insulation and warmth from the sun. However, people tended to use the conservatories as living areas, installing heating and ultimately increasing overall energy consumption.[71]\n A reward for lost nets found along the Normandy coast was offered by the French government between 1980 and 1981. This resulted in people vandalizing nets to collect the reward.[72]\n Beginning in the 1940s and continuing into the 1960s, the Canadian federal government gave Quebec $2.75 per day per psychiatric patient for their cost of care, but only $1.25 a day per orphan. The perverse result is that the orphan children were diagnosed as mentally ill so Quebec could receive the larger amount of money. This psychiatric misdiagnosis affected up to 20,000 people, and the children are known as the Duplessis Orphans in reference to the Premier of Quebec who oversaw the scheme, Maurice Duplessis.[73][74][75][76]\n There have been attempts to curb the consumption of sugary beverages by imposing a tax on them. However, a study found that the reduced consumption was only temporary. Also, there was an increase in the consumption of beer among households.[77]\n The New Jersey Childproof Handgun Law, which was intended to protect children from accidental discharge of firearms by forcing all future firearms sold in New Jersey to contain \"smart\" safety features, has delayed, if not stopped entirely, the introduction of such firearms to New Jersey markets. The wording of the law caused significant public backlash,[78] fuelled by gun rights lobbyists,[79][80] and several shop owners offering such guns received death threats and stopped stocking them.[81][82] In 2014, 12 years after the law was passed, it was suggested the law be repealed if gun rights lobbyists agree not to resist the introduction of \"smart\" firearms.[83]\n Drug prohibition can lead drug traffickers to prefer stronger, more dangerous substances, that can be more easily smuggled and distributed than other, less concentrated substances.[84]\n Televised drug prevention advertisements may lead to increased drug use.[85]\n Increasing usage of search engines, also including recent image search features, has contributed in the ease of which media is consumed. Some abnormalities in usage may have shifted preferences for pornographic film actors, as the producers began using common search queries or tags to label the actors in new roles.[86]\n The passage of the Stop Enabling Sex Traffickers Act has led to a reported increase in risky behaviors by sex workers as a result of quashing their ability to seek and screen clients online, forcing them back onto the streets or into the dark web. The ads posted were previously an avenue for advocates to reach out to those wanting to escape the trade.[87]\n The use of precision guided munitions meant to reduce the rate of civilian casualties encouraged armies to narrow their safety margins, and increase the use of deadly force in densely populated areas. This in turn increased the danger to uninvolved civilians, who in the past would have been out of the line of fire because of armies' aversion of using higher-risk weaponry in densely populated areas.[88] The perceived ability to operate precision weaponry from afar (where in the past heavy munitions or troop deployment would have been needed) also led to the expansion of the list of potential targets.[88] As put by Michael Walzer: \"Drones not only make it possible for us to get at our enemies, they may also lead us to broaden the list of enemies, to include presumptively hostile individuals and militant organizations simply because we can get at them–even if they aren't actually involved in attacks against us.\"[89] This idea is also echoed by Grégoire Chamayou: \"In a situation of moral hazard, military action is very likely to be deemed 'necessary' simply because it is possible, and possible at a lower cost.\"[90][page needed]\n According to Lynn White, the invention of the horse stirrup enabled new patterns of warfare that eventually led to the development of feudalism (see Stirrup Thesis).[91]\n Most modern technologies have negative consequences that are both unavoidable[dubious – discuss] and unpredictable.[dubious – discuss] For example, almost all environmental problems, from chemical pollution to global warming, are the unexpected consequences of the application of modern technologies. Traffic congestion, deaths and injuries from car accidents, air pollution, and global warming are unintended consequences of the invention and large scale adoption of the automobile. Hospital infections are the unexpected side-effect of antibiotic resistance, and even human population growth leading to environmental degradation is the side effect of various technological (i.e., agricultural and industrial) revolutions.[92]\n Because of the complexity of ecosystems, deliberate changes to an ecosystem or other environmental interventions will often have (usually negative) unintended consequences. Sometimes, these effects cause permanent irreversible changes. Examples include:"},
{"url": "https://en.wikipedia.org/wiki/Web_search_engine", "title": "Search engine - Wikipedia", "content": "A search engine is a software system that provides hyperlinks to web pages and other relevant information on the Web in response to a user's query. The user inputs a query within a web browser or a mobile app, and the search results are often a list of hyperlinks, accompanied by textual summaries and images. Users also have the option of limiting the search to a specific type of results, such as images, videos, or news.\n For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. The speed and accuracy of an engine's response to a query is based on a complex system of indexing that is continuously updated by automated web crawlers. This can include data mining the files and databases stored on web servers, but some content is not accessible to crawlers.\n There have been many search engines since the dawn of the Web in the 1990s, but Google Search became the dominant one in the 2000s and has remained so. It currently has a 90% global market share.[1][2] The business of websites improving their visibility in search results, known as marketing and optimization, has thus largely focused on Google.\n In 1945, Vannevar Bush described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk.[3] He called it a memex. He described the system in an article titled \"As We May Think\" that was published in The Atlantic Monthly.[4] The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.[5]\n Link analysis eventually became a crucial component of search engines through algorithms such as Hyper Search and PageRank.[6][7]\n The first internet search engines predate the debut of the Web in December 1990: WHOIS user search dates back to 1982,[8] and the Knowbot Information Service multi-network user search was first implemented in 1989.[9] The first well documented search engine that searched content files, namely FTP files, was Archie, which debuted on 10 September 1990.[10]\n Prior to September 1993, the World Wide Web was entirely indexed by hand. There was a list of webservers edited by Tim Berners-Lee and hosted on the CERN webserver. One snapshot of the list in 1992 remains,[11] but as more and more web servers went online the central list could no longer keep up. On the NCSA site, new servers were announced under the title \"What's New!\".[12]\n The first tool used for searching content (as opposed to users) on the Internet was Archie.[13] The name stands for \"archive\" without the \"v\".[14] It was created by Alan Emtage,[14][15][16][17] computer science student at McGill University in Montreal, Quebec, Canada. The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; however, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually.\n The rise of Gopher (created in 1991 by Mark McCahill at the University of Minnesota) led to two new search programs, Veronica and Jughead. Like Archie, they searched the file names and titles stored in Gopher index systems. Veronica (Very Easy Rodent-Oriented Net-wide Index to Computerized Archives) provided a keyword search of most Gopher menu titles in the entire Gopher listings. Jughead (Jonzy's Universal Gopher Hierarchy Excavation And Display) was a tool for obtaining menu information from specific Gopher servers. While the name of the search engine \"Archie Search Engine\" was not a reference to the Archie comic book series, \"Veronica\" and \"Jughead\" are characters in the series, thus referencing their predecessor.\n In the summer of 1993, no search engine existed for the web, though numerous specialized catalogs were maintained by hand. Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format. This formed the basis for W3Catalog, the web's first primitive search engine, released on September 2, 1993.[18]\n In June 1993, Matthew Gray, then at MIT, produced what was probably the first web robot, the Perl-based World Wide Web Wanderer, and used it to generate an index called \"Wandex\". The purpose of the Wanderer was to measure the size of the World Wide Web, which it did until late 1995. The web's second search engine Aliweb appeared in November 1993. Aliweb did not use a web robot, but instead depended on being notified by website administrators of the existence at each site of an index file in a particular format.\n JumpStation (created in December 1993[19] by Jonathon Fletcher) used a web robot to find web pages and to build its index, and used a web form as the interface to its query program. It was thus the first WWW resource-discovery tool to combine the three essential features of a web search engine (crawling, indexing, and searching) as described below. Because of the limited resources available on the platform it ran on, its indexing and hence searching were limited to the titles and headings found in the web pages the crawler encountered.\n One of the first \"all text\" crawler-based search engines was WebCrawler, which came out in 1994. Unlike its predecessors, it allowed users to search for any word in any web page, which has become the standard for all major search engines since. It was also the search engine that was widely known by the public. Also, in 1994, Lycos (which started at Carnegie Mellon University) was launched and became a major commercial endeavor.\n The first popular search engine on the Web was Yahoo! Search.[20] The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, was a Web directory called Yahoo! Directory. In 1995, a search function was added, allowing users to search Yahoo! Directory.[21][22] It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, rather than its full-text copies of web pages.\n Soon after, a number of search engines appeared and vied for popularity. These included Magellan, Excite, Infoseek, Inktomi, Northern Light, and AltaVista. Information seekers could also browse the directory instead of doing a keyword-based search.\n In 1996, Robin Li developed the RankDex site-scoring algorithm for search engines results page ranking[23][24][25] and received a US patent for the technology.[26] It was the first search engine that used hyperlinks to measure the quality of websites it was indexing,[27] predating the very similar algorithm patent filed by Google two years later in 1998.[28] Larry Page referenced Li's work in some of his U.S. patents for PageRank.[29] Li later used his Rankdex technology for the Baidu search engine, which was founded by him in China and launched in 2000.\n In 1996, Netscape was looking to give a single search engine an exclusive deal as the featured search engine on Netscape's web browser. There was so much interest that instead, Netscape struck deals with five of the major search engines: for $5 million a year, each search engine would be in rotation on the Netscape search engine page. The five engines were Yahoo!, Magellan, Lycos, Infoseek, and Excite.[30][31]\n Google adopted the idea of selling search terms in 1998 from a small search engine company named goto.com. This move had a significant effect on the search engine business, which went from struggling to one of the most profitable businesses in the Internet.[citation needed]\n Search engines were also known as some of the brightest stars in the Internet investing frenzy that occurred in the late 1990s.[32] Several companies entered the market spectacularly, receiving record gains during their initial public offerings. Some have taken down their public search engine and are marketing enterprise-only editions, such as Northern Light. Many search engine companies were caught up in the dot-com bubble, a speculation-driven market boom that peaked in March 2000.\n Around 2000, Google's search engine rose to prominence.[33] The company achieved better results for many searches with an algorithm called PageRank, as was explained in the paper Anatomy of a Search Engine written by Sergey Brin and Larry Page, the later founders of Google.[7] This iterative algorithm ranks web pages based on the number and PageRank of other web sites and pages that link there, on the premise that good or desirable pages are linked to more than others. Larry Page's patent for PageRank cites Robin Li's earlier RankDex patent as an influence.[29][25] Google also maintained a minimalist interface to its search engine. In contrast, many of its competitors embedded a search engine in a web portal. In fact, the Google search engine became so popular that spoof engines emerged such as Mystery Seeker.\n By 2000, Yahoo! was providing search services based on Inktomi's search engine. Yahoo! acquired Inktomi in 2002, and Overture (which owned AlltheWeb and AltaVista) in 2003. Yahoo! switched to Google's search engine until 2004, when it launched its own search engine based on the combined technologies of its acquisitions.\n Microsoft first launched MSN Search in the fall of 1998 using search results from Inktomi. In early 1999, the site began to display listings from Looksmart, blended with results from Inktomi. For a short time in 1999, MSN Search used results from AltaVista instead. In 2004, Microsoft began a transition to its own search technology, powered by its own web crawler (called msnbot).\n Microsoft's rebranded search engine, Bing, was launched on June 1, 2009. On July 29, 2009, Yahoo! and Microsoft finalized a deal in which Yahoo! Search would be powered by Microsoft Bing technology.\n As of 2019,[update] active search engine crawlers include those of Google, Sogou, Baidu, Bing, Gigablast, Mojeek, DuckDuckGo and Yandex.\n \n A search engine maintains the following processes in near real time:[34]\n Web search engines get their information by web crawling from site to site. The \"spider\" checks for the standard filename robots.txt, addressed to it. The robots.txt file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. After checking for robots.txt and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \"[N]o web crawler may actually crawl the entire reachable web. Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient. Some websites are crawled exhaustively, while others are crawled only partially\".[36]\n Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields. The associations are made in a public database, made available for web search queries. A query from a user can be a single word, multiple words or a sentence. The index helps find information relating to the query as quickly as possible.[35] Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis.\n Between visits by the spider, the cached version of the page (some or all the content needed to render it) stored in the search engine working memory is quickly sent to an inquirer. If a visit is overdue, the search engine can just act as a web proxy instead. In this case, the page may differ from the search terms indexed.[35] The cached page holds the appearance of the version whose words were previously indexed, so a cached version of a page can be useful to the website when the actual page has been lost, but this problem is also considered a mild form of linkrot.\n Typically when a user enters a query into a search engine it is a few keywords.[37] The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. The real processing load is in generating the web pages that are the search results list: Every page in the entire list must be weighted according to information in the indexes.[35] Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing.\n Beyond simple keyword lookups, search engines offer their own GUI- or command-driven operators and search parameters to refine the search results. These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results, given the initial pages of the first search results.\nFor example, from 2007 the Google.com search engine has allowed one to filter by date by clicking \"Show search tools\" in the leftmost column of the initial search results page, and then selecting the desired date range.[38] It is also possible to weight by date because each page has a modification time. Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. The engine looks for the words or phrases exactly as entered. Some search engines provide an advanced feature called proximity search, which allows users to define the distance between keywords.[35] There is also concept-based searching where the research involves using statistical analysis on pages containing the words or phrases you search for.\n The usefulness of a search engine depends on the relevance of the result set it gives back. While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. Most search engines employ methods to rank the results to provide the \"best\" results first. How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.[35] The methods also change over time as Internet usage changes and new techniques evolve. There are two main types of search engine that have evolved: one is a system of predefined and hierarchically ordered keywords that humans have programmed extensively. The other is a system that generates an \"inverted index\" by analyzing texts it locates. This first form relies much more heavily on the computer itself to do the bulk of the work.\n Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. The search engines make money every time someone clicks on one of these ads.[39]\n Local search is the process that optimizes the efforts of local businesses. They focus on change to make sure all searches are consistent. It is important because many people determine where they plan to go and what to buy based on their searches.[40]\n As of January 2022,[update] Google is by far the world's most used search engine, with a market share of 90%, and the world's other most used search engines were Bing at 4%, Yandex at 2%, Yahoo! at 1%. Other search engines not listed have less than a 3% market share.[2] In 2024, Google's dominance was ruled an illegal monopoly in a case brought by the US Department of Justice.[41]\n In Russia, Yandex has a market share of 62.6%, compared to Google's 28.3%. Yandex is the second most used search engine on smartphones in Asia and Europe.[42] In China, Baidu is the most popular search engine.[43] South Korea-based search portal Naver is used for 62.8% of online searches in the country.[44] Yahoo! Japan and Yahoo! Taiwan are the most popular choices for Internet searches in Japan and Taiwan, respectively.[45] China is one of few countries where Google is not in the top three web search engines for market share. Google was previously more popular in China, but withdrew significantly after a disagreement with the government over censorship and a cyberattack. Bing, however, is in the top three web search engines with a market share of 14.95%. Baidu is top with 49.1% of the market share.[46][failed verification]\n Most countries' markets in the European Union are dominated by Google, except for the Czech Republic, where Seznam is a strong competitor.[47]\n The search engine Qwant is based in Paris, France, where it attracts most of its 50 million monthly registered users from.\n Although search engines are programmed to rank websites based on some combination of their popularity and relevancy, empirical studies indicate various political, economic, and social biases in the information they provide[48][49] and the underlying assumptions about the technology.[50] These biases can be a direct result of economic and commercial processes (e.g., companies that advertise with a search engine can become also more popular in its organic search results), and political processes (e.g., the removal of search results to comply with local laws).[51] For example, Google will not surface certain neo-Nazi websites in France and Germany, where Holocaust denial is illegal.\n Biases can also be a result of social processes, as search engine algorithms are frequently designed to exclude non-normative viewpoints in favor of more \"popular\" results.[52] Indexing algorithms of major search engines skew towards coverage of U.S.-based sites, rather than websites from non-U.S. countries.[49]\n Google Bombing is one example of an attempt to manipulate search results for political, social or commercial reasons.\n Several scholars have studied the cultural changes triggered by search engines,[53] and the representation of certain controversial topics in their results, such as terrorism in Ireland,[54] climate change denial,[55] and conspiracy theories.[56]\n There has been concern raised that search engines such as Google and Bing provide customized results based on the user's activity history, leading to what has been termed echo chambers or filter bubbles by Eli Pariser in 2011.[57] The argument is that search engines and social media platforms use algorithms to selectively guess what information a user would like to see, based on information about the user (such as location, past click behaviour and search history). As a result, websites tend to show only information that agrees with the user's past viewpoint. According to Eli Pariser users get less exposure to conflicting viewpoints and are isolated intellectually in their own informational bubble. Since this problem has been identified, competing search engines have emerged that seek to avoid this problem by not tracking or \"bubbling\" users, such as DuckDuckGo. However many scholars have questioned Pariser's view, finding that there is little evidence for the filter bubble.[58][59][60] On the contrary, a number of studies trying to verify the existence of filter bubbles have found only minor levels of personalisation in search,[60] that most people encounter a range of views when browsing online, and that Google news tends to promote mainstream established news outlets.[61][59]\n The global growth of the Internet and electronic media in the Arab and Muslim world during the last decade has encouraged Islamic adherents in the Middle East and Asian sub-continent, to attempt their own search engines, their own filtered search portals that would enable users to perform safe searches. More than usual safe search filters, these Islamic web portals categorizing websites into being either \"halal\" or \"haram\", based on interpretation of Sharia law. ImHalal came online in September 2011. Halalgoogling came online in July 2013. These use haram filters on the collections from Google and Bing (and others).[62]\n While lack of investment and slow pace in technologies in the Muslim world has hindered progress and thwarted success of an Islamic search engine, targeting as the main consumers Islamic adherents, projects like Muxlim (a Muslim lifestyle site) received millions of dollars from investors like Rite Internet Ventures, and it also faltered. Other religion-oriented search engines are Jewogle, the Jewish version of Google,[63] and Christian search engine SeekFind.org. SeekFind filters sites that attack or degrade their faith.[64]\n Web search engine submission is a process in which a webmaster submits a website directly to a search engine. While search engine submission is sometimes presented as a way to promote a website, it generally is not necessary because the major search engines use web crawlers that will eventually find most web sites on the Internet without assistance. They can either submit one web page at a time, or they can submit the entire site using a sitemap, but it is normally only necessary to submit the home page of a web site as search engines are able to crawl a well designed website. There are two remaining reasons to submit a web site or web page to a search engine: to add an entirely new web site without waiting for a search engine to discover it, and to have a web site's record updated after a substantial redesign.\n Some search engine submission software not only submits websites to multiple search engines, but also adds links to websites from their own pages. This could appear helpful in increasing a website's ranking, because external links are one of the most important factors determining a website's ranking. However, John Mueller of Google has stated that this \"can lead to a tremendous number of unnatural links for your site\" with a negative impact on site ranking.[65]\n In comparison to search engines, a social bookmarking system has several advantages over traditional automated resource location and classification software, such as search engine spiders. All tag-based classification of Internet resources (such as web sites) is done by human beings, who understand the content of the resource, as opposed to software, which algorithmically attempts to determine the meaning and quality of a resource. Also, people can find and bookmark web pages that have not yet been noticed or indexed by web spiders.[66] Additionally, a social bookmarking system can rank a resource based on how many times it has been bookmarked by users, which may be a more useful metric for end-users than systems that rank resources based on the number of external links pointing to it.  However, both types of ranking are vulnerable to fraud, (see Gaming the system), and both need technical countermeasures to try to deal with this.\n The first web search engine was Archie, created in 1990[67] by Alan Emtage, a student at McGill University in Montreal. The author originally wanted to call the program \"archives\", but had to shorten it to comply with the Unix world standard of assigning programs and files short, cryptic names such as grep, cat, troff, sed, awk, perl, and so on.\n The primary method of storing and retrieving files was via the File Transfer Protocol (FTP). This was (and still is) a system that specified a common way for computers to exchange files over the Internet. It works like this: Some administrator decides that he wants to make files available from his computer. He sets up a program on his computer, called an FTP server. When someone on the Internet wants to retrieve a file from this computer, he or she connects to it via another program called an FTP client. Any FTP client program can connect with any FTP server program as long as the client and server programs both fully follow the specifications set forth in the FTP protocol.\n Initially, anyone who wanted to share a file had to set up an FTP server in order to make the file available to others. Later, \"anonymous\" FTP sites became repositories for files, allowing all users to post and retrieve them.\n Even with archive sites, many important files were still scattered on small FTP servers. These files could be located only by the Internet equivalent of word of mouth: Somebody would post an e-mail to a message list or a discussion forum announcing the availability of a file.\n Archie changed all that. It combined a script-based data gatherer, which fetched site listings of anonymous FTP files, with a regular expression matcher for retrieving file names matching a user query. (4) In other words, Archie's gatherer scoured FTP sites across the Internet and indexed all of the files it found. Its regular expression matcher provided users with access to its database.[68]\n In 1993, the University of Nevada System Computing Services group developed Veronica.[67] It was created as a type of searching device similar to Archie but for Gopher files. Another Gopher search service, called Jughead, appeared a little later, probably for the sole purpose of rounding out the comic-strip triumvirate. Jughead is an acronym for Jonzy's Universal Gopher Hierarchy Excavation and Display, although, like Veronica, it is probably safe to assume that the creator backed into the acronym. Jughead's functionality was pretty much identical to Veronica's, although it appears to be a little rougher around the edges.[68]\n The World Wide Web Wanderer, developed by Matthew Gray in 1993[69] was the first robot on the Web and was designed to track the Web's growth. Initially, the Wanderer counted only Web servers, but shortly after its introduction, it started to capture URLs as it went along. The database of captured URLs became the Wandex, the first web database.\n Matthew Gray's Wanderer created quite a controversy at the time, partially because early versions of the software ran rampant through the Net and caused a noticeable netwide performance degradation. This degradation occurred because the Wanderer would access the same page hundreds of times a day. The Wanderer soon amended its ways, but the controversy over whether robots were good or bad for the Internet remained.\n In response to the Wanderer, Martijn Koster created Archie-Like Indexing of the Web, or ALIWEB, in October 1993. As the name implies, ALIWEB was the HTTP equivalent of Archie, and because of this, it is still unique in many ways.\n ALIWEB does not have a web-searching robot. Instead, webmasters of participating sites post their own index information for each page they want listed. The advantage to this method is that users get to describe their own site, and a robot does not run about eating up Net bandwidth.  The disadvantages of ALIWEB are more of a problem today. The primary disadvantage is that a special indexing file must be submitted. Most users do not understand how to create such a file, and therefore they do not submit their pages. This leads to a relatively small database, which meant that users are less likely to search ALIWEB than one of the large bot-based sites. This Catch-22 has been somewhat offset by incorporating other databases into the ALIWEB search, but it still does not have the mass appeal of search engines such as Yahoo! or Lycos.[68]\n Excite, initially called Architext, was started by six Stanford undergraduates in February 1993. Their idea was to use statistical analysis of word relationships in order to provide more efficient searches through the large amount of information on the Internet.\nTheir project was fully funded by mid-1993. Once funding was secured. they released a version of their search software for webmasters to use on their own web sites. At the time, the software was called Architext, but it now goes by the name of Excite for Web Servers.[68]\n Excite was the first serious commercial search engine which launched in 1995.[70] It was developed in Stanford and was purchased for $6.5 billion by @Home. In 2001 Excite and @Home went bankrupt and InfoSpace bought Excite for $10 million.\n Some of the first analysis of web searching was conducted on search logs from Excite[71][72]\n In April 1994, two Stanford University Ph.D. candidates, David Filo and Jerry Yang, created some pages that became rather popular. They called the collection of pages Yahoo! Their official explanation for the name choice was that they considered themselves to be a pair of yahoos.\n As the number of links grew and their pages began to receive thousands of hits a day, the team created ways to better organize the data. In order to aid in data retrieval, Yahoo! (www.yahoo.com) became a searchable directory. The search feature was a simple database search engine. Because Yahoo! entries were entered and categorized manually, Yahoo! was not really classified as a search engine. Instead, it was generally considered to be a searchable directory. Yahoo! has since automated some aspects of the gathering and classification process, blurring the distinction between engine and directory.\n The Wanderer captured only URLs, which made it difficult to find things that were not explicitly described by their URL. Because URLs are rather cryptic to begin with, this did not help the average user. Searching Yahoo! or the Galaxy was much more effective because they contained additional descriptive information about the indexed sites.\n At Carnegie Mellon University during July 1994, Michael Mauldin, on leave from CMU, developed the Lycos search engine.\n Search engines on the web are sites enriched with facility to search the content stored on other sites.  There is difference in the way various search engines work, but they all perform three basic tasks.[73]\n The process begins when a user enters a query statement into the system through the interface provided.\n There are basically three types of search engines: Those that are powered by robots (called crawlers; ants or spiders) and those that are powered by human submissions; and those that are a hybrid of the two.\n Crawler-based search engines are those that use automated software agents (called crawlers) that visit a Web site, read the information on the actual site, read the site's meta tags and also follow the links that the site connects to performing indexing on all linked Web sites as well. The crawler returns all that information back to a central depository, where the data is indexed. The crawler will periodically return to the sites to check for any information that has changed. The frequency with which this happens is determined by the administrators of the search engine.\n Human-powered search engines rely on humans to submit information that is subsequently indexed and catalogued. Only information that is submitted is put into the index.\n In both cases, when you query a search engine to locate information, you're actually searching through the index that the search engine has created —you are not actually searching the Web. These indices are giant databases of information that is collected and stored and subsequently searched. This explains why sometimes a search on a commercial search engine, such as Yahoo! or Google, will return results that are, in fact, dead links. Since the search results are based on the index, if the index has not been updated since a Web page became invalid the search engine treats the page as still an active link even though it no longer is. It will remain that way until the index is updated.\n So why will the same search on different search engines produce different results? Part of the answer to that question is because not all indices are going to be exactly the same. It depends on what the spiders find or what the humans submitted. But more important, not every search engine uses the same algorithm to search through the indices. The algorithm is what the search engines use to determine the relevance of the information in the index to what the user is searching for.\n One of the elements that a search engine algorithm scans for is the frequency and location of keywords on a Web page. Those with higher frequency are typically considered more relevant. But search engine technology is becoming sophisticated in its attempt to discourage what is known as keyword stuffing, or spamdexing.\n Another common element that algorithms analyze is the way that pages link to other pages in the Web. By analyzing how pages link to each other, an engine can both determine what a page is about (if the keywords of the linked pages are similar to the keywords on the original page) and whether that page is considered \"important\" and deserving of a boost in ranking. Just as the technology is becoming increasingly sophisticated to ignore keyword stuffing, it is also becoming more savvy to Web masters who build artificial links into their sites in order to build an artificial ranking.\n Modern web search engines are highly intricate software systems that employ technology that has evolved over the years. There are a number of sub-categories of search engine software that are separately applicable to specific 'browsing' needs. These include web search engines (e.g. Google), database or structured data search engines (e.g. Dieselpoint), and mixed search engines or enterprise search. The more prevalent search engines, such as Google and Yahoo!, utilize hundreds of thousands computers to process trillions of web pages in order to return fairly well-aimed results. Due to this high volume of queries and text processing, the software is required to run in a highly dispersed environment with a high degree of superfluity.\n Another category of search engines is scientific search engines. These are search engines which search scientific literature. The best known example is Google Scholar. Researchers are working on improving search engine technology by making them understand the content element of the articles, such as extracting theoretical constructs or key research findings.[74]"},
{"url": "https://en.wikipedia.org/wiki/Spamdexing", "title": "Spamdexing - Wikipedia", "content": "Spamdexing (also known as search engine spam, search engine poisoning, black-hat search engine optimization, search spam or web spam)[1] is the deliberate manipulation of search engine indexes. It involves a number of methods, such as link building and repeating related and/or unrelated phrases, to manipulate the relevance or prominence of resources indexed in a manner inconsistent with the purpose of the indexing system.[2][3]\n Spamdexing could be considered to be a part of search engine optimization,[4] although there are many SEO methods that improve the quality and appearance of the content of web sites and serve content useful to many users.[5]\n Search engines use a variety of algorithms to determine relevancy ranking. Some of these include determining whether the search term appears in the body text or URL of a web page. Many search engines check for instances of spamdexing and will remove suspect pages from their indexes. Also, search-engine operators can quickly block the results listing from entire websites that use spamdexing, perhaps in response to user complaints of false matches. The rise of spamdexing in the mid-1990s made the leading search engines of the time less useful. Using unethical methods to make websites rank higher in search engine results than they otherwise would is commonly referred to in the SEO (search engine optimization) industry as \"black-hat SEO\".[6] These methods are more focused on breaking the search-engine-promotion rules and guidelines. In addition to this, the perpetrators run the risk of their websites being severely penalized by the Google Panda and Google Penguin search-results ranking algorithms.[7]\n Common spamdexing techniques can be classified into two broad classes: content spam[5] (term spam) and link spam.[3]\n The earliest known reference[2] to the term spamdexing is by Eric Convey in his article \"Porn sneaks way back on Web\", The Boston Herald, May 22, 1996, where he said:\n The problem arises when site operators load their Web pages with hundreds of extraneous terms so search engines will list them among legitimate addresses.\nThe process is called \"spamdexing,\" a combination of spamming—the Internet term for sending users unsolicited information—and \"indexing.\"[2] Keyword stuffing had been used in the past to obtain top search engine rankings and visibility for particular phrases. This method is outdated and adds no value to rankings today. In particular, Google no longer gives good rankings to pages employing this technique.\n Hiding text from the visitor is done in many different ways. Text colored to blend with the background, CSS z-index positioning to place text underneath an image — and therefore out of view of the visitor — and CSS absolute positioning to have the text positioned far from the page center are all common techniques. By 2005, many invisible text techniques were easily detected by major search engines.\n \"Noscript\" tags are another way to place hidden content within a page. While they are a valid optimization method for displaying an alternative representation of scripted content, they may be abused, since search engines may index content that is invisible to most visitors.\n Sometimes inserted text includes words that are frequently searched (such as \"sex\"), even if those terms bear little connection to the content of a page, in order to attract traffic to advert-driven pages.\n In the past, keyword stuffing was considered to be either a white hat or a black hat tactic, depending on the context of the technique, and the opinion of the person judging it. While a great deal of keyword stuffing was employed to aid in spamdexing, which is of little benefit to the user, keyword stuffing in certain circumstances was not intended to skew results in a deceptive manner. Whether the term carries a pejorative or neutral connotation is dependent on whether the practice is used to pollute the results with pages of little relevance, or to direct traffic to a page of relevance that would have otherwise been de-emphasized due to the search engine's inability to interpret and understand related ideas. This is no longer the case.  Search engines now employ themed, related keyword techniques to interpret the intent of the content on a page.\n These techniques involve altering the logical view that a search engine has over the page's contents. They all aim at variants of the vector space model for information retrieval on text collections.\n Keyword stuffing is a search engine optimization (SEO) technique in which keywords are loaded into a web page's meta tags, visible content, or backlink anchor text in an attempt to gain an unfair rank advantage in search engines. Keyword stuffing may lead to a website being temporarily or permanently banned or penalized on major search engines.[8] The repetition of words in meta tags may explain why many search engines no longer use these tags. Nowadays, search engines focus more on the content that is unique, comprehensive, relevant, and helpful that overall makes the quality better which makes keyword stuffing useless, but it is still practiced by many webmasters.[citation needed]\n Many major search engines have implemented algorithms that recognize keyword stuffing, and reduce or eliminate any unfair search advantage that the tactic may have been intended to gain, and oftentimes they will also penalize, demote or remove websites from their indexes that implement keyword stuffing.\n Changes and algorithms specifically intended to penalize or ban sites using keyword stuffing include the Google Florida update (November 2003) Google Panda (February 2011)[9] Google Hummingbird (August 2013)[10] and Bing's September 2014 update.[11]\n Headlines in online news sites are increasingly packed with just the search-friendly keywords that identify the story. Traditional reporters and editors frown on the practice, but it is effective in optimizing news stories for search.[12]\n Unrelated hidden text is disguised by making it the same color as the background, using a tiny font size, or hiding it within HTML code such as \"no frame\" sections, alt attributes, zero-sized DIVs, and \"no script\" sections. People manually screening red-flagged websites for a search-engine company might temporarily or permanently block an entire website for having invisible text on some of its pages. However, hidden text is not always spamdexing: it can also be used to enhance accessibility.[13]\n This involves repeating keywords in the meta tags, and using meta keywords that are unrelated to the site's content. This tactic has been ineffective. Google declared that it doesn't use the keywords meta tag in its online search ranking in September 2009.[14]\n \"Gateway\" or doorway pages are low-quality web pages created with very little content, which are instead stuffed with very similar keywords and phrases. They are designed to rank highly within the search results, but serve no purpose to visitors looking for information. A doorway page will generally have \"click here to enter\" on the page; autoforwarding can also be used for this purpose. In 2006, Google ousted vehicle manufacturer BMW for using \"doorway pages\" to the company's German site, BMW.de.[15]\n Scraper sites are created using various programs designed to \"scrape\" search-engine results pages or other sources of content and create \"content\" for a website.[citation needed] The specific presentation of content on these sites is unique, but is merely an amalgamation of content taken from other sources, often without permission. Such websites are generally full of advertising (such as pay-per-click ads), or they redirect the user to other sites. It is even feasible for scraper sites to outrank original websites for their own information and organization names.\n Article spinning involves rewriting existing articles, as opposed to merely scraping content from other sites, to avoid penalties imposed by search engines for duplicate content. This process is undertaken by hired writers[citation needed] or automated using a thesaurus database or an artificial neural network.\n Similarly to article spinning, some sites use machine translation to render their content in several languages, with no human editing, resulting in unintelligible texts that nonetheless continue to be indexed by search engines, thereby attracting traffic.\n Link spam is defined as links between pages that are present for reasons\nother than merit.[16] Link spam takes advantage of link-based ranking algorithms, which gives websites higher rankings the more other highly ranked websites link to it. These techniques also aim at influencing other link-based ranking techniques such as the HITS algorithm.[citation needed]\n Link farms are tightly-knit networks of websites that link to each other for the sole purpose of exploiting the search engine ranking algorithms. These are also known facetiously as mutual admiration societies.[17] Use of links farms has greatly reduced with the launch of Google's first Panda Update in February 2011, which introduced significant improvements in its spam-detection algorithm.\n Blog networks (PBNs) are a group of authoritative websites used as a source of contextual links that point to the owner's main website to achieve higher search engine ranking. Owners of PBN websites use expired domains or auction domains that have backlinks from high-authority websites. Google targeted and penalized PBN users on several occasions with several massive deindexing campaigns since 2014.[18]\n Putting hyperlinks where visitors will not see them is used to increase link popularity. Highlighted link text can help rank a webpage higher for matching that phrase.\n A Sybil attack is the forging of multiple identities for malicious intent, named after the famous dissociative identity disorder patient and the book about her that shares her name, \"Sybil\".[19][20] A spammer may create multiple web sites at different domain names that all link to each other, such as fake blogs (known as spam blogs).\n Spam blogs are blogs created solely for commercial promotion and the passage of link authority to target sites. Often these \"splogs\" are designed in a misleading manner that will give the effect of a legitimate website but upon close inspection will often be written using spinning software or be very poorly written with barely readable content. They are similar in nature to link farms.[21][22]\n Guest blog spam is the process of placing guest blogs on websites for the sole purpose of gaining a link to another website or websites. Unfortunately, these are often confused with legitimate forms of guest blogging with other motives than placing links. This technique was made famous by Matt Cutts, who publicly declared \"war\" against this form of link spam.[23]\n Some link spammers utilize expired domain crawler software or monitor DNS records for domains that will expire soon, then buy them when they expire and replace the pages with links to their pages. However, it is possible but not confirmed that Google resets the link data on expired domains. [citation needed] To maintain all previous Google ranking data for the domain, it is advisable that a buyer grab the domain before it is \"dropped\".\n Some of these techniques may be applied for creating a Google bomb—that is, to cooperate with other users to boost the ranking of a particular page for a particular query.\n Web sites that can be edited by users can be used by spamdexers to insert links to spam sites if the appropriate anti-spam measures are not taken.\n Automated spambots can rapidly make the user-editable portion of a site unusable.\nProgrammers have developed a variety of automated spam prevention techniques to block or at least slow down spambots.\n Spam in blogs is the placing or solicitation of links randomly on other sites, placing a desired keyword into the hyperlinked text of the inbound link. Guest books, forums, blogs, and any site that accepts visitors' comments are particular targets and are often victims of drive-by spamming where automated software creates nonsense posts with links that are usually irrelevant and unwanted.\n Comment spam is a form of link spam that has arisen in web pages that allow dynamic user editing such as wikis, blogs, and guestbooks. It can be problematic because agents can be written that automatically randomly select a user edited web page, such as a Wikipedia article, and add spamming links.[24]\n Wiki spam is when a spammer uses the open editability of wiki systems to place links from the wiki site to the spam site.\n Referrer spam takes place when a spam perpetrator or facilitator accesses a web page (the referee), by following a link from another web page (the referrer), so that the referee is given the address of the referrer by the person's web browser. Some websites have a referrer log which shows which pages link to that site. By having a robot randomly access many sites enough times, with a message or specific address given as the referrer, that message or Internet address then appears in the referrer log of those sites that have referrer logs. Since some Web search engines base the importance of sites on the number of different sites linking to them, referrer-log spam may increase the search engine rankings of the spammer's sites.  Also, site administrators who notice the referrer log entries in their logs may follow the link back to the spammer's referrer page.\n Because of the large amount of spam posted to user-editable webpages, Google proposed a \"nofollow\" tag that could be embedded with links. A link-based search engine, such as Google's PageRank system, will not use the link to increase the score of the linked website if the link carries a nofollow tag. This ensures that spamming links to user-editable websites will not raise the sites ranking with search engines. Nofollow is used by several websites, such as Wordpress, Blogger and Wikipedia.[citation needed]\n A mirror site is the hosting of multiple websites with conceptually similar content but using different URLs. Some search engines give a higher rank to results where the keyword searched for appears in the URL.\n URL redirection is the taking of the user to another page without his or her intervention, e.g., using META refresh tags, Flash,  JavaScript, Java or Server side redirects. However, 301 Redirect, or permanent redirect, is not considered as a malicious behavior.\n Cloaking refers to any of several means to serve a page to the search-engine spider that is different from that seen by human users. It can be an attempt to mislead search engines regarding the content on a particular web site. Cloaking, however, can also be used to ethically increase accessibility of a site to users with disabilities or provide human users with content that search engines aren't able to process or parse. It is also used to deliver content based on a user's location; Google itself uses IP delivery, a form of cloaking, to deliver results. Another form of cloaking is code swapping, i.e., optimizing a page for top ranking and then swapping another page in its place once a top ranking is achieved. Google refers to these type of redirects as Sneaky Redirects.[25]\n Spamdexed pages are sometimes eliminated from search results by the search engine.\n Users can employ search operators for filtering. For Google, a keyword preceded by \"-\" (minus) will omit sites that contains the keyword in their pages or in the URL of the pages from search result. As an example, the search \"-<unwanted site>\" will eliminate sites that contains word \"<unwanted site>\" in their pages and the pages whose URL contains \"<unwanted site>\".\n Users could also use the Google Chrome extension \"Personal Blocklist (by Google)\", launched by Google in 2011 as part of countermeasures against content farming.[26] Via the extension, users could block a specific page, or set of pages from appearing in their search results. As of 2021, the original extension appears to be removed, although similar-functioning extensions may be used.\n Possible solutions to overcome search-redirection poisoning redirecting to illegal internet pharmacies include notification of operators of vulnerable legitimate domains. Further, manual evaluation of SERPs, previously published link-based and content-based algorithms as well as tailor-made automatic detection and classification engines can be used as benchmarks in the effective identification of pharma scam campaigns.[27]"},
{"url": "https://en.wikipedia.org/wiki/Parallel_computing", "title": "Parallel computing - Wikipedia", "content": "Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously.[1] Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but has gained broader interest due to the physical constraints preventing frequency scaling.[2] As power consumption (and consequently heat generation) by computers has become a concern in recent years,[3] parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.[4]\n In computer science, parallelism and concurrency are two different things: a parallel program uses multiple CPU cores, each core performing a task independently. On the other hand, concurrency enables a program to deal with multiple tasks even on a single CPU core; the core switches between tasks (i.e. threads) without necessarily completing each one. A program can have both, neither or a combination of parallelism and concurrency characteristics.[5]\n Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.\n In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones,[6] because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting optimal parallel program performance.\n A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law, which states that it is limited by the fraction of time for which the parallelization can be utilised.\n Traditionally, computer software has been written for serial computation. To solve a problem, an algorithm is constructed and implemented as a serial stream of instructions. These instructions are executed on a central processing unit on one computer. Only one instruction may execute at a time—after that instruction is finished, the next one is executed.[7]\n Parallel computing, on the other hand, uses multiple processing elements simultaneously to solve a problem. This is accomplished by breaking the problem into independent parts so that each processing element can execute its part of the algorithm simultaneously with the others. The processing elements can be diverse and include resources such as a single computer with multiple processors, several networked computers, specialized hardware, or any combination of the above.[7] Historically parallel computing was used for scientific computing and the simulation of scientific problems, particularly in the natural and engineering sciences, such as meteorology. This led to the design of parallel hardware and software, as well as high performance computing.[8]\n Frequency scaling was the dominant reason for improvements in computer performance from the mid-1980s until 2004. The runtime of a program is equal to the number of instructions multiplied by the average time per instruction. Maintaining everything else constant, increasing the clock frequency decreases the average time it takes to execute an instruction. An increase in frequency thus decreases runtime for all compute-bound programs.[9] However, power consumption P by a chip is given by the equation P = C × V 2 × F, where C is the capacitance being switched per clock cycle (proportional to the number of transistors whose inputs change), V is voltage, and F is the processor frequency (cycles per second).[10] Increases in frequency increase the amount of power used in a processor. Increasing processor power consumption led ultimately to Intel's May 8, 2004 cancellation of its Tejas and Jayhawk processors, which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm.[11]\n To deal with the problem of power consumption and overheating the major central processing unit (CPU or processor) manufacturers started to produce power efficient processors with multiple cores. The core is the computing unit of the processor and in multi-core processors each core is independent and can access the same memory concurrently. Multi-core processors have brought parallel computing to desktop computers. Thus parallelization of serial programs has become a mainstream programming task. In 2012 quad-core processors became standard for desktop computers, while servers had 10+ core processors.  By 2023  some processors had over hundred cores.  Some designs having a mix of performance and efficiency cores (such as ARM's big.LITTLE design) due to thermal and design constraints.[12][citation needed]. From Moore's law it can be predicted that the number of cores per processor will double every 18–24 months.\n \nAn operating system can ensure that different tasks and user programs are run in parallel on the available cores. However, for a serial software program to take full advantage of the multi-core architecture the programmer needs to restructure and parallelize the code. A speed-up of application software runtime will no longer be achieved through frequency scaling, instead programmers will need to parallelize their software code to take advantage of the increasing computing power of multicore architectures.[13]\n Main article: Amdahl's law\n Optimally, the speedup from parallelization would be linear—doubling the number of processing elements should halve the runtime, and doubling it a second time should again halve the runtime. However, very few parallel algorithms achieve optimal speedup. Most of them have a near-linear speedup for small numbers of processing elements, which flattens out into a constant value for large numbers of processing elements.\n The maximum potential speedup of an overall system can be calculated by Amdahl's law. [14] Amdahl's Law indicates that optimal performance improvement is achieved by balancing enhancements to both parallelizable and non-parallelizable components of a task. Furthermore, it reveals that increasing the number of processors yields diminishing returns, with negligible speedup gains beyond a certain point. [15][16]\n Amdahl's Law has limitations, including assumptions of fixed workload, neglecting inter-process communication and synchronization overheads, primarily focusing on computational aspect and ignoring extrinsic factors such as data persistence, I/O operations, and memory access overheads. [17][18][19]\n Gustafson's law and Universal Scalability Law give a more realistic assessment of the parallel performance. [20][21] Understanding data dependencies is fundamental in implementing parallel algorithms. No program can run more quickly than the longest chain of dependent calculations (known as the critical path), since calculations that depend upon prior calculations in the chain must be executed in order. However, most algorithms do not consist of just a long chain of dependent calculations; there are usually opportunities to execute independent calculations in parallel.\n Let Pi and Pj be two program segments. Bernstein's conditions[22] describe when the two are independent and can be executed in parallel. For Pi, let Ii be all of the input variables and Oi the output variables, and likewise for Pj. Pi and Pj are independent if they satisfy\n Violation of the first condition introduces a flow dependency, corresponding to the first segment producing a result used by the second segment. The second condition represents an anti-dependency, when the second segment produces a variable needed by the first segment. The third and final condition represents an output dependency: when two segments write to the same location, the result comes from the logically last executed segment.[23]\n Consider the following functions, which demonstrate several kinds of dependencies:\n In this example, instruction 3 cannot be executed before (or even in parallel with) instruction 2, because instruction 3 uses a result from instruction 2. It violates condition 1, and thus introduces a flow dependency.\n In this example, there are no dependencies between the instructions, so they can all be run in parallel.\n Bernstein's conditions do not allow memory to be shared between different processes. For that, some means of enforcing an ordering between accesses is necessary, such as semaphores, barriers or some other synchronization method.\n Subtasks in a parallel program are often called threads. Some parallel computer architectures use smaller, lightweight versions of threads known as fibers, while others use bigger versions known as processes. However, \"threads\" is generally accepted as a generic term for subtasks.[24] Threads will often need synchronized access to an object or other resource, for example when they must update a variable that is shared between them. Without synchronization, the instructions between the two threads may be interleaved in any order. For example, consider the following program:\n If instruction 1B is executed between 1A and 3A, or if instruction 1A is executed between 1B and 3B, the program will produce incorrect data. This is known as a race condition. The programmer must use a lock to provide mutual exclusion. A lock is a programming language construct that allows one thread to take control of a variable and prevent other threads from reading or writing it, until that variable is unlocked. The thread holding the lock is free to execute its critical section (the section of a program that requires exclusive access to some variable), and to unlock the data when it is finished. Therefore, to guarantee correct program execution, the above program can be rewritten to use locks:\n One thread will successfully lock variable V, while the other thread will be locked out—unable to proceed until V is unlocked again. This guarantees correct execution of the program. Locks may be necessary to ensure correct program execution when threads must serialize access to resources, but their use can greatly slow a program and may affect its reliability.[25]\n Locking multiple variables using non-atomic locks introduces the possibility of program deadlock. An atomic lock locks multiple variables all at once. If it cannot lock all of them, it does not lock any of them. If two threads each need to lock the same two variables using non-atomic locks, it is possible that one thread will lock one of them and the second thread will lock the second variable. In such a case, neither thread can complete, and deadlock results.[26]\n Many parallel programs require that their subtasks act in synchrony. This requires the use of a barrier. Barriers are typically implemented using a lock or a semaphore.[27] One class of algorithms, known as lock-free and wait-free algorithms, altogether avoids the use of locks and barriers. However, this approach is generally difficult to implement and requires correctly designed data structures.[28]\n Not all parallelization results in speed-up. Generally, as a task is split up into more and more threads, those threads spend an ever-increasing portion of their time communicating with each other or waiting on each other for access to resources.[29][30] Once the overhead from resource contention or communication dominates the time spent on other computation, further parallelization (that is, splitting the workload over even more threads) increases rather than decreases the amount of time required to finish. This problem, known as parallel slowdown,[31] can be improved in some cases by software analysis and redesign.[32]\n Applications are often classified according to how often their subtasks need to synchronize or communicate with each other. An application exhibits fine-grained parallelism if its subtasks must communicate many times per second; it exhibits coarse-grained parallelism if they do not communicate many times per second, and it exhibits embarrassing parallelism if they rarely or never have to communicate. Embarrassingly parallel applications are considered the easiest to parallelize.\n Michael J. Flynn created one of the earliest classification systems for parallel (and sequential) computers and programs, now known as Flynn's taxonomy. Flynn classified programs and computers by whether they were operating using a single set or multiple sets of instructions, and whether or not those instructions were using a single set or multiple sets of data.\n The single-instruction-single-data (SISD) classification is equivalent to an entirely sequential program. The single-instruction-multiple-data (SIMD) classification is analogous to doing the same operation repeatedly over a large data set. This is commonly done in signal processing applications. Multiple-instruction-single-data (MISD) is a rarely used classification. While computer architectures to deal with this were devised (such as systolic arrays), few applications that fit this class materialized. Multiple-instruction-multiple-data (MIMD) programs are by far the most common type of parallel programs.\n According to David A. Patterson and John L. Hennessy, \"Some machines are hybrids of these categories, of course, but this classic model has survived because it is simple, easy to understand, and gives a good first approximation. It is also—perhaps because of its understandability—the most widely used scheme.\"[34]\n Parallel computing can incur significant overhead in practice, primarily due to the costs associated with merging data from multiple processes. Specifically, inter-process communication and synchronization can lead to overheads that are substantially higher—often by two or more orders of magnitude—compared to processing the same data on a single thread. [35][36][37] Therefore, the overall improvement should be carefully evaluated.\n From the advent of very-large-scale integration (VLSI) computer-chip fabrication technology in the 1970s until about 1986, speed-up in computer architecture was driven by doubling computer word size—the amount of information the processor can manipulate per cycle.[38] Increasing the word size reduces the number of instructions the processor must execute to perform an operation on variables whose sizes are greater than the length of the word. For example, where an 8-bit processor must add two 16-bit integers, the processor must first add the 8 lower-order bits from each integer using the standard addition instruction, then add the 8 higher-order bits using an add-with-carry instruction and the carry bit from the lower order addition; thus, an 8-bit processor requires two instructions to complete a single operation, where a 16-bit processor would be able to complete the operation with a single instruction.\n Historically, 4-bit microprocessors were replaced with 8-bit, then 16-bit, then 32-bit microprocessors. This trend generally came to an end with the introduction of 32-bit processors, which has been a standard in general-purpose computing for two decades. Not until the early 2000s, with the advent of x86-64 architectures, did 64-bit processors become commonplace.\n A computer program is, in essence, a stream of instructions executed by a processor. Without instruction-level parallelism, a processor can only issue less than one instruction per clock cycle (IPC < 1). These processors are known as subscalar processors. These instructions can be re-ordered and combined into groups which are then executed in parallel without changing the result of the program. This is known as instruction-level parallelism. Advances in instruction-level parallelism dominated computer architecture from the mid-1980s until the mid-1990s.[39]\n All modern processors have multi-stage instruction pipelines. Each stage in the pipeline corresponds to a different action the processor performs on that instruction in that stage; a processor with an N-stage pipeline can have up to N different instructions at different stages of completion and thus can issue one instruction per clock cycle (IPC = 1). These processors are known as scalar processors. The canonical example of a pipelined processor is a RISC processor, with five stages: instruction fetch (IF), instruction decode (ID), execute (EX), memory access (MEM), and register write back (WB). The Pentium 4 processor had a 35-stage pipeline.[40]\n Most modern processors also have multiple execution units. They usually combine this feature with pipelining and thus can issue more than one instruction per clock cycle (IPC > 1). These processors are known as superscalar processors. Superscalar processors differ from multi-core processors in that the several execution units are not entire processors (i.e. processing units). Instructions can be grouped together only if there is no data dependency between them. Scoreboarding and the Tomasulo algorithm (which is similar to scoreboarding but makes use of register renaming) are two of the most common techniques for implementing out-of-order execution and instruction-level parallelism.\n Task parallelisms is the characteristic of a parallel program that \"entirely different calculations can be performed on either the same or different sets of data\".[41] This contrasts with data parallelism, where the same calculation is performed on the same or different sets of data. Task parallelism involves the decomposition of a task into sub-tasks and then allocating each sub-task to a processor for execution. The processors would then execute these sub-tasks concurrently and often cooperatively. Task parallelism does not usually scale with the size of a problem.[42]\n Superword level parallelism is a vectorization technique based on loop unrolling and basic block vectorization. It is distinct from loop vectorization algorithms in that it can exploit parallelism of inline code, such as manipulating coordinates, color channels or in loops unrolled by hand.[43]\n Main memory in a parallel computer is either shared memory (shared between all processing elements in a single address space), or distributed memory (in which each processing element has its own local address space).[44] Distributed memory refers to the fact that the memory is logically distributed, but often implies that it is physically distributed as well. Distributed shared memory and memory virtualization combine the two approaches, where the processing element has its own local memory and access to the memory on non-local processors. Accesses to local memory are typically faster than accesses to non-local memory. On the supercomputers, distributed shared memory space can be implemented using the programming model such as PGAS.  This model allows processes on one compute node to transparently access the remote memory of another compute node. All compute nodes are also connected to an external shared memory system via high-speed interconnect, such as Infiniband, this external shared memory system is known as burst buffer, which is typically built from arrays of non-volatile memory physically distributed across multiple I/O nodes.\n Computer architectures in which each element of main memory can be accessed with equal latency and bandwidth are known as uniform memory access (UMA) systems. Typically, that can be achieved only by a shared memory system, in which the memory is not physically distributed. A system that does not have this property is known as a non-uniform memory access (NUMA) architecture. Distributed memory systems have non-uniform memory access.\n Computer systems make use of caches—small and fast memories located close to the processor which store temporary copies of memory values (nearby in both the physical and logical sense). Parallel computer systems have difficulties with caches that may store the same value in more than one location, with the possibility of incorrect program execution. These computers require a cache coherency system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. Bus snooping is one of the most common methods for keeping track of which values are being accessed (and thus should be purged). Designing large, high-performance cache coherence systems is a very difficult problem in computer architecture. As a result, shared memory computer architectures do not scale as well as distributed memory systems do.[44]\n Processor–processor and processor–memory communication can be implemented in hardware in several ways, including via shared (either multiported or multiplexed) memory, a crossbar switch, a shared bus or an interconnect network of a myriad of topologies including star, ring, tree, hypercube, fat hypercube (a hypercube with more than one processor at a node), or n-dimensional mesh.\n Parallel computers based on interconnected networks need to have some kind of routing to enable the passing of messages between nodes that are not directly connected. The medium used for communication between the processors is likely to be hierarchical in large multiprocessor machines.\n Parallel computers can be roughly classified according to the level at which the hardware supports parallelism. This classification is broadly analogous to the distance between basic computing nodes. These are not mutually exclusive; for example, clusters of symmetric multiprocessors are relatively common.\n A multi-core processor is a processor that includes multiple processing units (called \"cores\") on the same chip. This processor differs from a superscalar processor, which includes multiple execution units and can issue multiple instructions per clock cycle from one instruction stream (thread); in contrast, a multi-core processor can issue multiple instructions per clock cycle from multiple instruction streams. IBM's Cell microprocessor, designed for use in the Sony PlayStation 3, is a prominent multi-core processor. Each core in a multi-core processor can potentially be superscalar as well—that is, on every clock cycle, each core can issue multiple instructions from one thread.\n Simultaneous multithreading  (of which Intel's Hyper-Threading is the best known) was an early form of pseudo-multi-coreism. A processor capable of concurrent multithreading includes multiple execution units in the same processing unit—that is it has a superscalar architecture—and can issue multiple instructions per clock cycle from multiple threads. Temporal multithreading on the other hand includes a single execution unit in the same processing unit and can issue one instruction at a time from multiple threads.\n A symmetric multiprocessor (SMP) is a computer system with multiple identical processors that share memory and connect via a bus.[45] Bus contention prevents bus architectures from scaling. As a result, SMPs generally do not comprise more than 32 processors.[46] Because of the small size of the processors and the significant reduction in the requirements for bus bandwidth achieved by large caches, such symmetric multiprocessors are extremely cost-effective, provided that a sufficient amount of memory bandwidth exists.[45]\n A distributed computer (also known as a distributed memory multiprocessor) is a distributed memory computer system in which the processing elements are connected by a network. Distributed computers are highly scalable. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have a lot of overlap, and no clear distinction exists between them.[47] The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel.[48]\n A cluster is a group of loosely coupled computers that work together closely, so that in some respects they can be regarded as a single computer.[49] Clusters are composed of multiple standalone machines connected by a network. While machines in a cluster do not have to be symmetric, load balancing is more difficult if they are not. The most common type of cluster is the Beowulf cluster, which is a cluster implemented on multiple identical commercial off-the-shelf computers connected with a TCP/IP Ethernet local area network.[50] Beowulf technology was originally developed by Thomas Sterling and Donald Becker. 87% of all Top500 supercomputers are clusters.[51] The remaining are Massively Parallel Processors, explained below.\n Because grid computing systems (described below) can easily handle embarrassingly parallel problems, modern clusters are typically designed to handle more difficult problems—problems that require nodes to share intermediate results with each other more often. This requires a high bandwidth and, more importantly, a low-latency interconnection network. Many historic and current supercomputers use customized high-performance network hardware specifically designed for cluster computing, such as the Cray Gemini network.[52] As of 2014, most current supercomputers use some off-the-shelf standard network hardware, often Myrinet, InfiniBand, or Gigabit Ethernet.\n A massively parallel processor (MPP) is a single computer with many networked processors. MPPs have many of the same characteristics as clusters, but MPPs have specialized interconnect networks (whereas clusters use commodity hardware for networking). MPPs also tend to be larger than clusters, typically having \"far more\" than 100 processors.[53] In an MPP, \"each CPU contains its own memory and copy of the operating system and application. Each subsystem communicates with the others via a high-speed interconnect.\"[54]\n IBM's Blue Gene/L, the fifth fastest supercomputer in the world according to the June 2009 TOP500 ranking, is an MPP.\n Grid computing is the most distributed form of parallel computing. It makes use of computers communicating over the Internet to work on a given problem. Because of the low bandwidth and extremely high latency available on the Internet, distributed computing typically deals only with embarrassingly parallel problems.\n Most grid computing applications use middleware (software that sits between the operating system and the application to manage network resources and standardize the software interface). The most common grid computing middleware is the Berkeley Open Infrastructure for Network Computing (BOINC). Often volunteer computing software makes use of \"spare cycles\", performing computations at times when a computer is idling.[55]\n The ubiquity of Internet brought the possibility of large-scale cloud computing.\n Within parallel computing, there are specialized parallel devices that remain niche areas of interest. While not domain-specific, they tend to be applicable to only a few classes of parallel problems.\n Reconfigurable computing is the use of a field-programmable gate array (FPGA) as a co-processor to a general-purpose computer. An FPGA is, in essence, a computer chip that can rewire itself for a given task.\n FPGAs can be programmed with hardware description languages such as VHDL[56] or Verilog.[57] Several vendors have created C to HDL languages that attempt to emulate the syntax and semantics of the C programming language, with which most programmers are familiar. The best known C to HDL languages are Mitrion-C, Impulse C, and Handel-C. Specific subsets of SystemC based on C++ can also be used for this purpose.\n AMD's decision to open its HyperTransport technology to third-party vendors has become the enabling technology for high-performance reconfigurable computing.[58] According to Michael R. D'Amour, Chief Operating Officer of DRC Computer Corporation, \"when we first walked into AMD, they called us 'the socket stealers.' Now they call us their partners.\"[58]\n General-purpose computing on graphics processing units (GPGPU) is a fairly recent trend in computer engineering research. GPUs are co-processors that have been heavily optimized for computer graphics processing.[59] Computer graphics processing is a field dominated by data parallel operations—particularly linear algebra matrix operations.\n In the early days, GPGPU programs used the normal graphics APIs for executing programs. However, several new programming languages and platforms have been built to do general purpose computation on GPUs with both Nvidia and AMD releasing programming environments with CUDA and Stream SDK respectively. Other GPU programming languages include BrookGPU, PeakStream, and RapidMind. Nvidia has also released specific products for computation in their Tesla series. The technology consortium Khronos Group has released the OpenCL specification, which is a framework for writing programs that execute across platforms consisting of CPUs and GPUs. AMD, Apple, Intel, Nvidia and others are supporting OpenCL.\n Several application-specific integrated circuit (ASIC) approaches have been devised for dealing with parallel applications.[60][61][62]\n Because an ASIC is (by definition) specific to a given application, it can be fully optimized for that application. As a result, for a given application, an ASIC tends to outperform a general-purpose computer. However, ASICs are created by UV photolithography. This process requires a mask set, which can be extremely expensive. A mask set can cost over a million US dollars.[63] (The smaller the transistors required for the chip, the more expensive the mask will be.) Meanwhile, performance increases in general-purpose computing over time (as described by Moore's law) tend to wipe out these gains in only one or two chip generations.[58] High initial cost, and the tendency to be overtaken by Moore's-law-driven general-purpose computing, has rendered ASICs unfeasible for most parallel computing applications. However, some have been built. One example is the PFLOPS RIKEN MDGRAPE-3 machine which uses custom ASICs for molecular dynamics simulation.\n A vector processor is a CPU or computer system that can execute the same instruction on large sets of data. Vector processors have high-level operations that work on linear arrays of numbers or vectors. An example vector operation is A = B × C, where A, B, and C are each 64-element vectors of 64-bit floating-point numbers.[64] They are closely related to Flynn's SIMD classification.[64]\n Cray computers became famous for their vector-processing computers in the 1970s and 1980s. However, vector processors—both as CPUs and as full computer systems—have generally disappeared. Modern processor instruction sets do include some vector processing instructions, such as with Freescale Semiconductor's AltiVec and Intel's Streaming SIMD Extensions (SSE).\n Concurrent programming languages, libraries, APIs, and parallel programming models (such as algorithmic skeletons) have been created for programming parallel computers. These can generally be divided into classes based on the assumptions they make about the underlying memory architecture—shared memory, distributed memory, or shared distributed memory. Shared memory programming languages communicate by manipulating shared memory variables. Distributed memory uses message passing. POSIX Threads and OpenMP are two of the most widely used shared memory APIs, whereas Message Passing Interface (MPI) is the most widely used message-passing system API.[65] One concept used in programming parallel programs is the future concept, where one part of a program promises to deliver a required datum to another part of a program at some future time.\n Efforts to standardize parallel programming include an open standard called OpenHMPP for hybrid multi-core parallel programming. The OpenHMPP directive-based programming model offers a syntax to efficiently offload computations on hardware accelerators and to optimize data movement to/from the hardware memory using remote procedure calls.\n The rise of consumer GPUs has led to support for compute kernels, either in graphics  APIs (referred to as compute shaders), in dedicated APIs (such as OpenCL), or in other language extensions.\n Automatic parallelization of a sequential program by a compiler is the \"holy grail\" of parallel computing, especially with the aforementioned limit of processor frequency. Despite decades of work by compiler researchers, automatic parallelization has had only limited success.[66]\n Mainstream parallel programming languages remain either explicitly parallel or (at best) partially implicit, in which a programmer gives the compiler directives for parallelization. A few fully implicit parallel programming languages exist—SISAL, Parallel Haskell, SequenceL, System C (for FPGAs), Mitrion-C, VHDL, and Verilog.\n As a computer system grows in complexity, the mean time between failures usually decreases. Application checkpointing is a technique whereby the computer system takes a \"snapshot\" of the application—a record of all current resource allocations and variable states, akin to a core dump—; this information can be used to restore the program if the computer should fail. Application checkpointing means that the program has to restart from only its last checkpoint rather than the beginning. While checkpointing provides benefits in a variety of situations, it is especially useful in highly parallel systems with a large number of processors used in high performance computing.[67]\n As parallel computers become larger and faster, we are now able to solve problems that had previously taken too long to run. Fields as varied as bioinformatics (for protein folding and sequence analysis) and economics have taken advantage of parallel computing. Common types of problems in parallel computing applications include:[68]\n Parallel computing can also be applied to the design of fault-tolerant computer systems, particularly via lockstep systems performing the same operation in parallel. This provides redundancy in case one component fails, and also allows automatic error detection and error correction if the results differ. These methods can be used to help prevent single-event upsets caused by transient errors.[70] Although additional measures may be required in embedded or specialized systems, this method can provide a cost-effective approach to achieve n-modular redundancy in commercial off-the-shelf systems.\n The origins of true (MIMD) parallelism go back to Luigi Federico Menabrea and his Sketch of the Analytic Engine Invented by Charles Babbage.[72][73][74]\n In 1957, Compagnie des Machines Bull announced the first computer architecture specifically designed for parallelism, the Gamma 60.[75] It utilized a fork-join model and a \"Program Distributor\" to dispatch and collect data to and from independent processing units connected to a central memory.[76][77]\n In April 1958, Stanley Gill (Ferranti) discussed parallel programming and the need for branching and waiting.[78] Also in 1958, IBM researchers John Cocke and Daniel Slotnick discussed the use of parallelism in numerical calculations for the first time.[79] Burroughs Corporation introduced the D825 in 1962, a four-processor computer that accessed up to 16 memory modules through a crossbar switch.[80] In 1967, Amdahl and Slotnick published a debate about the feasibility of parallel processing at American Federation of Information Processing Societies Conference.[79] It was during this debate that Amdahl's law was coined to define the limit of speed-up due to parallelism.\n In 1969, Honeywell introduced its first Multics system, a symmetric multiprocessor system capable of running up to eight processors in parallel.[79] C.mmp, a multi-processor project at Carnegie Mellon University in the 1970s, was among the first multiprocessors with more than a few processors. The first bus-connected multiprocessor with snooping caches was the Synapse N+1 in 1984.[73]\n SIMD parallel computers can be traced back to the 1970s. The motivation behind early SIMD computers was to amortize the gate delay of the processor's control unit over multiple instructions.[81] In 1964, Slotnick had proposed building a massively parallel computer for the Lawrence Livermore National Laboratory.[79] His design was funded by the US Air Force, which was the earliest SIMD parallel-computing effort, ILLIAC IV.[79] The key to its design was a fairly high parallelism, with up to 256 processors, which allowed the machine to work on large datasets in what would later be known as vector processing. However, ILLIAC IV was called \"the most infamous of supercomputers\", because the project was only one-fourth completed, but took 11 years and cost almost four times the original estimate.[71] When it was finally ready to run its first real application in 1976, it was outperformed by existing commercial supercomputers such as the Cray-1.\n In the early 1970s, at the MIT Computer Science and Artificial Intelligence Laboratory, Marvin Minsky and Seymour Papert started developing the Society of Mind theory, which views the biological brain as massively parallel computer. In 1986, Minsky published The Society of Mind, which claims that \"mind is formed from many little agents, each mindless by itself\".[82] The theory attempts to explain how what we call intelligence could be a product of the interaction of non-intelligent parts. Minsky says that the biggest source of ideas about the theory came from his work in trying to create a machine that uses a robotic arm, a video camera, and a computer to build with children's blocks.[83]\n Similar models (which also view the biological brain as a massively parallel computer, i.e., the brain is made up of a constellation of independent or semi-independent agents) were also described by:"},
{"url": "https://en.wikipedia.org/wiki/Distributed_web_crawling", "title": "Distributed web crawling - Wikipedia", "content": "Distributed web crawling is a distributed computing technique whereby Internet search engines employ many computers to index the Internet via web crawling. Such systems may allow for users to voluntarily offer their own computing and bandwidth resources towards crawling web pages. By spreading the load of these tasks across  many computers, costs that would otherwise be spent on maintaining large computing clusters are avoided.\n Cho[1] and Garcia-Molina studied two types of policies:\n With this type of policy, a central server assigns new URLs to different crawlers dynamically. This allows the central server to, for instance, dynamically balance the load of each crawler.[2]\n With dynamic assignment, typically the systems can also add or remove downloader processes. The central server may become the bottleneck, so most of the workload must be transferred to the distributed crawling processes for large crawls.\n There are two configurations of crawling architectures with dynamic assignments that have been described by Shkapenyuk and Suel:[3]\n With this type of policy, there is a fixed rule stated from the beginning of the crawl that defines how to assign new URLs to the crawlers.\n For static assignment, a hashing function can be used to transform URLs (or, even better, complete website names) into a number that corresponds to the index of the corresponding crawling process.[4] As there are external links that will go from a Web site assigned to one crawling process to a website assigned to a different crawling process, some exchange of URLs must occur.\n To reduce the overhead due to the exchange of URLs between crawling processes, the exchange should be done in batch, several URLs at a time, and the most cited URLs in the collection should be known by all crawling processes before the crawl (e.g.: using data from a previous crawl).[1]\n As of 2003, most modern commercial search engines use this technique. Google and Yahoo use thousands of individual computers to crawl the Web.\n Newer projects are attempting to use a less structured, more ad hoc form of collaboration by enlisting volunteers to join the effort using, in many cases, their home or personal computers. LookSmart is the largest search engine to use this technique, which powers its Grub distributed web-crawling project. Wikia (now known as Fandom) acquired Grub from LookSmart in 2007.[5]\n This solution uses computers that are connected to the Internet to crawl Internet addresses in the background. Upon downloading crawled web pages, they are compressed and sent back, together with a status flag (e.g. changed, new, down, redirected) to the powerful central servers. The servers, which manage a large database, send out new URLs to clients for testing.\n According to the FAQ about Nutch, an open-source search engine website, the savings in bandwidth by distributed web crawling are not significant, since \"A successful search engine requires more bandwidth to upload query result pages than its crawler needs to download pages...\".[6]"},
{"url": "https://en.wikipedia.org/wiki/Larry_Page", "title": "Larry Page - Wikipedia", "content": "Lawrence Edward Page[2][3][4] (born March 26, 1973) is an American businessman, computer engineer and computer scientist best known for co-founding Google with Sergey Brin.[2][5]\n Page was chief executive officer of Google from 1997 until August 2001 when he stepped down in favor of Eric Schmidt, and then again from April 2011 until July 2015 when he became CEO of its newly formed parent organization Alphabet Inc.[6] He held that post until December 4, 2019, when he and Brin stepped down from all executive positions and day-to-day roles within the company. He remains an Alphabet board member, employee, and controlling shareholder.[7]\n Page has an estimated net worth of $175 billion as of December  2024, according to the Bloomberg Billionaires Index, and $162.2 billion according to Forbes, making him the sixth-richest person in the world.[8][9] He has also invested in flying car startups Kitty Hawk and Opener.[10]\n Page is the co-creator and namesake of PageRank, a search ranking algorithm for Google[18] for which he received the Marconi Prize in 2004 along with co-writer Brin.[19]\n Lawrence Edward Page was born on March 26, 1973,[20] in Lansing, Michigan.[21][22] His mother is Jewish;[23] his maternal grandfather later immigrated to Israel,[22] though Page's household while growing up was secular.[23][24] His father, Carl Victor Page Sr., earned a PhD in computer science from the University of Michigan. BBC reporter Will Smale described him as a \"pioneer in computer science and artificial intelligence\".[25] Page's paternal grandparents came from a Protestant background.[citation needed] Page's father was a computer science professor at Michigan State University and his mother Gloria was an instructor in computer programming at Lyman Briggs College at the same institution.[26][25][27] Larry's parents divorced when he was eight years old, but he maintained a good relationship both with his mother Gloria and his father's long-term partner and MSU professor Joyce Wildenthal.[28]: ch. 2 \n When Larry Page was six years old, in 1979, his father brought home an Exidy Sorcerer computer, which Larry soon mastered and began using for schoolwork.[29]\n During an interview, Page recalled his childhood home \"was usually a mess, with computers, science, and technology magazines and Popular Science magazines all over the place\", an environment in which he immersed himself.[30] Page was an avid reader during his youth, writing in his 2013 Google founders letter: \"I remember spending a huge amount of time pouring [sic] over books and magazines\".[31] According to writer Nicholas Carlson, the combined influence of Page's home atmosphere and his attentive parents \"fostered creativity and invention\". Page also played instruments and studied music composition while growing up. His parents sent him to music summer camp—Interlochen Arts Camp in Interlochen, Michigan, and Page has mentioned that his musical education inspired his impatience and obsession with speed in computing. \"In some sense, I feel like music training led to the high-speed legacy of Google for me\". In an interview Page said that \"In music, you're very cognizant of time. Time is like the primary thing\" and that \"If you think about it from a music point of view, if you're a percussionist, you hit something, it's got to happen in milliseconds, fractions of a second\".[11]\n Page was first attracted to computers when he was six years old, as he was able to \"play with the stuff lying around\"—first-generation personal computers—that had been left by his mother and father.[26] He became the \"first kid in his elementary school to turn in an assignment from a word processor\".[32] His older brother Carl Victor Page Jr.[28]: ch. 2  also taught him to take things apart and before long he was taking \"everything in his house apart to see how it worked\". He said that \"from a very early age, I also realized I wanted to invent things. So I became interested in technology and business. Probably from when I was 12, I knew I was going to start a company eventually.\"[32]\n Page attended Okemos Montessori School (now called Montessori Radmoor) in Okemos, Michigan, from ages 2 to 7 (1975 to 1979). He attended East Lansing High School, graduating in 1991. In summer school, he attended Interlochen Center for the Arts at Interlochen, Michigan, playing flute but mainly saxophone for two summers.\n Page received a Bachelor of Science with a major in computer engineering with honors from the University of Michigan in 1995 and a Master of Science in computer science from Stanford University in 1998.[33][34][35][36]\n While at the University of Michigan, Page created an inkjet printer made of Lego bricks (literally a line plotter), after he thought it possible to print large posters cheaply with the use of inkjet cartridges—Page reverse-engineered the ink cartridge and built the electronics and mechanics to drive it.[26] Page served as the president of the Beta Epsilon chapter of the Eta Kappa Nu honor society,[37] and was a member of the 1993 \"Maize & Blue\" University of Michigan Solar Car team.[38] As an undergraduate at the University of Michigan, he proposed that the school replace its bus system with a personal rapid-transit system, which is essentially a driverless monorail with separate cars for every passenger.[11] He also developed a business plan for a company that would use software to build a music synthesizer during this time.[39]\n After enrolling in a computer science PhD program at Stanford University, Page was in search of a dissertation theme and considered exploring the mathematical properties of the World Wide Web, understanding its link structure as a huge graph. His supervisor, Terry Winograd, encouraged him to pursue the idea, and Page recalled in 2008 that it was the best advice he had ever received.[40] He also considered doing research on telepresence and self-driving cars during this time.[41][42][43][44]\n Page focused on the problem of finding out which web pages linked to a given page, considering the number and nature of such backlinks as valuable information for that page. The role of citations in academic publishing would also become pertinent for the research.[44] Sergey Brin, a fellow Stanford PhD student, would soon join Page's research project, nicknamed \"BackRub.\"[44] Together, the pair authored a research paper titled \"The Anatomy of a Large-Scale Hypertextual Web Search Engine\", which became one of the most downloaded scientific documents in the history of the Internet at the time.[26][42]\n John Battelle, co-founder of Wired magazine, wrote that Page had reasoned that:\n \"[the] entire Web was loosely based on the premise of citation—after all, what is a link but a citation? If he could devise a method to count and qualify each backlink on the Web, as Page puts it \"the Web would become a more valuable place.\"\"[44] Battelle further described how Page and Brin began working together on the project:\n \"At the time Page conceived of BackRub, the Web comprised an estimated 10 million documents, with an untold number of links between them. The computing resources required to crawl such a beast were well beyond the usual bounds of a student project. Unaware of exactly what he was getting into, Page began building out his crawler. The idea's complexity and scale lured Brin to the job. A polymath who had jumped from project to project without settling on a thesis topic, he found the premise behind BackRub fascinating. \"I talked to lots of research groups\" around the school, Brin recalls, \"and this was the most exciting project, both because it tackled the Web, which represents human knowledge, and because I liked Larry.\"\"[44] To convert the backlink data gathered by BackRub's web crawler into a measure of importance for a given web page, Brin and Page developed the PageRank algorithm, and realized that it could be used to build a search engine far superior to existing ones.[44] The algorithm relied on a new technology that analyzed the relevance of the backlinks that connected one web page to another.[45]\n Combining their ideas, the pair began utilizing Page's dormitory room as a machine laboratory, and extracted spare parts from inexpensive computers to create a device that they used to connect the now nascent search engine with Stanford's broadband campus network.[44] After filling Page's room with equipment, they then converted Brin's dorm room into an office and programming center, where they tested their new search engine designs on the Web. The rapid growth of their project caused Stanford's computing infrastructure to experience problems.[46]\n Page and Brin used the former's basic HTML programming skills to set up a simple search page for users, as they did not have a web page developer to create anything visually elaborate. They also began using any computer part they could find to assemble the necessary computing power to handle searches by multiple users. As their search engine grew in popularity among Stanford users, it required additional servers to process the queries. In August 1996, the initial version of Google, still on the Stanford University website, was made available to Internet users.[44]\n By early 1997, the BackRub page described the state as follows:\n \"Some Rough Statistics (from August 29, 1996)\n Total indexable HTML URLs: 75.2306 Million\n Total content downloaded: 207.022 gigabytes\n ...\n \nBackRub is written in Java and Python and runs on several Sun Ultras and Intel Pentiums running Linux. The primary database is kept on a Sun Ultra series II with 28GB of a disk. Scott Hassan and Alan Steremberg have provided a great deal of very talented implementation help. Sergey Brin has also been very involved and deserves many thanks.\" BackRub already exhibited the rudimentary functions and characteristics of a search engine: a query input was entered and it provided a list of backlinks ranked by importance. Page recalled: \"We realized that we had a querying tool. It gave you a good overall ranking of pages and ordering of follow-up pages.\"[48] Page said that in mid-1998 they finally realized the further potential of their project: \"Pretty soon, we had 10,000 searches a day. And we figured, maybe this is real.\"[46]\n Page and Brin's vision has been compared to that of Johannes Gutenberg, the inventor of modern printing:[49]\n \"In 1440, Johannes Gutenberg introduced Europe to the mechanical printing press, printing Bibles for mass consumption. The technology allowed for books and manuscripts – originally replicated by hand – to be printed at a much faster rate, thus spreading knowledge and helping to usher in the European Renaissance [...] Google has done a similar job.\" The comparison was also noted by the authors of The Google Story: \"Not since Gutenberg [...] has any new invention empowered individuals, and transformed access to information, as profoundly as Google.\"[28] Also, not long after the two \"cooked up their new engine for web searches, they began thinking about information that was at the time beyond the web\" such as digitizing books and expanding health information.[46]\n \nMark Malseed wrote in a 2003 feature story: \"Soliciting funds from faculty members, family and friends, Brin and Page scraped together enough to buy some servers and rent that famous garage in Menlo Park. [Soon after], Sun Microsystems co-founder Andy Bechtolsheim wrote a $100,000 cheque to \"Google, Inc.\" The only problem was, \"Google, Inc.\" did not yet exist—the company hadn't yet been incorporated. For two weeks, as they handled the paperwork, the young men had nowhere to deposit the money.\"[50] In 1998,[51] Brin and Page incorporated Google, Inc.[52] with the initial domain name of \"Googol\", derived from a number that consists of one followed by one hundred zeros representing the vast amount of data that the search engine was intended to explore. Using the garage in their friend Susan Wojcicki's Menlo Park home for $1,700 a month, Page and Brin were able to successfully build the Google search engine.[53] Following inception, Page appointed himself as CEO, while Brin, named Google's co-founder, was Google's president.[11] Writer Nicholas Carlson wrote in 2014: \"The pair's mission was 'to organize the world's information and make it universally accessible and useful.\"[54] With a US$1-million loan from friends and family, the inaugural team moved into a Mountain View office by the start of 2000. In 1999, Page experimented with smaller servers so Google could fit more into each square meter of the third-party warehouses the company rented for their servers. This eventually led to a search engine that ran much faster than Google's competitors at the time.\"[11] By June 2000, Google had indexed one billion Internet URLs (Uniform Resource Locators), making it the most comprehensive search engine on the Web at the time. The company cited NEC Research Institute data in its June 26 press release, stating that \"there are more than 1 billion web pages online today\", with Google \"providing access to 560 million full-text indexed web pages and 500 million partially indexed URLs.\"[55]\n During his first tenure as CEO, Page embarked on an attempt to fire all of Google's project managers in 2001. Page's plan involved all of Google's engineers reporting to a VP of engineering, who would then report directly to him—Page explained that he did not like non-engineers supervising engineers due to their limited technical knowledge.[11] Page even documented his management tenets for his team to use as a reference:\n Even though Page's new model was unsustainable and led to disgruntlement among the affected employees, his issue with engineers being managed by non-engineering staff gained traction.[56] Page also believed that the faster Google's search engine returned answers, the more it would be used. He fretted over milliseconds and pushed his engineers—from those who developed algorithms to those who built data centers—to think about lag times. He also pushed for keeping Google's home page famously sparse in its design because it would help the page load faster.[39]\n Before Silicon Valley's two most prominent investors, Kleiner Perkins and Sequoia Capital, agreed to invest a combined total of $50 million in Google, they applied pressure on Page to step down as CEO so that a more experienced leader could build a \"world-class management team.\" Page eventually became amenable to the idea after meeting with other technology CEOs, including Steve Jobs and Intel's Andrew Grove. Eric Schmidt, who had been hired as chairman of Google in March 2001, left his full-time position as the CEO of Novell to take the same role at Google in August of the same year, and Page moved aside to assume the president of products role.[11]\n Under Schmidt's leadership, Google underwent a period of major growth and expansion, which included its initial public offering (IPO) on August 20, 2004. He always acted in consultation with Page and Brin when he embarked on initiatives such as the hiring of an executive team and the creation of a sales force management system. Page remained the boss at Google in the eyes of the employees, as he gave final approval on all new hires, and it was Page who provided the signature for the IPO, the latter making him a billionaire at the age of 30.[11]\n Page led the acquisition of Android for $50 million in 2005 to fulfill his ambition to place handheld computers in the possession of consumers so that they could access Google anywhere. The purchase was made without Schmidt's knowledge, but the CEO was not perturbed by the relatively small acquisition. Page became passionate about Android and spent large amounts of time with Android CEO and cofounder Andy Rubin. By September 2008, T-Mobile launched the G1, the first phone using Android software and, by 2010, 17.2% of the handset market consisted of Android sales, overtaking Apple for the first time. Android became the world's most popular mobile operating system shortly afterward.[11]\n Following a January 2011 announcement,[57] Page officially became the chief executive of Google on April 4, 2011, while Schmidt stepped down to become executive chairman.[58] By this time, Google had over $180 billion market capitalization and more than 24,000 employees.[59] Reporter Max Nisen described the decade prior to Page's second appointment as Google's CEO as Page's \"lost decade\" saying that while he exerted significant influence at Google via product development and other operations, he became increasingly disconnected and less responsive over time.[11][56]\n Schmidt announced the end of his tenure as CEO on January 20, 2011, jokingly tweeting on Twitter: \"Adult-supervision no longer needed.\"[60]\n As Google's new CEO, Page's two key goals were the development of greater autonomy for the executives overseeing the most important divisions, and higher levels of collaboration, communication, and unity among the teams. Then Page also formed what the media called the \"L-Team\", a group of senior vice-presidents who reported directly to him and worked near his office for a portion of the working week.[61] Additionally, he reorganized the company's senior management, placing a CEO-like manager at the top of Google's most important product divisions, including YouTube, AdWords, and Google Search.[11]\n Following a more cohesive team environment, Page declared a new \"zero tolerance for fighting\" policy that contrasted with his approach during the early days of Google, when he would use his harsh and intense arguments with Brin as an exemplar for senior management. Page had changed his thinking during his time away from the CEO role, as he eventually concluded that ambitious goals required a harmonious team dynamic. As part of Page's collaborative rejuvenation process, Google's products and applications were consolidated and underwent an aesthetic overhaul.[56][62]\n At least 70 of Google's products, features and services were eventually shut down by March 2013, while the appearance and nature of the remaining ones were unified.[63][64] Jon Wiley, lead designer of Google Search at the time, codenamed Page's redesign overhaul, which officially commenced on April 4, 2011, \"Project Kennedy\", based on Page's use of the term \"moonshots\" to describe ambitious projects in a January 2013 Wired interview.[62][65] An initiative named \"Kanna\" previously attempted to create a uniform design aesthetic for Google's range of products, but it was too difficult at that point in the company's history for one team to drive such change. Matias Duarte, senior director of the Android user experience when \"Kennedy\" started, explained in 2013 that \"Google passionately cares about design.\" Page proceeded to consult with the Google Creative Lab design team, based in New York City, to find an answer to his question of what a \"cohesive vision\" of Google might look like.[62]\n The eventual results of \"Kennedy\" which were progressively rolled out from June 2011 until January 2013, were described by The Verge technology publication as focused upon \"refinement, white space, cleanliness, elasticity, usefulness, and most of all simplicity.\" The final products were aligned with Page's aim for a consistent suite of products that can \"move fast\", and \"Kennedy\" was called a \"design revolution\" by Duarte. Page's \"UXA\" (user/graphics interface) design team then emerged from the \"Kennedy\" project, tasked with \"designing and developing a true UI framework that transforms Google's application software into a beautiful, mature, accessible and consistent platform for its users.\" Unspoken of in public, the small UXA unit was designed to ensure that \"Kennedy\" became an \"institution.\"[62]\n When acquiring products and companies for Google, Page asked whether the business acquisition passed the toothbrush test as an initial qualifier, asking the question \"Is it something you will use once or twice a day, and does it make your life better?\". This approach looked for usefulness above profitability, and long-term potential over near-term financial gain, which has been noted as rare in business acquiring processes.[66][67][68]\n With Facebook's influence rapidly expanding during the start of Page's second tenure, he finally responded to the intensive competition with Google's own social network, Google+, in mid-2011. After several delays, the social network was released through a very limited field test and was led by Vic Gundotra, Google's then senior vice president of social.[69]\n In August 2011, Page announced that Google would spend $12.5 billion to acquire Motorola Mobility.[70] The purchase was primarily motivated by Google's need to secure patents to protect Android from lawsuits by companies including Apple Inc.[11] Page wrote on Google's official blog on August 15, 2011, that \"companies including Microsoft and Apple are banding together in anti-competitive patent attacks on Android. The United States Department of Justice had to intervene in the results of one recent patent auction to 'protect competition and innovation in the open source software community' [...] Our acquisition of Motorola will increase competition by strengthening Google's patent portfolio, which will enable us to better protect Android from anti-competitive threats from Microsoft, Apple and other companies\".[71][72] In 2014, Page sold Motorola Mobility for $2.9 billion to Personal Computer maker, Lenovo which represented a loss in value of $9.5 billion over two years.[73]\n Page also ventured into hardware and Google unveiled the Chromebook in May 2012. The hardware product was a laptop that ran on a Google operating system, ChromeOS.[74]\n \nIn January 2013, Page participated in a rare interview with Wired, in which writer Steven Levy discussed Page's \"10X\" mentality—Google employees are expected to create products and services that are at least 10 times better than those of its competitors—in the introductory blurb. Astro Teller, the head of Google X, explained to Levy that 10X is \"just core to who he [Page] is\", while Page's \"focus is on where the next 10X will come from.\"[65] In his interview with Levy, Page referred to the success of YouTube and Android as examples of \"crazy\" ideas that investors were not initially interested in, saying: \"If you're not doing some things that are crazy, then you're doing the wrong things.\"[65] Page also stated he was \"very happy\" with the status of Google+, and discussed concerns over the Internet concerning the SOPA bill and an International Telecommunication Union proposal that had been recently introduced: \"I do think the Internet's under much greater attack than it has been in the past. Governments are now afraid of the Internet because of the Middle East stuff, and so they're a little more willing to listen to what I see as a lot of commercial interests that just want to make money by restricting people's freedoms. But they've also seen a tremendous user reaction, like the backlash against SOPA. I think that governments fight users' freedoms at their peril.\"[65] At the May 2013 I/O developers conference in San Francisco, Page delivered a keynote address and said \"We're at maybe 1% of what is possible. Despite the faster change, we're still moving slow relative to the opportunities we have. I think a lot of that is because of the negativity [...] Every story I read is Google vs someone else. That's boring. We should be focusing on building the things that don't exist\" and that he was \"sad the Web isn't advancing as fast as it should be\", citing a perceived focus on negativity and zero-sum games among some in the technology sector as a cause.[75] In response to an audience question, Page noted an issue that Google had been experiencing with Microsoft, whereby the latter made its Outlook program interoperable with Google but did not allow for backward compatibility—he referred to Microsoft's practice as \"milking off\". During the question-and-answer section of his keynote, Page expressed interest in Burning Man, which Brin had previously praised—it was a motivating factor for the latter during Schmidt's hiring process, as Brin liked that Schmidt had attended the week-long annual event.[11][76][75]\n In September 2013, Page launched the independent Calico initiative, a R&D project in the field of biotechnology. Google announced that Calico seeks to innovate and make improvements in the field of human health, and appointed Art Levinson, chairman of Apple's board and former CEO of Genentech, to be the new division's CEO. Page's official statement read: \"Illness and aging affect all our families. With some longer term, moonshot thinking around healthcare and biotechnology, I believe we can improve millions of lives.\"[77]\n Page participated in a March 2014 TedX conference that was held in Vancouver, British Columbia, Canada. The presentation was scripted by Page's chief PR executive Rachel Whetstone, and Google's CMO Lorraine Twohill, and a demonstration of an artificially intelligent computer program was displayed on a large screen.[11]\n \nPage responded to a question about corporations, noting that corporations largely get a \"bad rap\", which he stated was because they were probably doing the same incremental things they were doing \"50 or 20 years ago\". He went on to juxtapose that kind of incremental approach to his vision of Google counteracting calcification through driving technology innovation at a high rate. Page mentioned Elon Musk and SpaceX: \"He [Musk] wants to go to Mars to back up humanity. That's a worthy goal. We have a lot of employees at Google who've become pretty wealthy. You're working because you want to change the world and make it better [...] I'd like for us to help out more than we are.\"[78] Page also mentioned Nikola Tesla with regard to invention and commercialization: \"Invention is not enough. [Nikola] Tesla invented the electric power we use, but he struggled to get it out to people. [You have to] combine both things []... invention and innovation focus, plus [...] a company that can really commercialize things and get them to people.\"[79] Page announced a major management restructure in October 2014 so that he would no longer need to be responsible for day-to-day product-related decision making. In a memo, Page said that Google's core businesses would be able to progress in a typical manner, while he could focus on the next generation of ambitious projects, including Google X initiatives; access and energy, including Google Fiber; smart-home automation through Nest Labs; and biotechnology innovations under Calico.[80] Page maintained that he would continue as the unofficial \"chief product officer\".[64] Subsequent to the announcement, the executives in charge of Google's core products reported to then Google Senior Vice President Sundar Pichai, who reported directly to Page.[80][81][82][83]\n In a November 2014 interview, Page stated that he prioritized the maintenance of his \"deep knowledge\" of Google's products and breadth of projects, as it had been a key motivating factor for team members. About his then role as the company's CEO, Page said: \"I think my job as CEO—I feel like it's always to be pushing people ahead.\"[64]\n \nOn August 10, 2015, Page announced on Google's official blog that Google had restructured into a number of subsidiaries of a new holding company known as Alphabet Inc with Page becoming CEO of Alphabet Inc and Sundar Pichai assuming the position of CEO of Google Inc. In his announcement, Page described the planned holding company as follows:[84] \"Alphabet is mostly a collection of companies. The largest of which, of course, is Google. This newer Google is a bit slimmed down, with the companies that are pretty far afield of our main Internet products contained in Alphabet instead. [...] Fundamentally, we believe this allows us more management scale, as we can run things independently that aren't very related.\" As well as explaining the origin of the company's name: \"We liked the name Alphabet because it means a collection of letters that represent language, one of humanity's most important innovations, and is the core of how we index with Google search! We also like that it means alpha‑bet (Alpha is investment return above benchmark), which we strive for!\" Page wrote that the motivation behind the reorganization is to make Google \"cleaner and more accountable.\" He also wrote that there was a desire to improve \"the transparency and oversight of what we're doing\" and to allow greater control of unrelated companies previously within the Google ecosystem.[84][85][86]\n Page has not been on any press conferences since 2015 and has not presented at product launches or earnings calls since 2013. The Bloomberg Businessweek termed the reorganization into Alphabet a clever retirement plan allowing Page to retain control over Google, at the same time relinquishing all responsibilities over it. Executives at Alphabet describe Page as a \"futurist\", highly detached from day-to-day business dealings, and more focused on moon-shot projects. While some managers of Alphabet companies speak of Page as intensely involved, others say that his rare office check-ins are \"akin to a royal visit\".[87]\n On December 3, 2019, Larry Page announced that he would step down from the position of Alphabet CEO and be replaced by Google CEO Sundar Pichai. Pichai also continued as Google CEO. Page and Google co-founder and Alphabet president Sergey Brin announced the change in a joint blog post, \"With Alphabet now well-established, and Google and the Other Bets operating effectively as independent companies, it's the natural time to simplify our management structure. We’ve never been ones to hold on to management roles when we think there's a better way to run the company. And Alphabet and Google no longer need two CEOs and a President.\"[88]\n Page is an investor in Tesla Motors co-founded by friend and fellow billionaire Elon Musk.[89] He has invested in renewable energy technology, and with the help of Google.org, Google's philanthropic arm, promotes the adoption of plug-in hybrid electric cars[90][91][92][93] and other alternative energy investments.[94] He also was a strategic backer in the Opener and Kitty Hawk[10] startups, developing aerial vehicles for consumer travel.[95] The company has ceased all activities. It was merged into the Wisk Aero joint venture with Boeing in September 2022.\n Page is interested in the socio-economic effects of advanced intelligent systems and how advanced digital technologies can be used to create abundance (as described in Peter Diamandis' book), provide for people's needs, shorten the workweek, and mitigate the potential detrimental effects of technological unemployment.[96][97]\n Page helped to set up Singularity University, a transhumanist think-tank.[98]\n In the early 2000s, Page briefly dated Marissa Mayer, American business leader and former CEO of Yahoo!, who was a Google employee at that time.[99][100]\n On February 18, 2005, Page bought a 9,000 square feet (840 m2) Spanish Colonial Revival architecture house in Palo Alto, California, designed by American artistic polymath Pedro Joseph de Lemos, a former curator of the Stanford Art Museum and founder of the Carmel Art Institute, after the historic building had been on the market for years with an asking price of US$7.95 million. A two-story stucco archway spans the driveway and the home features intricate stucco work, as well as stone and tile in California Arts and Crafts movement style built to resemble de Lemos's family's castle in Spain. The Pedro de Lemos House was constructed between 1931 and 1941 by de Lemos.[101][102][103][104][105] It is also on the National Register of Historic Places.[106]\n \nIn 2007, Page married Lucinda Southworth on Necker Island, the Caribbean island owned by Richard Branson.[107] Southworth is a research scientist and the sister of American actress and model Carrie Southworth.[108] Page and Southworth have two children, born in 2009 and 2011 respectively.[109][110] In 2009, Page began purchasing properties and tearing down homes adjacent to his home in Palo Alto to make room for a large ecohouse. The existing buildings were \"deconstructed\" and the materials donated for reuse. The ecohouse was designed to \"minimize the impact on the environment.\" Page worked with an arborist to replace some trees that were in poor health with others that used less water to maintain. Page also applied for Green Point Certification, with points given for use of recycled and low or no-VOC (volatile organic compound) materials and for a roof garden with solar panels. The house's exterior features zinc cladding and plenty of windows, including a wall of sliding-glass doors in the rear. It includes eco-friendly elements such as permeable paving in the parking court and a pervious path through the trees on the property. The 6,000 square feet (560 m2) house also observes other green home design features such as organic architecture building materials and low volatile organic compound paint.[111][112][113][114]\n In 2011, Page bought the $45-million 193-foot (59 m) superyacht Senses.[115] Later on, Page announced on his Google+ profile in May 2013 that his right vocal cord is paralyzed from a cold that he contracted the previous summer, while his left cord was paralyzed in 1999, and that the doctors were unable to identify the exact cause.[116] The Google+ post also revealed that Page had made a large donation to a vocal-cord nerve-function research program at the Voice Health Institute in Boston. An anonymous source stated that the donation exceeded $20 million.[117] In October 2013, Business Insider reported that Page's paralysis were caused by an autoimmune disease called Hashimoto's thyroiditis, and prevented him from undertaking Google quarterly earnings conference calls for an indefinite period.[118]\n In November 2014, Page's family foundation, the Carl Victor Page Memorial Fund, reportedly holding assets in excess of a billion dollars at the end of 2013, gave $15 million to aid the effort against the Ebola virus epidemic in West Africa. Page wrote on his Google+ page that \"My wife and I just donated $15 million [...] Our hearts go out to everyone affected.\"[119][120][121][122]\n In August 2021 it was revealed that Page holds a New Zealand resident's visa and had traveled to the country on a medivac flight from Fiji for his son's treatment in New Zealand. The flight took place on January 12, 2021. Page had been living in Fiji with his family during the duration of the COVID-19 pandemic.[123]\n In 2023, the US Virgin Islands tried several times to serve Page a subpoena in the lawsuit over JPMorgan Chase's links to Jeffrey Epstein.[124][125]\n Page has purchased multiple private islands across the Caribbean and South Pacific, including the Hans Lollik Island in 2014, Eustatia Island, Cayo Norte in 2018, and Tavarua in 2020.[126][127]\n A fictionalized version of Larry Page portrayed by actor Ben Feldman appeared in the Showtime drama series Super Pumped.[145]"}[
{"url": "https://en.wikipedia.org/wiki/Web_crawler", "title": "Web crawler - Wikipedia", "content": "A Web crawler, sometimes called a spider or spiderbot and often shortened to crawler, is an Internet bot that systematically browses the World Wide Web and that is typically operated by search engines for the purpose of Web indexing (web spidering).[1]\n Web search engines and some other websites use Web crawling or spidering software to update their web content or indices of other sites' web content. Web crawlers copy pages for processing by a search engine, which indexes the downloaded pages so that users can search more efficiently.\n Crawlers consume resources on visited systems and often visit sites unprompted. Issues of schedule, load, and \"politeness\" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For example, including a robots.txt file can request bots to index only parts of a website, or nothing at all.\n The number of Internet pages is extremely large; even the largest crawlers fall short of making a complete index. For this reason, search engines struggled to give relevant search results in the early years of the World Wide Web, before 2000. Today, relevant results are given almost instantly.\n Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping and data-driven programming.\n A web crawler is also known as a spider,[2] an ant, an automatic indexer,[3] or (in the FOAF software context) a Web scutter.[4]\n A Web crawler starts with a list of URLs to visit. Those first URLs are called the seeds. As the crawler visits these URLs, by communicating with web servers that respond to those URLs, it identifies all the hyperlinks in the retrieved web pages and adds them to the list of URLs to visit, called the crawl frontier. URLs from the frontier are recursively visited according to a set of policies. If the crawler is performing archiving of websites (or web archiving), it copies and saves the information as it goes. The archives are usually stored in such a way they can be viewed, read and navigated as if they were on the live web, but are preserved as 'snapshots'.[5]\n The archive is known as the repository and is designed to store and manage the collection of web pages. The repository only stores HTML pages and these pages are stored as distinct files. A repository is similar to any other system that stores data, like a modern-day database. The only difference is that a repository does not need all the functionality offered by a database system. The repository stores the most recent version of the web page retrieved by the crawler.[citation needed]\n The large volume implies the crawler can only download a limited number of the Web pages within a given time, so it needs to prioritize its downloads. The high rate of change can imply the pages might have already been updated or even deleted.\n The number of possible URLs crawled being generated by server-side software has also made it difficult for web crawlers to avoid retrieving duplicate content. Endless combinations of HTTP GET (URL-based) parameters exist, of which only a small selection will actually return unique content. For example, a simple online photo gallery may offer three options to users, as specified through HTTP GET parameters in the URL. If there exist four ways to sort images, three choices of thumbnail size, two file formats, and an option to disable user-provided content, then the same set of content can be accessed with 48 different URLs, all of which may be linked on the site. This mathematical combination creates a problem for crawlers, as they must sort through endless combinations of relatively minor scripted changes in order to retrieve unique content.\n As Edwards et al. noted, \"Given that the bandwidth for conducting crawls is neither infinite nor free, it is becoming essential to crawl the Web in not only a scalable, but efficient way, if some reasonable measure of quality or freshness is to be maintained.\"[6] A crawler must carefully choose at each step which pages to visit next.\n The behavior of a Web crawler is the outcome of a combination of policies:[7]\n Given the current size of the Web, even large search engines cover only a portion of the publicly available part. A 2009 study showed even large-scale search engines index no more than 40–70% of the indexable Web;[8] a previous study by Steve Lawrence and Lee Giles showed that no search engine indexed more than 16% of the Web in 1999.[9] As a crawler always downloads just a fraction of the Web pages, it is highly desirable for the downloaded fraction to contain the most relevant pages and not just a random sample of the Web.\n This requires a metric of importance for prioritizing Web pages. The importance of a page is a function of its intrinsic quality, its popularity in terms of links or visits, and even of its URL (the latter is the case of vertical search engines restricted to a single top-level domain, or search engines restricted to a fixed Web site). Designing a good selection policy has an added difficulty: it must work with partial information, as the complete set of Web pages is not known during crawling.\n Junghoo Cho et al. made the first study on policies for crawling scheduling. Their data set was a 180,000-pages crawl from the stanford.edu domain, in which a crawling simulation was done with different strategies.[10] The ordering metrics tested were breadth-first, backlink count and partial PageRank calculations. One of the conclusions was that if the crawler wants to download pages with high Pagerank early during the crawling process, then the partial Pagerank strategy is the better, followed by breadth-first and backlink-count. However, these results are for just a single domain. Cho also wrote his PhD dissertation at Stanford on web crawling.[11]\n Najork and Wiener performed an actual crawl on 328 million pages, using breadth-first ordering.[12] They found that a breadth-first crawl captures pages with high Pagerank early in the crawl (but they did not compare this strategy against other strategies). The explanation given by the authors for this result is that \"the most important pages have many links to them from numerous hosts, and those links will be found early, regardless of on which host or page the crawl originates.\"\n Abiteboul designed a crawling strategy based on an algorithm called OPIC (On-line Page Importance Computation).[13] In OPIC, each page is given an initial sum of \"cash\" that is distributed equally among the pages it points to. It is similar to a PageRank computation, but it is faster and is only done in one step. An OPIC-driven crawler downloads first the pages in the crawling frontier with higher amounts of \"cash\". Experiments were carried in a 100,000-pages synthetic graph with a power-law distribution of in-links. However, there was no comparison with other strategies nor experiments in the real Web.\n Boldi et al. used simulation on subsets of the Web of 40 million pages from the .it domain and 100 million pages from the WebBase crawl, testing breadth-first against depth-first, random ordering and an omniscient strategy. The comparison was based on how well PageRank computed on a partial crawl approximates the true PageRank value. Some visits that accumulate PageRank very quickly (most notably, breadth-first and the omniscient visit) provide very poor progressive approximations.[14][15]\n Baeza-Yates et al. used simulation on two subsets of the Web of 3 million pages from the .gr and .cl domain, testing several crawling strategies.[16] They showed that both the OPIC strategy and a strategy that uses the length of the per-site queues are better than breadth-first crawling, and that it is also very effective to use a previous crawl, when it is available, to guide the current one.\n Daneshpajouh et al. designed a community based algorithm for discovering good seeds.[17] Their method crawls web pages with high PageRank from different communities in less iteration in comparison with crawl starting from random seeds. One can extract good seed from a previously-crawled-Web graph using this new method. Using these seeds, a new crawl can be very effective.\n A crawler may only want to seek out HTML pages and avoid all other MIME types. In order to request only HTML resources, a crawler may make an HTTP HEAD request to determine a Web resource's MIME type before requesting the entire resource with a GET request. To avoid making numerous HEAD requests, a crawler may examine the URL and only request a resource if the URL ends with certain characters such as .html, .htm, .asp, .aspx, .php, .jsp, .jspx or a slash. This strategy may cause numerous HTML Web resources to be unintentionally skipped.\n Some crawlers may also avoid requesting any resources that have a \"?\" in them (are dynamically produced) in order to avoid spider traps that may cause the crawler to download an infinite number of URLs from a Web site. This strategy is unreliable if the site uses URL rewriting to simplify its URLs.\n Crawlers usually perform some type of URL normalization in order to avoid crawling the same resource more than once. The term URL normalization, also called URL canonicalization, refers to the process of modifying and standardizing a URL in a consistent manner. There are several types of normalization that may be performed including conversion of URLs to lowercase, removal of \".\" and \"..\" segments, and adding trailing slashes to the non-empty path component.[18]\n Some crawlers intend to download/upload as many resources as possible from a particular web site. So path-ascending crawler was introduced that would ascend to every path in each URL that it intends to crawl.[19] For example, when given a seed URL of http://llama.org/hamster/monkey/page.html, it will attempt to crawl /hamster/monkey/, /hamster/, and /. Cothey found that a path-ascending crawler was very effective in finding isolated resources, or resources for which no inbound link would have been found in regular crawling.\n The importance of a page for a crawler can also be expressed as a function of the similarity of a page to a given query. Web crawlers that attempt to download pages that are similar to each other are called focused crawler or topical crawlers. The concepts of topical and focused crawling were first introduced by Filippo Menczer[20][21] and by Soumen Chakrabarti et al.[22]\n The main problem in focused crawling is that in the context of a Web crawler, we would like to be able to predict the similarity of the text of a given page to the query before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton[23] in the first web crawler of the early days of the Web. Diligenti et al.[24] propose using the complete content of the pages already visited to infer the similarity between the driving query and the pages that have not been visited yet. The performance of a focused crawling depends mostly on the richness of links in the specific topic being searched, and a focused crawling usually relies on a general Web search engine for providing starting points.\n An example of the focused crawlers are academic crawlers, which crawls free-access academic related documents, such as the citeseerxbot, which is the crawler of CiteSeerX search engine. Other academic search engines are Google Scholar and Microsoft Academic Search etc. Because most academic papers are published in PDF formats, such kind of crawler is particularly interested in crawling PDF, PostScript files, Microsoft Word including their zipped formats. Because of this, general open-source crawlers, such as Heritrix, must be customized to filter out other MIME types, or a middleware is used to extract these documents out and import them to the focused crawl database and repository.[25] Identifying whether these documents are academic or not is challenging and can add a significant overhead to the crawling process, so this is performed as a post crawling process using machine learning or regular expression algorithms. These academic documents are usually obtained from home pages of faculties and students or from publication page of research institutes. Because academic documents make up only a small fraction of all web pages, a good seed selection is important in boosting the efficiencies of these web crawlers.[26] Other academic crawlers may download plain text and HTML files, that contains metadata of academic papers, such as titles, papers, and abstracts. This increases the overall number of papers, but a significant fraction may not provide free PDF downloads.\n Another type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes.[27] In addition, ontologies can be automatically updated in the crawling process. Dong et al.[28] introduced such an ontology-learning-based crawler using a support-vector machine to update the content of ontological concepts when crawling Web pages.\n The Web has a very dynamic nature, and crawling a fraction of the Web can take weeks or months. By the time a Web crawler has finished its crawl, many events could have happened, including creations, updates, and deletions.\n From the search engine's point of view, there is a cost associated with not detecting an event, and thus having an outdated copy of a resource. The most-used cost functions are freshness and age.[29]\n Freshness: This is a binary measure that indicates whether the local copy is accurate or not. The freshness of a page p in the repository at time t is defined as:\n Age: This is a measure that indicates how outdated the local copy is. The age of a page p in the repository, at time t is defined as:\n Coffman et al. worked with a definition of the objective of a Web crawler that is equivalent to freshness, but use a different wording: they propose that a crawler must minimize the fraction of time pages remain outdated. They also noted that the problem of Web crawling can be modeled as a multiple-queue, single-server polling system, on which the Web crawler is the server and the Web sites are the queues. Page modifications are the arrival of the customers, and switch-over times are the interval between page accesses to a single Web site. Under this model, mean waiting time for a customer in the polling system is equivalent to the average age for the Web crawler.[30]\n The objective of the crawler is to keep the average freshness of pages in its collection as high as possible, or to keep the average age of pages as low as possible. These objectives are not equivalent: in the first case, the crawler is just concerned with how many pages are outdated, while in the second case, the crawler is concerned with how old the local copies of pages are.\n Two simple re-visiting policies were studied by Cho and Garcia-Molina:[31]\n In both cases, the repeated crawling order of pages can be done either in a random or a fixed order.\n Cho and Garcia-Molina proved the surprising result that, in terms of average freshness, the uniform policy outperforms the proportional policy in both a simulated Web and a real Web crawl. Intuitively, the reasoning is that, as web crawlers have a limit to how many pages they can crawl in a given time frame, (1) they will allocate too many new crawls to rapidly changing pages at the expense of less frequently updating pages, and (2) the freshness of rapidly changing pages lasts for shorter period than that of less frequently changing pages. In other words, a proportional policy allocates more resources to crawling frequently updating pages, but experiences less overall freshness time from them.\n To improve freshness, the crawler should penalize the elements that change too often.[32] The optimal re-visiting policy is neither the uniform policy nor the proportional policy. The optimal method for keeping average freshness high includes ignoring the pages that change too often, and the optimal for keeping average age low is to use access frequencies that monotonically (and sub-linearly) increase with the rate of change of each page. In both cases, the optimal is closer to the uniform policy than to the proportional policy: as Coffman et al. note, \"in order to minimize the expected obsolescence time, the accesses to any particular page should be kept as evenly spaced as possible\".[30] Explicit formulas for the re-visit policy are not attainable in general, but they are obtained numerically, as they depend on the distribution of page changes. Cho and Garcia-Molina show that the exponential distribution is a good fit for describing page changes,[32] while Ipeirotis et al. show how to use statistical tools to discover parameters that affect this distribution.[33] The re-visiting policies considered here regard all pages as homogeneous in terms of quality (\"all pages on the Web are worth the same\"), something that is not a realistic scenario, so further information about the Web page quality should be included to achieve a better crawling policy.\n Crawlers can retrieve data much quicker and in greater depth than human searchers, so they can have a crippling impact on the performance of a site. If a single crawler is performing multiple requests per second and/or downloading large files, a server can have a hard time keeping up with requests from multiple crawlers.\n As noted by Koster, the use of Web crawlers is useful for a number of tasks, but comes with a price for the general community.[34] The costs of using Web crawlers include:\n A partial solution to these problems is the robots exclusion protocol, also known as the robots.txt protocol that is a standard for administrators to indicate which parts of their Web servers should not be accessed by crawlers.[35] This standard does not include a suggestion for the interval of visits to the same server, even though this interval is the most effective way of avoiding server overload. Recently commercial search engines like Google, Ask Jeeves, MSN and Yahoo! Search are able to use an extra \"Crawl-delay:\" parameter in the robots.txt file to indicate the number of seconds to delay between requests.\n The first proposed interval between successive pageloads was 60 seconds.[36] However, if pages were downloaded at this rate from a website with more than 100,000 pages over a perfect connection with zero latency and infinite bandwidth, it would take more than 2 months to download only that entire Web site; also, only a fraction of the resources from that Web server would be used.\n Cho uses 10 seconds as an interval for accesses,[31] and the WIRE crawler uses 15 seconds as the default.[37] The MercatorWeb crawler follows an adaptive politeness policy: if it took t seconds to download a document from a given server, the crawler waits for 10t seconds before downloading the next page.[38] Dill et al. use 1 second.[39]\n For those using Web crawlers for research purposes, a more detailed cost-benefit analysis is needed and ethical considerations should be taken into account when deciding where to crawl and how fast to crawl.[40]\n Anecdotal evidence from access logs shows that access intervals from known crawlers vary between 20 seconds and 3–4 minutes. It is worth noticing that even when being very polite, and taking all the safeguards to avoid overloading Web servers, some complaints from Web server administrators are received. Sergey Brin and Larry Page noted in 1998, \"... running a crawler which connects to more than half a million servers ... generates a fair amount of e-mail and phone calls. Because of the vast number of people coming on line, there are always those who do not know what a crawler is, because this is the first one they have seen.\"[41]\n A parallel crawler is a crawler that runs multiple processes in parallel. The goal is to maximize the download rate while minimizing the overhead from parallelization and to avoid repeated downloads of the same page. To avoid downloading the same page more than once, the crawling system requires a policy for assigning the new URLs discovered during the crawling process, as the same URL can be found by two different crawling processes.\n A crawler must not only have a good crawling strategy, as noted in the previous sections, but it should also have a highly optimized architecture.\n Shkapenyuk and Suel noted that:[42]\n While it is fairly easy to build a slow crawler that downloads a few pages per second for a short period of time, building a high-performance system that can download hundreds of millions of pages over several weeks presents a number of challenges in system design, I/O and network efficiency, and robustness and manageability. Web crawlers are a central part of search engines, and details on their algorithms and architecture are kept as business secrets. When crawler designs are published, there is often an important lack of detail that prevents others from reproducing the work. There are also emerging concerns about \"search engine spamming\", which prevent major search engines from publishing their ranking algorithms.\n While most of the website owners are keen to have their pages indexed as broadly as possible to have strong presence in search engines, web crawling can also have unintended consequences and lead to a compromise or data breach if a search engine indexes resources that should not be publicly available, or pages revealing potentially vulnerable versions of software.\n Apart from standard web application security recommendations website owners can reduce their exposure to opportunistic hacking by only allowing search engines to index the public parts of their websites (with robots.txt) and explicitly blocking them from indexing transactional parts (login pages, private pages, etc.).\n Web crawlers typically identify themselves to a Web server by using the User-agent field of an HTTP request. Web site administrators typically examine their Web servers' log and use the user agent field to determine which crawlers have visited the web server and how often. The user agent field may include a URL where the Web site administrator may find out more information about the crawler. Examining Web server log is tedious task, and therefore some administrators use tools to identify, track and verify Web crawlers. Spambots and other malicious Web crawlers are unlikely to place identifying information in the user agent field, or they may mask their identity as a browser or other well-known crawler.\n Web site administrators prefer Web crawlers to identify themselves so that they can contact the owner if needed. In some cases, crawlers may be accidentally trapped in a crawler trap or they may be overloading a Web server with requests, and the owner needs to stop the crawler. Identification is also useful for administrators that are interested in knowing when they may expect their Web pages to be indexed by a particular search engine.\n A vast amount of web pages lie in the deep or invisible web.[43] These pages are typically only accessible by submitting queries to a database, and regular crawlers are unable to find these pages if there are no links that point to them. Google's Sitemaps protocol and mod oai[44] are intended to allow discovery of these deep-Web resources.\n Deep web crawling also multiplies the number of web links to be crawled. Some crawlers only take some of the URLs in <a href=\"URL\"> form. In some cases, such as the Googlebot, Web crawling is done on all text contained inside the hypertext content, tags, or text.\n Strategic approaches may be taken to target deep Web content. With a technique called screen scraping, specialized software may be customized to automatically and repeatedly query a given Web form with the intention of aggregating the resulting data. Such software can be used to span multiple Web forms across multiple Websites. Data extracted from the results of one Web form submission can be taken and applied as input to another Web form thus establishing continuity across the Deep Web in a way not possible with traditional web crawlers.[45]\n Pages built on AJAX are among those causing problems to web crawlers. Google has proposed a format of AJAX calls that their bot can recognize and index.[46]\n There are a number of \"visual web scraper/crawler\" products available on the web which will crawl pages and structure data into columns and rows based on the users requirements. One of the main difference between a classic and a visual crawler is the level of programming ability required to set up a crawler. The latest generation of \"visual scrapers\" remove the majority of the programming skill needed to be able to program and start a crawl to scrape web data.\n The visual scraping/crawling method relies on the user \"teaching\" a piece of crawler technology, which then follows patterns in semi-structured data sources. The dominant method for teaching a visual crawler is by highlighting data in a browser and training columns and rows. While the technology is not new, for example it was the basis of Needlebase which has been bought by Google (as part of a larger acquisition of ITA Labs[47]), there is continued growth and investment in this area by investors and end-users.[citation needed]\n The following is a list of published crawler architectures for general-purpose crawlers (excluding focused web crawlers), with a brief description that includes the names given to the different components and outstanding features:\n The following web crawlers are available, for a price::"},
{"url": "https://en.wikipedia.org/wiki/Parallel_computing", "title": "Parallel computing - Wikipedia", "content": "Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously.[1] Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but has gained broader interest due to the physical constraints preventing frequency scaling.[2] As power consumption (and consequently heat generation) by computers has become a concern in recent years,[3] parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.[4]\n In computer science, parallelism and concurrency are two different things: a parallel program uses multiple CPU cores, each core performing a task independently. On the other hand, concurrency enables a program to deal with multiple tasks even on a single CPU core; the core switches between tasks (i.e. threads) without necessarily completing each one. A program can have both, neither or a combination of parallelism and concurrency characteristics.[5]\n Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.\n In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones,[6] because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting optimal parallel program performance.\n A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law, which states that it is limited by the fraction of time for which the parallelization can be utilised.\n Traditionally, computer software has been written for serial computation. To solve a problem, an algorithm is constructed and implemented as a serial stream of instructions. These instructions are executed on a central processing unit on one computer. Only one instruction may execute at a time—after that instruction is finished, the next one is executed.[7]\n Parallel computing, on the other hand, uses multiple processing elements simultaneously to solve a problem. This is accomplished by breaking the problem into independent parts so that each processing element can execute its part of the algorithm simultaneously with the others. The processing elements can be diverse and include resources such as a single computer with multiple processors, several networked computers, specialized hardware, or any combination of the above.[7] Historically parallel computing was used for scientific computing and the simulation of scientific problems, particularly in the natural and engineering sciences, such as meteorology. This led to the design of parallel hardware and software, as well as high performance computing.[8]\n Frequency scaling was the dominant reason for improvements in computer performance from the mid-1980s until 2004. The runtime of a program is equal to the number of instructions multiplied by the average time per instruction. Maintaining everything else constant, increasing the clock frequency decreases the average time it takes to execute an instruction. An increase in frequency thus decreases runtime for all compute-bound programs.[9] However, power consumption P by a chip is given by the equation P = C × V 2 × F, where C is the capacitance being switched per clock cycle (proportional to the number of transistors whose inputs change), V is voltage, and F is the processor frequency (cycles per second).[10] Increases in frequency increase the amount of power used in a processor. Increasing processor power consumption led ultimately to Intel's May 8, 2004 cancellation of its Tejas and Jayhawk processors, which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm.[11]\n To deal with the problem of power consumption and overheating the major central processing unit (CPU or processor) manufacturers started to produce power efficient processors with multiple cores. The core is the computing unit of the processor and in multi-core processors each core is independent and can access the same memory concurrently. Multi-core processors have brought parallel computing to desktop computers. Thus parallelization of serial programs has become a mainstream programming task. In 2012 quad-core processors became standard for desktop computers, while servers had 10+ core processors.  By 2023  some processors had over hundred cores.  Some designs having a mix of performance and efficiency cores (such as ARM's big.LITTLE design) due to thermal and design constraints.[12][citation needed]. From Moore's law it can be predicted that the number of cores per processor will double every 18–24 months.\n \nAn operating system can ensure that different tasks and user programs are run in parallel on the available cores. However, for a serial software program to take full advantage of the multi-core architecture the programmer needs to restructure and parallelize the code. A speed-up of application software runtime will no longer be achieved through frequency scaling, instead programmers will need to parallelize their software code to take advantage of the increasing computing power of multicore architectures.[13]\n Main article: Amdahl's law\n Optimally, the speedup from parallelization would be linear—doubling the number of processing elements should halve the runtime, and doubling it a second time should again halve the runtime. However, very few parallel algorithms achieve optimal speedup. Most of them have a near-linear speedup for small numbers of processing elements, which flattens out into a constant value for large numbers of processing elements.\n The maximum potential speedup of an overall system can be calculated by Amdahl's law. [14] Amdahl's Law indicates that optimal performance improvement is achieved by balancing enhancements to both parallelizable and non-parallelizable components of a task. Furthermore, it reveals that increasing the number of processors yields diminishing returns, with negligible speedup gains beyond a certain point. [15][16]\n Amdahl's Law has limitations, including assumptions of fixed workload, neglecting inter-process communication and synchronization overheads, primarily focusing on computational aspect and ignoring extrinsic factors such as data persistence, I/O operations, and memory access overheads. [17][18][19]\n Gustafson's law and Universal Scalability Law give a more realistic assessment of the parallel performance. [20][21] Understanding data dependencies is fundamental in implementing parallel algorithms. No program can run more quickly than the longest chain of dependent calculations (known as the critical path), since calculations that depend upon prior calculations in the chain must be executed in order. However, most algorithms do not consist of just a long chain of dependent calculations; there are usually opportunities to execute independent calculations in parallel.\n Let Pi and Pj be two program segments. Bernstein's conditions[22] describe when the two are independent and can be executed in parallel. For Pi, let Ii be all of the input variables and Oi the output variables, and likewise for Pj. Pi and Pj are independent if they satisfy\n Violation of the first condition introduces a flow dependency, corresponding to the first segment producing a result used by the second segment. The second condition represents an anti-dependency, when the second segment produces a variable needed by the first segment. The third and final condition represents an output dependency: when two segments write to the same location, the result comes from the logically last executed segment.[23]\n Consider the following functions, which demonstrate several kinds of dependencies:\n In this example, instruction 3 cannot be executed before (or even in parallel with) instruction 2, because instruction 3 uses a result from instruction 2. It violates condition 1, and thus introduces a flow dependency.\n In this example, there are no dependencies between the instructions, so they can all be run in parallel.\n Bernstein's conditions do not allow memory to be shared between different processes. For that, some means of enforcing an ordering between accesses is necessary, such as semaphores, barriers or some other synchronization method.\n Subtasks in a parallel program are often called threads. Some parallel computer architectures use smaller, lightweight versions of threads known as fibers, while others use bigger versions known as processes. However, \"threads\" is generally accepted as a generic term for subtasks.[24] Threads will often need synchronized access to an object or other resource, for example when they must update a variable that is shared between them. Without synchronization, the instructions between the two threads may be interleaved in any order. For example, consider the following program:\n If instruction 1B is executed between 1A and 3A, or if instruction 1A is executed between 1B and 3B, the program will produce incorrect data. This is known as a race condition. The programmer must use a lock to provide mutual exclusion. A lock is a programming language construct that allows one thread to take control of a variable and prevent other threads from reading or writing it, until that variable is unlocked. The thread holding the lock is free to execute its critical section (the section of a program that requires exclusive access to some variable), and to unlock the data when it is finished. Therefore, to guarantee correct program execution, the above program can be rewritten to use locks:\n One thread will successfully lock variable V, while the other thread will be locked out—unable to proceed until V is unlocked again. This guarantees correct execution of the program. Locks may be necessary to ensure correct program execution when threads must serialize access to resources, but their use can greatly slow a program and may affect its reliability.[25]\n Locking multiple variables using non-atomic locks introduces the possibility of program deadlock. An atomic lock locks multiple variables all at once. If it cannot lock all of them, it does not lock any of them. If two threads each need to lock the same two variables using non-atomic locks, it is possible that one thread will lock one of them and the second thread will lock the second variable. In such a case, neither thread can complete, and deadlock results.[26]\n Many parallel programs require that their subtasks act in synchrony. This requires the use of a barrier. Barriers are typically implemented using a lock or a semaphore.[27] One class of algorithms, known as lock-free and wait-free algorithms, altogether avoids the use of locks and barriers. However, this approach is generally difficult to implement and requires correctly designed data structures.[28]\n Not all parallelization results in speed-up. Generally, as a task is split up into more and more threads, those threads spend an ever-increasing portion of their time communicating with each other or waiting on each other for access to resources.[29][30] Once the overhead from resource contention or communication dominates the time spent on other computation, further parallelization (that is, splitting the workload over even more threads) increases rather than decreases the amount of time required to finish. This problem, known as parallel slowdown,[31] can be improved in some cases by software analysis and redesign.[32]\n Applications are often classified according to how often their subtasks need to synchronize or communicate with each other. An application exhibits fine-grained parallelism if its subtasks must communicate many times per second; it exhibits coarse-grained parallelism if they do not communicate many times per second, and it exhibits embarrassing parallelism if they rarely or never have to communicate. Embarrassingly parallel applications are considered the easiest to parallelize.\n Michael J. Flynn created one of the earliest classification systems for parallel (and sequential) computers and programs, now known as Flynn's taxonomy. Flynn classified programs and computers by whether they were operating using a single set or multiple sets of instructions, and whether or not those instructions were using a single set or multiple sets of data.\n The single-instruction-single-data (SISD) classification is equivalent to an entirely sequential program. The single-instruction-multiple-data (SIMD) classification is analogous to doing the same operation repeatedly over a large data set. This is commonly done in signal processing applications. Multiple-instruction-single-data (MISD) is a rarely used classification. While computer architectures to deal with this were devised (such as systolic arrays), few applications that fit this class materialized. Multiple-instruction-multiple-data (MIMD) programs are by far the most common type of parallel programs.\n According to David A. Patterson and John L. Hennessy, \"Some machines are hybrids of these categories, of course, but this classic model has survived because it is simple, easy to understand, and gives a good first approximation. It is also—perhaps because of its understandability—the most widely used scheme.\"[34]\n Parallel computing can incur significant overhead in practice, primarily due to the costs associated with merging data from multiple processes. Specifically, inter-process communication and synchronization can lead to overheads that are substantially higher—often by two or more orders of magnitude—compared to processing the same data on a single thread. [35][36][37] Therefore, the overall improvement should be carefully evaluated.\n From the advent of very-large-scale integration (VLSI) computer-chip fabrication technology in the 1970s until about 1986, speed-up in computer architecture was driven by doubling computer word size—the amount of information the processor can manipulate per cycle.[38] Increasing the word size reduces the number of instructions the processor must execute to perform an operation on variables whose sizes are greater than the length of the word. For example, where an 8-bit processor must add two 16-bit integers, the processor must first add the 8 lower-order bits from each integer using the standard addition instruction, then add the 8 higher-order bits using an add-with-carry instruction and the carry bit from the lower order addition; thus, an 8-bit processor requires two instructions to complete a single operation, where a 16-bit processor would be able to complete the operation with a single instruction.\n Historically, 4-bit microprocessors were replaced with 8-bit, then 16-bit, then 32-bit microprocessors. This trend generally came to an end with the introduction of 32-bit processors, which has been a standard in general-purpose computing for two decades. Not until the early 2000s, with the advent of x86-64 architectures, did 64-bit processors become commonplace.\n A computer program is, in essence, a stream of instructions executed by a processor. Without instruction-level parallelism, a processor can only issue less than one instruction per clock cycle (IPC < 1). These processors are known as subscalar processors. These instructions can be re-ordered and combined into groups which are then executed in parallel without changing the result of the program. This is known as instruction-level parallelism. Advances in instruction-level parallelism dominated computer architecture from the mid-1980s until the mid-1990s.[39]\n All modern processors have multi-stage instruction pipelines. Each stage in the pipeline corresponds to a different action the processor performs on that instruction in that stage; a processor with an N-stage pipeline can have up to N different instructions at different stages of completion and thus can issue one instruction per clock cycle (IPC = 1). These processors are known as scalar processors. The canonical example of a pipelined processor is a RISC processor, with five stages: instruction fetch (IF), instruction decode (ID), execute (EX), memory access (MEM), and register write back (WB). The Pentium 4 processor had a 35-stage pipeline.[40]\n Most modern processors also have multiple execution units. They usually combine this feature with pipelining and thus can issue more than one instruction per clock cycle (IPC > 1). These processors are known as superscalar processors. Superscalar processors differ from multi-core processors in that the several execution units are not entire processors (i.e. processing units). Instructions can be grouped together only if there is no data dependency between them. Scoreboarding and the Tomasulo algorithm (which is similar to scoreboarding but makes use of register renaming) are two of the most common techniques for implementing out-of-order execution and instruction-level parallelism.\n Task parallelisms is the characteristic of a parallel program that \"entirely different calculations can be performed on either the same or different sets of data\".[41] This contrasts with data parallelism, where the same calculation is performed on the same or different sets of data. Task parallelism involves the decomposition of a task into sub-tasks and then allocating each sub-task to a processor for execution. The processors would then execute these sub-tasks concurrently and often cooperatively. Task parallelism does not usually scale with the size of a problem.[42]\n Superword level parallelism is a vectorization technique based on loop unrolling and basic block vectorization. It is distinct from loop vectorization algorithms in that it can exploit parallelism of inline code, such as manipulating coordinates, color channels or in loops unrolled by hand.[43]\n Main memory in a parallel computer is either shared memory (shared between all processing elements in a single address space), or distributed memory (in which each processing element has its own local address space).[44] Distributed memory refers to the fact that the memory is logically distributed, but often implies that it is physically distributed as well. Distributed shared memory and memory virtualization combine the two approaches, where the processing element has its own local memory and access to the memory on non-local processors. Accesses to local memory are typically faster than accesses to non-local memory. On the supercomputers, distributed shared memory space can be implemented using the programming model such as PGAS.  This model allows processes on one compute node to transparently access the remote memory of another compute node. All compute nodes are also connected to an external shared memory system via high-speed interconnect, such as Infiniband, this external shared memory system is known as burst buffer, which is typically built from arrays of non-volatile memory physically distributed across multiple I/O nodes.\n Computer architectures in which each element of main memory can be accessed with equal latency and bandwidth are known as uniform memory access (UMA) systems. Typically, that can be achieved only by a shared memory system, in which the memory is not physically distributed. A system that does not have this property is known as a non-uniform memory access (NUMA) architecture. Distributed memory systems have non-uniform memory access.\n Computer systems make use of caches—small and fast memories located close to the processor which store temporary copies of memory values (nearby in both the physical and logical sense). Parallel computer systems have difficulties with caches that may store the same value in more than one location, with the possibility of incorrect program execution. These computers require a cache coherency system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. Bus snooping is one of the most common methods for keeping track of which values are being accessed (and thus should be purged). Designing large, high-performance cache coherence systems is a very difficult problem in computer architecture. As a result, shared memory computer architectures do not scale as well as distributed memory systems do.[44]\n Processor–processor and processor–memory communication can be implemented in hardware in several ways, including via shared (either multiported or multiplexed) memory, a crossbar switch, a shared bus or an interconnect network of a myriad of topologies including star, ring, tree, hypercube, fat hypercube (a hypercube with more than one processor at a node), or n-dimensional mesh.\n Parallel computers based on interconnected networks need to have some kind of routing to enable the passing of messages between nodes that are not directly connected. The medium used for communication between the processors is likely to be hierarchical in large multiprocessor machines.\n Parallel computers can be roughly classified according to the level at which the hardware supports parallelism. This classification is broadly analogous to the distance between basic computing nodes. These are not mutually exclusive; for example, clusters of symmetric multiprocessors are relatively common.\n A multi-core processor is a processor that includes multiple processing units (called \"cores\") on the same chip. This processor differs from a superscalar processor, which includes multiple execution units and can issue multiple instructions per clock cycle from one instruction stream (thread); in contrast, a multi-core processor can issue multiple instructions per clock cycle from multiple instruction streams. IBM's Cell microprocessor, designed for use in the Sony PlayStation 3, is a prominent multi-core processor. Each core in a multi-core processor can potentially be superscalar as well—that is, on every clock cycle, each core can issue multiple instructions from one thread.\n Simultaneous multithreading  (of which Intel's Hyper-Threading is the best known) was an early form of pseudo-multi-coreism. A processor capable of concurrent multithreading includes multiple execution units in the same processing unit—that is it has a superscalar architecture—and can issue multiple instructions per clock cycle from multiple threads. Temporal multithreading on the other hand includes a single execution unit in the same processing unit and can issue one instruction at a time from multiple threads.\n A symmetric multiprocessor (SMP) is a computer system with multiple identical processors that share memory and connect via a bus.[45] Bus contention prevents bus architectures from scaling. As a result, SMPs generally do not comprise more than 32 processors.[46] Because of the small size of the processors and the significant reduction in the requirements for bus bandwidth achieved by large caches, such symmetric multiprocessors are extremely cost-effective, provided that a sufficient amount of memory bandwidth exists.[45]\n A distributed computer (also known as a distributed memory multiprocessor) is a distributed memory computer system in which the processing elements are connected by a network. Distributed computers are highly scalable. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have a lot of overlap, and no clear distinction exists between them.[47] The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel.[48]\n A cluster is a group of loosely coupled computers that work together closely, so that in some respects they can be regarded as a single computer.[49] Clusters are composed of multiple standalone machines connected by a network. While machines in a cluster do not have to be symmetric, load balancing is more difficult if they are not. The most common type of cluster is the Beowulf cluster, which is a cluster implemented on multiple identical commercial off-the-shelf computers connected with a TCP/IP Ethernet local area network.[50] Beowulf technology was originally developed by Thomas Sterling and Donald Becker. 87% of all Top500 supercomputers are clusters.[51] The remaining are Massively Parallel Processors, explained below.\n Because grid computing systems (described below) can easily handle embarrassingly parallel problems, modern clusters are typically designed to handle more difficult problems—problems that require nodes to share intermediate results with each other more often. This requires a high bandwidth and, more importantly, a low-latency interconnection network. Many historic and current supercomputers use customized high-performance network hardware specifically designed for cluster computing, such as the Cray Gemini network.[52] As of 2014, most current supercomputers use some off-the-shelf standard network hardware, often Myrinet, InfiniBand, or Gigabit Ethernet.\n A massively parallel processor (MPP) is a single computer with many networked processors. MPPs have many of the same characteristics as clusters, but MPPs have specialized interconnect networks (whereas clusters use commodity hardware for networking). MPPs also tend to be larger than clusters, typically having \"far more\" than 100 processors.[53] In an MPP, \"each CPU contains its own memory and copy of the operating system and application. Each subsystem communicates with the others via a high-speed interconnect.\"[54]\n IBM's Blue Gene/L, the fifth fastest supercomputer in the world according to the June 2009 TOP500 ranking, is an MPP.\n Grid computing is the most distributed form of parallel computing. It makes use of computers communicating over the Internet to work on a given problem. Because of the low bandwidth and extremely high latency available on the Internet, distributed computing typically deals only with embarrassingly parallel problems.\n Most grid computing applications use middleware (software that sits between the operating system and the application to manage network resources and standardize the software interface). The most common grid computing middleware is the Berkeley Open Infrastructure for Network Computing (BOINC). Often volunteer computing software makes use of \"spare cycles\", performing computations at times when a computer is idling.[55]\n The ubiquity of Internet brought the possibility of large-scale cloud computing.\n Within parallel computing, there are specialized parallel devices that remain niche areas of interest. While not domain-specific, they tend to be applicable to only a few classes of parallel problems.\n Reconfigurable computing is the use of a field-programmable gate array (FPGA) as a co-processor to a general-purpose computer. An FPGA is, in essence, a computer chip that can rewire itself for a given task.\n FPGAs can be programmed with hardware description languages such as VHDL[56] or Verilog.[57] Several vendors have created C to HDL languages that attempt to emulate the syntax and semantics of the C programming language, with which most programmers are familiar. The best known C to HDL languages are Mitrion-C, Impulse C, and Handel-C. Specific subsets of SystemC based on C++ can also be used for this purpose.\n AMD's decision to open its HyperTransport technology to third-party vendors has become the enabling technology for high-performance reconfigurable computing.[58] According to Michael R. D'Amour, Chief Operating Officer of DRC Computer Corporation, \"when we first walked into AMD, they called us 'the socket stealers.' Now they call us their partners.\"[58]\n General-purpose computing on graphics processing units (GPGPU) is a fairly recent trend in computer engineering research. GPUs are co-processors that have been heavily optimized for computer graphics processing.[59] Computer graphics processing is a field dominated by data parallel operations—particularly linear algebra matrix operations.\n In the early days, GPGPU programs used the normal graphics APIs for executing programs. However, several new programming languages and platforms have been built to do general purpose computation on GPUs with both Nvidia and AMD releasing programming environments with CUDA and Stream SDK respectively. Other GPU programming languages include BrookGPU, PeakStream, and RapidMind. Nvidia has also released specific products for computation in their Tesla series. The technology consortium Khronos Group has released the OpenCL specification, which is a framework for writing programs that execute across platforms consisting of CPUs and GPUs. AMD, Apple, Intel, Nvidia and others are supporting OpenCL.\n Several application-specific integrated circuit (ASIC) approaches have been devised for dealing with parallel applications.[60][61][62]\n Because an ASIC is (by definition) specific to a given application, it can be fully optimized for that application. As a result, for a given application, an ASIC tends to outperform a general-purpose computer. However, ASICs are created by UV photolithography. This process requires a mask set, which can be extremely expensive. A mask set can cost over a million US dollars.[63] (The smaller the transistors required for the chip, the more expensive the mask will be.) Meanwhile, performance increases in general-purpose computing over time (as described by Moore's law) tend to wipe out these gains in only one or two chip generations.[58] High initial cost, and the tendency to be overtaken by Moore's-law-driven general-purpose computing, has rendered ASICs unfeasible for most parallel computing applications. However, some have been built. One example is the PFLOPS RIKEN MDGRAPE-3 machine which uses custom ASICs for molecular dynamics simulation.\n A vector processor is a CPU or computer system that can execute the same instruction on large sets of data. Vector processors have high-level operations that work on linear arrays of numbers or vectors. An example vector operation is A = B × C, where A, B, and C are each 64-element vectors of 64-bit floating-point numbers.[64] They are closely related to Flynn's SIMD classification.[64]\n Cray computers became famous for their vector-processing computers in the 1970s and 1980s. However, vector processors—both as CPUs and as full computer systems—have generally disappeared. Modern processor instruction sets do include some vector processing instructions, such as with Freescale Semiconductor's AltiVec and Intel's Streaming SIMD Extensions (SSE).\n Concurrent programming languages, libraries, APIs, and parallel programming models (such as algorithmic skeletons) have been created for programming parallel computers. These can generally be divided into classes based on the assumptions they make about the underlying memory architecture—shared memory, distributed memory, or shared distributed memory. Shared memory programming languages communicate by manipulating shared memory variables. Distributed memory uses message passing. POSIX Threads and OpenMP are two of the most widely used shared memory APIs, whereas Message Passing Interface (MPI) is the most widely used message-passing system API.[65] One concept used in programming parallel programs is the future concept, where one part of a program promises to deliver a required datum to another part of a program at some future time.\n Efforts to standardize parallel programming include an open standard called OpenHMPP for hybrid multi-core parallel programming. The OpenHMPP directive-based programming model offers a syntax to efficiently offload computations on hardware accelerators and to optimize data movement to/from the hardware memory using remote procedure calls.\n The rise of consumer GPUs has led to support for compute kernels, either in graphics  APIs (referred to as compute shaders), in dedicated APIs (such as OpenCL), or in other language extensions.\n Automatic parallelization of a sequential program by a compiler is the \"holy grail\" of parallel computing, especially with the aforementioned limit of processor frequency. Despite decades of work by compiler researchers, automatic parallelization has had only limited success.[66]\n Mainstream parallel programming languages remain either explicitly parallel or (at best) partially implicit, in which a programmer gives the compiler directives for parallelization. A few fully implicit parallel programming languages exist—SISAL, Parallel Haskell, SequenceL, System C (for FPGAs), Mitrion-C, VHDL, and Verilog.\n As a computer system grows in complexity, the mean time between failures usually decreases. Application checkpointing is a technique whereby the computer system takes a \"snapshot\" of the application—a record of all current resource allocations and variable states, akin to a core dump—; this information can be used to restore the program if the computer should fail. Application checkpointing means that the program has to restart from only its last checkpoint rather than the beginning. While checkpointing provides benefits in a variety of situations, it is especially useful in highly parallel systems with a large number of processors used in high performance computing.[67]\n As parallel computers become larger and faster, we are now able to solve problems that had previously taken too long to run. Fields as varied as bioinformatics (for protein folding and sequence analysis) and economics have taken advantage of parallel computing. Common types of problems in parallel computing applications include:[68]\n Parallel computing can also be applied to the design of fault-tolerant computer systems, particularly via lockstep systems performing the same operation in parallel. This provides redundancy in case one component fails, and also allows automatic error detection and error correction if the results differ. These methods can be used to help prevent single-event upsets caused by transient errors.[70] Although additional measures may be required in embedded or specialized systems, this method can provide a cost-effective approach to achieve n-modular redundancy in commercial off-the-shelf systems.\n The origins of true (MIMD) parallelism go back to Luigi Federico Menabrea and his Sketch of the Analytic Engine Invented by Charles Babbage.[72][73][74]\n In 1957, Compagnie des Machines Bull announced the first computer architecture specifically designed for parallelism, the Gamma 60.[75] It utilized a fork-join model and a \"Program Distributor\" to dispatch and collect data to and from independent processing units connected to a central memory.[76][77]\n In April 1958, Stanley Gill (Ferranti) discussed parallel programming and the need for branching and waiting.[78] Also in 1958, IBM researchers John Cocke and Daniel Slotnick discussed the use of parallelism in numerical calculations for the first time.[79] Burroughs Corporation introduced the D825 in 1962, a four-processor computer that accessed up to 16 memory modules through a crossbar switch.[80] In 1967, Amdahl and Slotnick published a debate about the feasibility of parallel processing at American Federation of Information Processing Societies Conference.[79] It was during this debate that Amdahl's law was coined to define the limit of speed-up due to parallelism.\n In 1969, Honeywell introduced its first Multics system, a symmetric multiprocessor system capable of running up to eight processors in parallel.[79] C.mmp, a multi-processor project at Carnegie Mellon University in the 1970s, was among the first multiprocessors with more than a few processors. The first bus-connected multiprocessor with snooping caches was the Synapse N+1 in 1984.[73]\n SIMD parallel computers can be traced back to the 1970s. The motivation behind early SIMD computers was to amortize the gate delay of the processor's control unit over multiple instructions.[81] In 1964, Slotnick had proposed building a massively parallel computer for the Lawrence Livermore National Laboratory.[79] His design was funded by the US Air Force, which was the earliest SIMD parallel-computing effort, ILLIAC IV.[79] The key to its design was a fairly high parallelism, with up to 256 processors, which allowed the machine to work on large datasets in what would later be known as vector processing. However, ILLIAC IV was called \"the most infamous of supercomputers\", because the project was only one-fourth completed, but took 11 years and cost almost four times the original estimate.[71] When it was finally ready to run its first real application in 1976, it was outperformed by existing commercial supercomputers such as the Cray-1.\n In the early 1970s, at the MIT Computer Science and Artificial Intelligence Laboratory, Marvin Minsky and Seymour Papert started developing the Society of Mind theory, which views the biological brain as massively parallel computer. In 1986, Minsky published The Society of Mind, which claims that \"mind is formed from many little agents, each mindless by itself\".[82] The theory attempts to explain how what we call intelligence could be a product of the interaction of non-intelligent parts. Minsky says that the biggest source of ideas about the theory came from his work in trying to create a machine that uses a robotic arm, a video camera, and a computer to build with children's blocks.[83]\n Similar models (which also view the biological brain as a massively parallel computer, i.e., the brain is made up of a constellation of independent or semi-independent agents) were also described by:"},
{"url": "https://en.wikipedia.org/wiki/Distributed_web_crawling", "title": "Distributed web crawling - Wikipedia", "content": "Distributed web crawling is a distributed computing technique whereby Internet search engines employ many computers to index the Internet via web crawling. Such systems may allow for users to voluntarily offer their own computing and bandwidth resources towards crawling web pages. By spreading the load of these tasks across  many computers, costs that would otherwise be spent on maintaining large computing clusters are avoided.\n Cho[1] and Garcia-Molina studied two types of policies:\n With this type of policy, a central server assigns new URLs to different crawlers dynamically. This allows the central server to, for instance, dynamically balance the load of each crawler.[2]\n With dynamic assignment, typically the systems can also add or remove downloader processes. The central server may become the bottleneck, so most of the workload must be transferred to the distributed crawling processes for large crawls.\n There are two configurations of crawling architectures with dynamic assignments that have been described by Shkapenyuk and Suel:[3]\n With this type of policy, there is a fixed rule stated from the beginning of the crawl that defines how to assign new URLs to the crawlers.\n For static assignment, a hashing function can be used to transform URLs (or, even better, complete website names) into a number that corresponds to the index of the corresponding crawling process.[4] As there are external links that will go from a Web site assigned to one crawling process to a website assigned to a different crawling process, some exchange of URLs must occur.\n To reduce the overhead due to the exchange of URLs between crawling processes, the exchange should be done in batch, several URLs at a time, and the most cited URLs in the collection should be known by all crawling processes before the crawl (e.g.: using data from a previous crawl).[1]\n As of 2003, most modern commercial search engines use this technique. Google and Yahoo use thousands of individual computers to crawl the Web.\n Newer projects are attempting to use a less structured, more ad hoc form of collaboration by enlisting volunteers to join the effort using, in many cases, their home or personal computers. LookSmart is the largest search engine to use this technique, which powers its Grub distributed web-crawling project. Wikia (now known as Fandom) acquired Grub from LookSmart in 2007.[5]\n This solution uses computers that are connected to the Internet to crawl Internet addresses in the background. Upon downloading crawled web pages, they are compressed and sent back, together with a status flag (e.g. changed, new, down, redirected) to the powerful central servers. The servers, which manage a large database, send out new URLs to clients for testing.\n According to the FAQ about Nutch, an open-source search engine website, the savings in bandwidth by distributed web crawling are not significant, since \"A successful search engine requires more bandwidth to upload query result pages than its crawler needs to download pages...\".[6]"},
{"url": "https://en.wikipedia.org/wiki/Larry_Page", "title": "Larry Page - Wikipedia", "content": "Lawrence Edward Page[2][3][4] (born March 26, 1973) is an American businessman, computer engineer and computer scientist best known for co-founding Google with Sergey Brin.[2][5]\n Page was chief executive officer of Google from 1997 until August 2001 when he stepped down in favor of Eric Schmidt, and then again from April 2011 until July 2015 when he became CEO of its newly formed parent organization Alphabet Inc.[6] He held that post until December 4, 2019, when he and Brin stepped down from all executive positions and day-to-day roles within the company. He remains an Alphabet board member, employee, and controlling shareholder.[7]\n Page has an estimated net worth of $175 billion as of December  2024, according to the Bloomberg Billionaires Index, and $162.2 billion according to Forbes, making him the sixth-richest person in the world.[8][9] He has also invested in flying car startups Kitty Hawk and Opener.[10]\n Page is the co-creator and namesake of PageRank, a search ranking algorithm for Google[18] for which he received the Marconi Prize in 2004 along with co-writer Brin.[19]\n Lawrence Edward Page was born on March 26, 1973,[20] in Lansing, Michigan.[21][22] His mother is Jewish;[23] his maternal grandfather later immigrated to Israel,[22] though Page's household while growing up was secular.[23][24] His father, Carl Victor Page Sr., earned a PhD in computer science from the University of Michigan. BBC reporter Will Smale described him as a \"pioneer in computer science and artificial intelligence\".[25] Page's paternal grandparents came from a Protestant background.[citation needed] Page's father was a computer science professor at Michigan State University and his mother Gloria was an instructor in computer programming at Lyman Briggs College at the same institution.[26][25][27] Larry's parents divorced when he was eight years old, but he maintained a good relationship both with his mother Gloria and his father's long-term partner and MSU professor Joyce Wildenthal.[28]: ch. 2 \n When Larry Page was six years old, in 1979, his father brought home an Exidy Sorcerer computer, which Larry soon mastered and began using for schoolwork.[29]\n During an interview, Page recalled his childhood home \"was usually a mess, with computers, science, and technology magazines and Popular Science magazines all over the place\", an environment in which he immersed himself.[30] Page was an avid reader during his youth, writing in his 2013 Google founders letter: \"I remember spending a huge amount of time pouring [sic] over books and magazines\".[31] According to writer Nicholas Carlson, the combined influence of Page's home atmosphere and his attentive parents \"fostered creativity and invention\". Page also played instruments and studied music composition while growing up. His parents sent him to music summer camp—Interlochen Arts Camp in Interlochen, Michigan, and Page has mentioned that his musical education inspired his impatience and obsession with speed in computing. \"In some sense, I feel like music training led to the high-speed legacy of Google for me\". In an interview Page said that \"In music, you're very cognizant of time. Time is like the primary thing\" and that \"If you think about it from a music point of view, if you're a percussionist, you hit something, it's got to happen in milliseconds, fractions of a second\".[11]\n Page was first attracted to computers when he was six years old, as he was able to \"play with the stuff lying around\"—first-generation personal computers—that had been left by his mother and father.[26] He became the \"first kid in his elementary school to turn in an assignment from a word processor\".[32] His older brother Carl Victor Page Jr.[28]: ch. 2  also taught him to take things apart and before long he was taking \"everything in his house apart to see how it worked\". He said that \"from a very early age, I also realized I wanted to invent things. So I became interested in technology and business. Probably from when I was 12, I knew I was going to start a company eventually.\"[32]\n Page attended Okemos Montessori School (now called Montessori Radmoor) in Okemos, Michigan, from ages 2 to 7 (1975 to 1979). He attended East Lansing High School, graduating in 1991. In summer school, he attended Interlochen Center for the Arts at Interlochen, Michigan, playing flute but mainly saxophone for two summers.\n Page received a Bachelor of Science with a major in computer engineering with honors from the University of Michigan in 1995 and a Master of Science in computer science from Stanford University in 1998.[33][34][35][36]\n While at the University of Michigan, Page created an inkjet printer made of Lego bricks (literally a line plotter), after he thought it possible to print large posters cheaply with the use of inkjet cartridges—Page reverse-engineered the ink cartridge and built the electronics and mechanics to drive it.[26] Page served as the president of the Beta Epsilon chapter of the Eta Kappa Nu honor society,[37] and was a member of the 1993 \"Maize & Blue\" University of Michigan Solar Car team.[38] As an undergraduate at the University of Michigan, he proposed that the school replace its bus system with a personal rapid-transit system, which is essentially a driverless monorail with separate cars for every passenger.[11] He also developed a business plan for a company that would use software to build a music synthesizer during this time.[39]\n After enrolling in a computer science PhD program at Stanford University, Page was in search of a dissertation theme and considered exploring the mathematical properties of the World Wide Web, understanding its link structure as a huge graph. His supervisor, Terry Winograd, encouraged him to pursue the idea, and Page recalled in 2008 that it was the best advice he had ever received.[40] He also considered doing research on telepresence and self-driving cars during this time.[41][42][43][44]\n Page focused on the problem of finding out which web pages linked to a given page, considering the number and nature of such backlinks as valuable information for that page. The role of citations in academic publishing would also become pertinent for the research.[44] Sergey Brin, a fellow Stanford PhD student, would soon join Page's research project, nicknamed \"BackRub.\"[44] Together, the pair authored a research paper titled \"The Anatomy of a Large-Scale Hypertextual Web Search Engine\", which became one of the most downloaded scientific documents in the history of the Internet at the time.[26][42]\n John Battelle, co-founder of Wired magazine, wrote that Page had reasoned that:\n \"[the] entire Web was loosely based on the premise of citation—after all, what is a link but a citation? If he could devise a method to count and qualify each backlink on the Web, as Page puts it \"the Web would become a more valuable place.\"\"[44] Battelle further described how Page and Brin began working together on the project:\n \"At the time Page conceived of BackRub, the Web comprised an estimated 10 million documents, with an untold number of links between them. The computing resources required to crawl such a beast were well beyond the usual bounds of a student project. Unaware of exactly what he was getting into, Page began building out his crawler. The idea's complexity and scale lured Brin to the job. A polymath who had jumped from project to project without settling on a thesis topic, he found the premise behind BackRub fascinating. \"I talked to lots of research groups\" around the school, Brin recalls, \"and this was the most exciting project, both because it tackled the Web, which represents human knowledge, and because I liked Larry.\"\"[44] To convert the backlink data gathered by BackRub's web crawler into a measure of importance for a given web page, Brin and Page developed the PageRank algorithm, and realized that it could be used to build a search engine far superior to existing ones.[44] The algorithm relied on a new technology that analyzed the relevance of the backlinks that connected one web page to another.[45]\n Combining their ideas, the pair began utilizing Page's dormitory room as a machine laboratory, and extracted spare parts from inexpensive computers to create a device that they used to connect the now nascent search engine with Stanford's broadband campus network.[44] After filling Page's room with equipment, they then converted Brin's dorm room into an office and programming center, where they tested their new search engine designs on the Web. The rapid growth of their project caused Stanford's computing infrastructure to experience problems.[46]\n Page and Brin used the former's basic HTML programming skills to set up a simple search page for users, as they did not have a web page developer to create anything visually elaborate. They also began using any computer part they could find to assemble the necessary computing power to handle searches by multiple users. As their search engine grew in popularity among Stanford users, it required additional servers to process the queries. In August 1996, the initial version of Google, still on the Stanford University website, was made available to Internet users.[44]\n By early 1997, the BackRub page described the state as follows:\n \"Some Rough Statistics (from August 29, 1996)\n Total indexable HTML URLs: 75.2306 Million\n Total content downloaded: 207.022 gigabytes\n ...\n \nBackRub is written in Java and Python and runs on several Sun Ultras and Intel Pentiums running Linux. The primary database is kept on a Sun Ultra series II with 28GB of a disk. Scott Hassan and Alan Steremberg have provided a great deal of very talented implementation help. Sergey Brin has also been very involved and deserves many thanks.\" BackRub already exhibited the rudimentary functions and characteristics of a search engine: a query input was entered and it provided a list of backlinks ranked by importance. Page recalled: \"We realized that we had a querying tool. It gave you a good overall ranking of pages and ordering of follow-up pages.\"[48] Page said that in mid-1998 they finally realized the further potential of their project: \"Pretty soon, we had 10,000 searches a day. And we figured, maybe this is real.\"[46]\n Page and Brin's vision has been compared to that of Johannes Gutenberg, the inventor of modern printing:[49]\n \"In 1440, Johannes Gutenberg introduced Europe to the mechanical printing press, printing Bibles for mass consumption. The technology allowed for books and manuscripts – originally replicated by hand – to be printed at a much faster rate, thus spreading knowledge and helping to usher in the European Renaissance [...] Google has done a similar job.\" The comparison was also noted by the authors of The Google Story: \"Not since Gutenberg [...] has any new invention empowered individuals, and transformed access to information, as profoundly as Google.\"[28] Also, not long after the two \"cooked up their new engine for web searches, they began thinking about information that was at the time beyond the web\" such as digitizing books and expanding health information.[46]\n \nMark Malseed wrote in a 2003 feature story: \"Soliciting funds from faculty members, family and friends, Brin and Page scraped together enough to buy some servers and rent that famous garage in Menlo Park. [Soon after], Sun Microsystems co-founder Andy Bechtolsheim wrote a $100,000 cheque to \"Google, Inc.\" The only problem was, \"Google, Inc.\" did not yet exist—the company hadn't yet been incorporated. For two weeks, as they handled the paperwork, the young men had nowhere to deposit the money.\"[50] In 1998,[51] Brin and Page incorporated Google, Inc.[52] with the initial domain name of \"Googol\", derived from a number that consists of one followed by one hundred zeros representing the vast amount of data that the search engine was intended to explore. Using the garage in their friend Susan Wojcicki's Menlo Park home for $1,700 a month, Page and Brin were able to successfully build the Google search engine.[53] Following inception, Page appointed himself as CEO, while Brin, named Google's co-founder, was Google's president.[11] Writer Nicholas Carlson wrote in 2014: \"The pair's mission was 'to organize the world's information and make it universally accessible and useful.\"[54] With a US$1-million loan from friends and family, the inaugural team moved into a Mountain View office by the start of 2000. In 1999, Page experimented with smaller servers so Google could fit more into each square meter of the third-party warehouses the company rented for their servers. This eventually led to a search engine that ran much faster than Google's competitors at the time.\"[11] By June 2000, Google had indexed one billion Internet URLs (Uniform Resource Locators), making it the most comprehensive search engine on the Web at the time. The company cited NEC Research Institute data in its June 26 press release, stating that \"there are more than 1 billion web pages online today\", with Google \"providing access to 560 million full-text indexed web pages and 500 million partially indexed URLs.\"[55]\n During his first tenure as CEO, Page embarked on an attempt to fire all of Google's project managers in 2001. Page's plan involved all of Google's engineers reporting to a VP of engineering, who would then report directly to him—Page explained that he did not like non-engineers supervising engineers due to their limited technical knowledge.[11] Page even documented his management tenets for his team to use as a reference:\n Even though Page's new model was unsustainable and led to disgruntlement among the affected employees, his issue with engineers being managed by non-engineering staff gained traction.[56] Page also believed that the faster Google's search engine returned answers, the more it would be used. He fretted over milliseconds and pushed his engineers—from those who developed algorithms to those who built data centers—to think about lag times. He also pushed for keeping Google's home page famously sparse in its design because it would help the page load faster.[39]\n Before Silicon Valley's two most prominent investors, Kleiner Perkins and Sequoia Capital, agreed to invest a combined total of $50 million in Google, they applied pressure on Page to step down as CEO so that a more experienced leader could build a \"world-class management team.\" Page eventually became amenable to the idea after meeting with other technology CEOs, including Steve Jobs and Intel's Andrew Grove. Eric Schmidt, who had been hired as chairman of Google in March 2001, left his full-time position as the CEO of Novell to take the same role at Google in August of the same year, and Page moved aside to assume the president of products role.[11]\n Under Schmidt's leadership, Google underwent a period of major growth and expansion, which included its initial public offering (IPO) on August 20, 2004. He always acted in consultation with Page and Brin when he embarked on initiatives such as the hiring of an executive team and the creation of a sales force management system. Page remained the boss at Google in the eyes of the employees, as he gave final approval on all new hires, and it was Page who provided the signature for the IPO, the latter making him a billionaire at the age of 30.[11]\n Page led the acquisition of Android for $50 million in 2005 to fulfill his ambition to place handheld computers in the possession of consumers so that they could access Google anywhere. The purchase was made without Schmidt's knowledge, but the CEO was not perturbed by the relatively small acquisition. Page became passionate about Android and spent large amounts of time with Android CEO and cofounder Andy Rubin. By September 2008, T-Mobile launched the G1, the first phone using Android software and, by 2010, 17.2% of the handset market consisted of Android sales, overtaking Apple for the first time. Android became the world's most popular mobile operating system shortly afterward.[11]\n Following a January 2011 announcement,[57] Page officially became the chief executive of Google on April 4, 2011, while Schmidt stepped down to become executive chairman.[58] By this time, Google had over $180 billion market capitalization and more than 24,000 employees.[59] Reporter Max Nisen described the decade prior to Page's second appointment as Google's CEO as Page's \"lost decade\" saying that while he exerted significant influence at Google via product development and other operations, he became increasingly disconnected and less responsive over time.[11][56]\n Schmidt announced the end of his tenure as CEO on January 20, 2011, jokingly tweeting on Twitter: \"Adult-supervision no longer needed.\"[60]\n As Google's new CEO, Page's two key goals were the development of greater autonomy for the executives overseeing the most important divisions, and higher levels of collaboration, communication, and unity among the teams. Then Page also formed what the media called the \"L-Team\", a group of senior vice-presidents who reported directly to him and worked near his office for a portion of the working week.[61] Additionally, he reorganized the company's senior management, placing a CEO-like manager at the top of Google's most important product divisions, including YouTube, AdWords, and Google Search.[11]\n Following a more cohesive team environment, Page declared a new \"zero tolerance for fighting\" policy that contrasted with his approach during the early days of Google, when he would use his harsh and intense arguments with Brin as an exemplar for senior management. Page had changed his thinking during his time away from the CEO role, as he eventually concluded that ambitious goals required a harmonious team dynamic. As part of Page's collaborative rejuvenation process, Google's products and applications were consolidated and underwent an aesthetic overhaul.[56][62]\n At least 70 of Google's products, features and services were eventually shut down by March 2013, while the appearance and nature of the remaining ones were unified.[63][64] Jon Wiley, lead designer of Google Search at the time, codenamed Page's redesign overhaul, which officially commenced on April 4, 2011, \"Project Kennedy\", based on Page's use of the term \"moonshots\" to describe ambitious projects in a January 2013 Wired interview.[62][65] An initiative named \"Kanna\" previously attempted to create a uniform design aesthetic for Google's range of products, but it was too difficult at that point in the company's history for one team to drive such change. Matias Duarte, senior director of the Android user experience when \"Kennedy\" started, explained in 2013 that \"Google passionately cares about design.\" Page proceeded to consult with the Google Creative Lab design team, based in New York City, to find an answer to his question of what a \"cohesive vision\" of Google might look like.[62]\n The eventual results of \"Kennedy\" which were progressively rolled out from June 2011 until January 2013, were described by The Verge technology publication as focused upon \"refinement, white space, cleanliness, elasticity, usefulness, and most of all simplicity.\" The final products were aligned with Page's aim for a consistent suite of products that can \"move fast\", and \"Kennedy\" was called a \"design revolution\" by Duarte. Page's \"UXA\" (user/graphics interface) design team then emerged from the \"Kennedy\" project, tasked with \"designing and developing a true UI framework that transforms Google's application software into a beautiful, mature, accessible and consistent platform for its users.\" Unspoken of in public, the small UXA unit was designed to ensure that \"Kennedy\" became an \"institution.\"[62]\n When acquiring products and companies for Google, Page asked whether the business acquisition passed the toothbrush test as an initial qualifier, asking the question \"Is it something you will use once or twice a day, and does it make your life better?\". This approach looked for usefulness above profitability, and long-term potential over near-term financial gain, which has been noted as rare in business acquiring processes.[66][67][68]\n With Facebook's influence rapidly expanding during the start of Page's second tenure, he finally responded to the intensive competition with Google's own social network, Google+, in mid-2011. After several delays, the social network was released through a very limited field test and was led by Vic Gundotra, Google's then senior vice president of social.[69]\n In August 2011, Page announced that Google would spend $12.5 billion to acquire Motorola Mobility.[70] The purchase was primarily motivated by Google's need to secure patents to protect Android from lawsuits by companies including Apple Inc.[11] Page wrote on Google's official blog on August 15, 2011, that \"companies including Microsoft and Apple are banding together in anti-competitive patent attacks on Android. The United States Department of Justice had to intervene in the results of one recent patent auction to 'protect competition and innovation in the open source software community' [...] Our acquisition of Motorola will increase competition by strengthening Google's patent portfolio, which will enable us to better protect Android from anti-competitive threats from Microsoft, Apple and other companies\".[71][72] In 2014, Page sold Motorola Mobility for $2.9 billion to Personal Computer maker, Lenovo which represented a loss in value of $9.5 billion over two years.[73]\n Page also ventured into hardware and Google unveiled the Chromebook in May 2012. The hardware product was a laptop that ran on a Google operating system, ChromeOS.[74]\n \nIn January 2013, Page participated in a rare interview with Wired, in which writer Steven Levy discussed Page's \"10X\" mentality—Google employees are expected to create products and services that are at least 10 times better than those of its competitors—in the introductory blurb. Astro Teller, the head of Google X, explained to Levy that 10X is \"just core to who he [Page] is\", while Page's \"focus is on where the next 10X will come from.\"[65] In his interview with Levy, Page referred to the success of YouTube and Android as examples of \"crazy\" ideas that investors were not initially interested in, saying: \"If you're not doing some things that are crazy, then you're doing the wrong things.\"[65] Page also stated he was \"very happy\" with the status of Google+, and discussed concerns over the Internet concerning the SOPA bill and an International Telecommunication Union proposal that had been recently introduced: \"I do think the Internet's under much greater attack than it has been in the past. Governments are now afraid of the Internet because of the Middle East stuff, and so they're a little more willing to listen to what I see as a lot of commercial interests that just want to make money by restricting people's freedoms. But they've also seen a tremendous user reaction, like the backlash against SOPA. I think that governments fight users' freedoms at their peril.\"[65] At the May 2013 I/O developers conference in San Francisco, Page delivered a keynote address and said \"We're at maybe 1% of what is possible. Despite the faster change, we're still moving slow relative to the opportunities we have. I think a lot of that is because of the negativity [...] Every story I read is Google vs someone else. That's boring. We should be focusing on building the things that don't exist\" and that he was \"sad the Web isn't advancing as fast as it should be\", citing a perceived focus on negativity and zero-sum games among some in the technology sector as a cause.[75] In response to an audience question, Page noted an issue that Google had been experiencing with Microsoft, whereby the latter made its Outlook program interoperable with Google but did not allow for backward compatibility—he referred to Microsoft's practice as \"milking off\". During the question-and-answer section of his keynote, Page expressed interest in Burning Man, which Brin had previously praised—it was a motivating factor for the latter during Schmidt's hiring process, as Brin liked that Schmidt had attended the week-long annual event.[11][76][75]\n In September 2013, Page launched the independent Calico initiative, a R&D project in the field of biotechnology. Google announced that Calico seeks to innovate and make improvements in the field of human health, and appointed Art Levinson, chairman of Apple's board and former CEO of Genentech, to be the new division's CEO. Page's official statement read: \"Illness and aging affect all our families. With some longer term, moonshot thinking around healthcare and biotechnology, I believe we can improve millions of lives.\"[77]\n Page participated in a March 2014 TedX conference that was held in Vancouver, British Columbia, Canada. The presentation was scripted by Page's chief PR executive Rachel Whetstone, and Google's CMO Lorraine Twohill, and a demonstration of an artificially intelligent computer program was displayed on a large screen.[11]\n \nPage responded to a question about corporations, noting that corporations largely get a \"bad rap\", which he stated was because they were probably doing the same incremental things they were doing \"50 or 20 years ago\". He went on to juxtapose that kind of incremental approach to his vision of Google counteracting calcification through driving technology innovation at a high rate. Page mentioned Elon Musk and SpaceX: \"He [Musk] wants to go to Mars to back up humanity. That's a worthy goal. We have a lot of employees at Google who've become pretty wealthy. You're working because you want to change the world and make it better [...] I'd like for us to help out more than we are.\"[78] Page also mentioned Nikola Tesla with regard to invention and commercialization: \"Invention is not enough. [Nikola] Tesla invented the electric power we use, but he struggled to get it out to people. [You have to] combine both things []... invention and innovation focus, plus [...] a company that can really commercialize things and get them to people.\"[79] Page announced a major management restructure in October 2014 so that he would no longer need to be responsible for day-to-day product-related decision making. In a memo, Page said that Google's core businesses would be able to progress in a typical manner, while he could focus on the next generation of ambitious projects, including Google X initiatives; access and energy, including Google Fiber; smart-home automation through Nest Labs; and biotechnology innovations under Calico.[80] Page maintained that he would continue as the unofficial \"chief product officer\".[64] Subsequent to the announcement, the executives in charge of Google's core products reported to then Google Senior Vice President Sundar Pichai, who reported directly to Page.[80][81][82][83]\n In a November 2014 interview, Page stated that he prioritized the maintenance of his \"deep knowledge\" of Google's products and breadth of projects, as it had been a key motivating factor for team members. About his then role as the company's CEO, Page said: \"I think my job as CEO—I feel like it's always to be pushing people ahead.\"[64]\n \nOn August 10, 2015, Page announced on Google's official blog that Google had restructured into a number of subsidiaries of a new holding company known as Alphabet Inc with Page becoming CEO of Alphabet Inc and Sundar Pichai assuming the position of CEO of Google Inc. In his announcement, Page described the planned holding company as follows:[84] \"Alphabet is mostly a collection of companies. The largest of which, of course, is Google. This newer Google is a bit slimmed down, with the companies that are pretty far afield of our main Internet products contained in Alphabet instead. [...] Fundamentally, we believe this allows us more management scale, as we can run things independently that aren't very related.\" As well as explaining the origin of the company's name: \"We liked the name Alphabet because it means a collection of letters that represent language, one of humanity's most important innovations, and is the core of how we index with Google search! We also like that it means alpha‑bet (Alpha is investment return above benchmark), which we strive for!\" Page wrote that the motivation behind the reorganization is to make Google \"cleaner and more accountable.\" He also wrote that there was a desire to improve \"the transparency and oversight of what we're doing\" and to allow greater control of unrelated companies previously within the Google ecosystem.[84][85][86]\n Page has not been on any press conferences since 2015 and has not presented at product launches or earnings calls since 2013. The Bloomberg Businessweek termed the reorganization into Alphabet a clever retirement plan allowing Page to retain control over Google, at the same time relinquishing all responsibilities over it. Executives at Alphabet describe Page as a \"futurist\", highly detached from day-to-day business dealings, and more focused on moon-shot projects. While some managers of Alphabet companies speak of Page as intensely involved, others say that his rare office check-ins are \"akin to a royal visit\".[87]\n On December 3, 2019, Larry Page announced that he would step down from the position of Alphabet CEO and be replaced by Google CEO Sundar Pichai. Pichai also continued as Google CEO. Page and Google co-founder and Alphabet president Sergey Brin announced the change in a joint blog post, \"With Alphabet now well-established, and Google and the Other Bets operating effectively as independent companies, it's the natural time to simplify our management structure. We’ve never been ones to hold on to management roles when we think there's a better way to run the company. And Alphabet and Google no longer need two CEOs and a President.\"[88]\n Page is an investor in Tesla Motors co-founded by friend and fellow billionaire Elon Musk.[89] He has invested in renewable energy technology, and with the help of Google.org, Google's philanthropic arm, promotes the adoption of plug-in hybrid electric cars[90][91][92][93] and other alternative energy investments.[94] He also was a strategic backer in the Opener and Kitty Hawk[10] startups, developing aerial vehicles for consumer travel.[95] The company has ceased all activities. It was merged into the Wisk Aero joint venture with Boeing in September 2022.\n Page is interested in the socio-economic effects of advanced intelligent systems and how advanced digital technologies can be used to create abundance (as described in Peter Diamandis' book), provide for people's needs, shorten the workweek, and mitigate the potential detrimental effects of technological unemployment.[96][97]\n Page helped to set up Singularity University, a transhumanist think-tank.[98]\n In the early 2000s, Page briefly dated Marissa Mayer, American business leader and former CEO of Yahoo!, who was a Google employee at that time.[99][100]\n On February 18, 2005, Page bought a 9,000 square feet (840 m2) Spanish Colonial Revival architecture house in Palo Alto, California, designed by American artistic polymath Pedro Joseph de Lemos, a former curator of the Stanford Art Museum and founder of the Carmel Art Institute, after the historic building had been on the market for years with an asking price of US$7.95 million. A two-story stucco archway spans the driveway and the home features intricate stucco work, as well as stone and tile in California Arts and Crafts movement style built to resemble de Lemos's family's castle in Spain. The Pedro de Lemos House was constructed between 1931 and 1941 by de Lemos.[101][102][103][104][105] It is also on the National Register of Historic Places.[106]\n \nIn 2007, Page married Lucinda Southworth on Necker Island, the Caribbean island owned by Richard Branson.[107] Southworth is a research scientist and the sister of American actress and model Carrie Southworth.[108] Page and Southworth have two children, born in 2009 and 2011 respectively.[109][110] In 2009, Page began purchasing properties and tearing down homes adjacent to his home in Palo Alto to make room for a large ecohouse. The existing buildings were \"deconstructed\" and the materials donated for reuse. The ecohouse was designed to \"minimize the impact on the environment.\" Page worked with an arborist to replace some trees that were in poor health with others that used less water to maintain. Page also applied for Green Point Certification, with points given for use of recycled and low or no-VOC (volatile organic compound) materials and for a roof garden with solar panels. The house's exterior features zinc cladding and plenty of windows, including a wall of sliding-glass doors in the rear. It includes eco-friendly elements such as permeable paving in the parking court and a pervious path through the trees on the property. The 6,000 square feet (560 m2) house also observes other green home design features such as organic architecture building materials and low volatile organic compound paint.[111][112][113][114]\n In 2011, Page bought the $45-million 193-foot (59 m) superyacht Senses.[115] Later on, Page announced on his Google+ profile in May 2013 that his right vocal cord is paralyzed from a cold that he contracted the previous summer, while his left cord was paralyzed in 1999, and that the doctors were unable to identify the exact cause.[116] The Google+ post also revealed that Page had made a large donation to a vocal-cord nerve-function research program at the Voice Health Institute in Boston. An anonymous source stated that the donation exceeded $20 million.[117] In October 2013, Business Insider reported that Page's paralysis were caused by an autoimmune disease called Hashimoto's thyroiditis, and prevented him from undertaking Google quarterly earnings conference calls for an indefinite period.[118]\n In November 2014, Page's family foundation, the Carl Victor Page Memorial Fund, reportedly holding assets in excess of a billion dollars at the end of 2013, gave $15 million to aid the effort against the Ebola virus epidemic in West Africa. Page wrote on his Google+ page that \"My wife and I just donated $15 million [...] Our hearts go out to everyone affected.\"[119][120][121][122]\n In August 2021 it was revealed that Page holds a New Zealand resident's visa and had traveled to the country on a medivac flight from Fiji for his son's treatment in New Zealand. The flight took place on January 12, 2021. Page had been living in Fiji with his family during the duration of the COVID-19 pandemic.[123]\n In 2023, the US Virgin Islands tried several times to serve Page a subpoena in the lawsuit over JPMorgan Chase's links to Jeffrey Epstein.[124][125]\n Page has purchased multiple private islands across the Caribbean and South Pacific, including the Hans Lollik Island in 2014, Eustatia Island, Cayo Norte in 2018, and Tavarua in 2020.[126][127]\n A fictionalized version of Larry Page portrayed by actor Ben Feldman appeared in the Showtime drama series Super Pumped.[145]"},
{"url": "https://en.wikipedia.org/wiki/Sergey_Brin", "title": "Sergey Brin - Wikipedia", "content": "This is an accepted version of this page \n Sergey Mikhailovich Brin (Russian: Сергей Михайлович Брин; born August 21, 1973) is an American computer scientist and businessman who co-founded Google with Larry Page. He was the president of Google's parent company, Alphabet Inc., until stepping down from the role on December 3, 2019.[1] He and Page remain at Alphabet as co-founders, controlling shareholders and board members. As of December 2024, Brin is the 7th-richest person in the world, with an estimated net worth of $164 billion, according to the Bloomberg Billionaires Index and Forbes.[2][3]\n Brin immigrated to the United States from the Soviet Union at the age of six. He earned his bachelor's degree at the University of Maryland, College Park, following in his father's and grandfather's [4][5] footsteps by studying mathematics as well as computer science. After graduation, in September 1993, he enrolled in Stanford University to acquire a PhD in computer science. There he met Page, with whom he built a web search engine. The program became popular at Stanford, and he discontinued his PhD studies to start Google in Susan Wojcicki's garage in Menlo Park.[6]\n Sergey Mikhailovich Brin was born on August 21, 1973, in Moscow in the Soviet Union,[7] to Russian Jewish parents,[8] Mikhail and Eugenia Brin, both graduates of Moscow State University (MSU).[9] His father is a retired mathematics professor at the University of Maryland, and his mother is a researcher at NASA's Goddard Space Flight Center.[10][11]\n The Brin family lived in a three-room apartment in central Moscow, which they also shared with Sergey's paternal grandmother.[10] In 1977, after his father returned from a mathematics conference in Warsaw, Poland, Mikhail Brin announced that it was time for the family to emigrate.[10] They formally applied for their exit visa in September 1978, and as a result, his father was \"promptly fired\". For related reasons, his mother had to leave her job. For the next eight months, without any steady income, they were forced to take on temporary jobs as they waited, afraid their request would be denied as it was for many refuseniks. In May 1979, they were granted their official exit visas and were allowed to leave the country.[10]\n The Brin family lived in Vienna and Paris while Mikhail Brin secured a teaching position at the University of Maryland with help from Anatole Katok. During this time, the Brin family received support and assistance from the Hebrew Immigrant Aid Society. They arrived in the United States on October 25, 1979.[10][12]\n Brin attended elementary school at Paint Branch Montessori School in Adelphi, Maryland, but he received further education at home; his father, a professor in the department of mathematics at the University of Maryland, encouraged him to learn mathematics and his family helped him retain his Russian-language skills. He attended Eleanor Roosevelt High School, Maryland. In September 1990, Brin enrolled in the University of Maryland, where he received his Bachelor of Science from the Department of Computer Science in 1993 with honors in computer science and mathematics at the age of 19.[13] In 1993, he interned at Wolfram Research, the developers of Mathematica.[13]\n Brin began his graduate study in computer science at Stanford University on a graduate fellowship from the National Science Foundation, receiving an M.S. in computer science in 1995.[14] As of 2008[update], he was on leave from his PhD studies at Stanford.[15]\n During an orientation for new students at Stanford, he met Larry Page. The two men seemed to disagree on most subjects, but after spending time together they \"became intellectual soul-mates and close friends.\" Brin's focus was on developing data mining systems while Page's was on extending \"the concept of inferring the importance of a research paper from its citations in other papers\".[16] Together, they authored a paper titled \"The Anatomy of a Large-Scale Hypertextual Web Search Engine\".[17]\n To convert the backlink data gathered by BackRub's web crawler into a measure of importance for a given web page, Brin and Page developed the PageRank algorithm, and realized that it could be used to build a search engine far superior to those existing at the time.[18] The new algorithm relied on a new kind of technology that analyzed the relevance of the backlinks that connected one Web page to another, and allowed the number of links and their rank, to determine the rank of the page.[19] Combining their ideas, they began utilizing Page's dormitory room as a machine laboratory, and extracted spare parts from inexpensive computers to create a device that they used to connect the nascent search engine with Stanford's broadband campus network.[18]\n After filling Page's room with equipment, they then converted Brin's dorm room into an office and programming center, where they tested their new search engine designs on the web. The rapid growth of their project caused Stanford's computing infrastructure to experience problems.[20]\n Page and Brin used Page's basic HTML programming skills to set up a simple search page for users, as they did not have a web page developer to create anything visually elaborate. They also began using any computer part they could find to assemble the necessary computing power to handle searches by multiple users. As their search engine grew in popularity among Stanford users, it required additional servers to process the queries. In August 1996, the initial version of Google was made available on the Stanford Web site.[18]\n By early 1997, the BackRub page described the state as follows:\n BackRub already exhibited the rudimentary functions and characteristics of a search engine: a query input was entered and it provided a list of backlinks ranked by importance. Page recalled: \"We realized that we had a querying tool. It gave you a good overall ranking of pages and ordering of follow-up pages.\"[22] Page said that in mid-1998 they finally realized the further potential of their project: \"Pretty soon, we had 10,000 searches a day. And we figured, maybe this is really real.\"[20]\n Some compared Page and Brin's vision to the impact of Johannes Gutenberg, the inventor of modern printing:\n In 1440, Johannes Gutenberg introduced Europe to the mechanical printing press, printing Bibles for mass consumption. The technology allowed for books and manuscripts‍—‌originally replicated by hand‍—‌to be printed at a much faster rate, thus spreading knowledge and helping to usher in the European Renaissance ... Google has done a similar job.[23] The comparison was also noted by the authors of The Google Story: \"Not since Gutenberg ... has any new invention empowered individuals, and transformed access to information, as profoundly as Google.\"[24] Also, not long after the two \"cooked up their new engine for web searches, they began thinking about information that was at the time beyond the web,\" such as digitizing books and expanding health information.[20]\n In June 2008, Brin invested $4.5 million in Space Adventures, a Virginia-based space tourism company.[25]\n Brin and Page jointly own a customized Boeing 767–200 and a Dassault/Dornier Alpha Jet,[26] and pay $1.3 million a year to house them and two Gulfstream V jets owned by Google executives at Moffett Federal Airfield. The aircraft has scientific equipment installed by NASA to allow experimental data to be collected in flight.[27][28]\n Brin is a backer of LTA Research & Exploration LLC, an airship maker company.[29] In October 2023, LTA's 124-meter long flagship, Pathfinder 1, became the largest airship since the Hindenburg to receive clearance for flight testing, permitted over the boundaries of Moffett Field and neighboring Palo Alto Airport’s airspaces, at a height of up to 460 meters.[30]\n Brin was raised Jewish, but is not religious.[31][better source needed]\n In May 2007, Brin married biotech analyst and entrepreneur Anne Wojcicki in the Bahamas.[32][33] They had a son in late 2008 and a daughter in late 2011.[34] In August 2013, it was announced that Brin and his wife were living separately after Brin had an extramarital affair with a Google Glass colleague.[35][36][37] In June 2015, Brin and Wojcicki finalized their divorce.[38]\n On November 7, 2018, he married Nicole Shanahan, a legal tech founder.[39] They have a daughter, born in late 2018.[40] Brin and Shanahan separated on December 15, 2021, and Brin filed for divorce on January 4, 2022.[39] In summer 2023, the divorce was finalized.[41] The Wall Street Journal reported that a reason for the breakup was a \"brief affair\" in 2021 between Shanahan and Elon Musk.[42]\n Brin's mother, Eugenia, has been diagnosed with Parkinson's disease. In 2008, he decided to make a donation to the University of Maryland School of Medicine, where his mother has received treatment.[43] According to Forbes, Brin has donated over $1 billion to fund research on the disease.[44]\n Brin and Wojcicki, although separated, jointly ran The Brin Wojcicki Foundation until 2014. Since then, Brin has used the Sergey Brin Family Foundation and a donor-advised fund for his philanthropic giving.[45] They donated extensively to The Michael J. Fox Foundation and in 2009 gave $1 million to support the Hebrew Immigrant Aid Society.[12]\n Brin is a donor to Democratic Party candidates and organizations, having donated $5,000 to Barack Obama's reelection campaign and $30,800 to the DNC.[46] Brin attended the second inauguration of Donald Trump, sitting alongside Donald Trump supporters and other tech moguls.[47][48]\n he was a featured speaker at the World Economic Forum and the Technology, Entertainment and Design Conference. ... PC Magazine has praised Google in the Top 100 Web Sites and Search Engines (1998) and awarded Google the Technical Excellence Award, for Innovation in Web Application Development in 1999. In 2000, Google earned a Webby Award, a People's Voice Award for technical achievement, and in 2001, was awarded Outstanding Search Service, Best Image Search Engine, Best Design, Most Webmaster Friendly Search Engine, and Best Search Feature at the Search Engine Watch Awards.[58]"},
{"url": "https://en.wikipedia.org/wiki/Yahoo!_Search", "title": "Yahoo Search - Wikipedia", "content": "Yahoo! Search is a search engine owned and operated by Yahoo!, using Microsoft Bing to power results.\n Originally, \"Yahoo! Search\" referred to a Yahoo!-provided interface that sent queries to a searchable index of pages supplemented with its directory of websites. The results were presented to the user under the Yahoo! brand. The actual web crawling and data housing was not done by Yahoo! itself –  in 2001, the searchable index was powered by Inktomi and later by Google until 2004, when Yahoo! built its own crawler, becoming independent.\n On July 29, 2009, Microsoft and Yahoo! announced a deal in which Bing would henceforth power Yahoo! Search, putting an end to Yahoo!'s in-house crawler.[2] For four years between 2015 until the end of 2018, it was powered by Google,[3] before returning to Microsoft Bing again.\n As of July 2018, Microsoft Sites handled 24.2 percent of all desktop search queries in the United States. During the same period of time, Oath (the then-owner of the Yahoo brand) had a search market share of 11.5 percent. Market leader Google generated 63.2 percent of all core search queries in the United States.[4]\n The roots of Search date back to Yahoo! Directory, which was launched in 1994 by Jerry Yang and David Filo, then students at Stanford University. In 1995, they introduced a search engine function, called Yahoo! Search, that allowed users to search Yahoo! Directory.[5][6] it was the first popular search engine on the Web,[7] despite not being a true Web crawler search engine. They later licensed Web search engines from other companies. Seeking to provide its own Web search engine results, Yahoo! acquired their own Web search technology. In 2002, they bought Inktomi, a \"behind the scenes\" or OEM search engine provider, whose results are shown on other companies' websites and powered Yahoo! in its earlier days.\n In 2003, they purchased Overture Services, Inc., which included the AlltheWeb and AltaVista search engines. Initially, even though Yahoo! owned multiple search engines, they didn't use them on the main yahoo.com website, but kept using Google's search engine for its results.\n Starting on April 7, 2003, Yahoo! Search became its own web crawler-based search engine.[8] They combined the capabilities of search engine companies they had acquired and their prior research into a reinvented crawler called Yahoo!. The new search engine results were included in all of Yahoo's websites that had a web search function. Yahoo! also started to sell the search engine results to other companies, to show on their own websites. Their relationship with Google was terminated at that time, with the former partners becoming each other's main competitors.\n In October 2007, Yahoo! Search was updated with a more modern appearance in line with the redesigned Yahoo! home page. In addition, Search Assist was added; which provides real-time query suggestions and related concepts as they are typed.\n In July 2008, Yahoo! Search announced the introduction of a new service called Yahoo! Search BOSS (\"Build your Own Search Service\"). This service opens the doors for developers to use Yahoo!'s system for indexing information and images and create their own custom search engine.[9]\n In January 2010, Microsoft announced a deal in which it would take over the functional operation of Yahoo! Search, and set up a joint venture to sell advertising on both Yahoo! Search and Bing known as the Microsoft Search Alliance. A complete transition of all Yahoo! sponsored ad clients to Microsoft adCenter (now Bing Ads) occurred in October 2010.[2]\n On March 12, 2014, Yahoo! announced a partnership with Yelp to integrate its reviews and user-contributed photos into Yahoo! Search (as Bing had previously done).[10]\n In November 2014, Mozilla signed a five-year partnership with Yahoo, making Yahoo Search the default search engine for Firefox browsers in the US.[11]\n In April 2015, the Microsoft partnership was modified, now only requiring Bing results on the \"majority\" of desktop traffic, opening the ability for Yahoo to enter into non-exclusive deals for search services on mobile platforms and the remainder of desktop traffic. The amendment also gives either company the ability to terminate the contract with four months' notice.  In October 2015, Yahoo subsequently reached an agreement with Google to provide services to Yahoo Search through the end of 2018, including advertising, search, and image search services.[12][13][3] As of October 2019, Yahoo! Search is once again powered by Bing.\n In September 2021, investment funds managed by the private equity firm Apollo Global Management acquired 90% of Yahoo.[14][15]\n Yahoo! Search also provided their search interface in at least 38 international markets and a variety of available languages.[16] Yahoo! has a presence in Europe, Asia and across the Emerging Markets.\n Yahoo Image Search Results in 24 languages\n Yahoo Search indexed and cached the common HTML page formats, as well as several of the more popular file-types, such as PDF, Excel spreadsheets, PowerPoint presentations, Word documents, RSS/XML and plain text files. For some of these supported file-types, Yahoo! Search provided cached links on their search results allowing for viewing of these file-types in standard HTML. Using the Advanced Search interface or Preferences settings, Yahoo Search allowed the customization of search results and enabling of certain settings such as: SafeSearch, Language Selection, Number of results, Domain restrictions, etc.[17] For a Basic and starter guide to Yahoo Search, they also provided a Search Basics tutorial.[18] In 2005, Yahoo began to provide links to previous versions of pages archived on the Wayback Machine.[19] In the first week of May 2008, Yahoo launched a new search paradigm called Yahoo Glue.[20][21]\n Yahoo! Search was criticized in 2020 for favoring websites owned by Yahoo!'s then-parent company, Verizon Media, in its search results.[22]\n On June 20, 2007, Yahoo introduced a selection-based search feature called Yahoo Shortcuts. When activated this selection-based search feature enabled users to invoke search using only their mouse and receive search suggestions in floating windows while remaining on Yahoo properties such as Yahoo Mail.  This feature was only active on Yahoo web pages or pages within the Yahoo Publisher Network.  Yahoo Shortcuts required the content-owner to modify the underlying HTML of his or her webpage to call out the specific keywords to be enhanced.  The technology for context-aware selection-based search on Yahoo pages was first developed by Reiner Kraft.[23]\n On May 11, 2008, Yahoo introduced SearchScan. If enabled this add-on/feature enhanced Yahoo Search by automatically alerting users of viruses, spyware and spam websites.[24]\n Yahoo Search provided the ability to search across numerous vertical properties outside just the Web at large. These included Images, Videos, Local, Shopping, Yahoo! Answers, Audio, Directory, Jobs, News, Mobile, Travel and various other services as listed on their About Yahoo Search page.\n OneSearch\n Yahoo introduced its Internet search system, called OneSearch,[25] for mobile phones on March 20, 2007. The results include news headlines, images from Flickr, business listings, local weather and links to other sites. Instead of showing only, for example, popular movies or some critical reviews, OneSearch lists local theaters that at the moment are playing the movie, along with user ratings and news headlines regarding the movie. A zip code or city name is required for OneSearch to start delivering local search results.[26]\n The results of a Web search are listed on a single page and are prioritized into categories.[27]\n As of 2012, Yahoo used Novarra's mobile content transcoding service for OneSearch.[28]\n OneSearch\n On January 14, 2020, Verizon announced the launch of its privacy-focused search engine OneSearch.[29][30][31]\n OneSearch was criticized for favoring websites owned by Yahoo!'s then-parent company, Verizon Media, in its search results.[22]\n \"EFF Privacy Badger (a browser extension that detects cookies) discovered a tracker connected to Yahoo’s image search engine...Verizon owns Yahoo... OneSearch actually pulls all of its search results from Microsoft’s Bing search engine. \"[32][33]\n Verizon was fined $1.3 million for using super cookie trackers.[34]\n www.onesearch.com is excluded from the Wayback Machine.[35][36]"},
{"url": "https://en.wikipedia.org/wiki/Distributed_web_crawler", "title": "Distributed web crawling - Wikipedia", "content": "Distributed web crawling is a distributed computing technique whereby Internet search engines employ many computers to index the Internet via web crawling. Such systems may allow for users to voluntarily offer their own computing and bandwidth resources towards crawling web pages. By spreading the load of these tasks across  many computers, costs that would otherwise be spent on maintaining large computing clusters are avoided.\n Cho[1] and Garcia-Molina studied two types of policies:\n With this type of policy, a central server assigns new URLs to different crawlers dynamically. This allows the central server to, for instance, dynamically balance the load of each crawler.[2]\n With dynamic assignment, typically the systems can also add or remove downloader processes. The central server may become the bottleneck, so most of the workload must be transferred to the distributed crawling processes for large crawls.\n There are two configurations of crawling architectures with dynamic assignments that have been described by Shkapenyuk and Suel:[3]\n With this type of policy, there is a fixed rule stated from the beginning of the crawl that defines how to assign new URLs to the crawlers.\n For static assignment, a hashing function can be used to transform URLs (or, even better, complete website names) into a number that corresponds to the index of the corresponding crawling process.[4] As there are external links that will go from a Web site assigned to one crawling process to a website assigned to a different crawling process, some exchange of URLs must occur.\n To reduce the overhead due to the exchange of URLs between crawling processes, the exchange should be done in batch, several URLs at a time, and the most cited URLs in the collection should be known by all crawling processes before the crawl (e.g.: using data from a previous crawl).[1]\n As of 2003, most modern commercial search engines use this technique. Google and Yahoo use thousands of individual computers to crawl the Web.\n Newer projects are attempting to use a less structured, more ad hoc form of collaboration by enlisting volunteers to join the effort using, in many cases, their home or personal computers. LookSmart is the largest search engine to use this technique, which powers its Grub distributed web-crawling project. Wikia (now known as Fandom) acquired Grub from LookSmart in 2007.[5]\n This solution uses computers that are connected to the Internet to crawl Internet addresses in the background. Upon downloading crawled web pages, they are compressed and sent back, together with a status flag (e.g. changed, new, down, redirected) to the powerful central servers. The servers, which manage a large database, send out new URLs to clients for testing.\n According to the FAQ about Nutch, an open-source search engine website, the savings in bandwidth by distributed web crawling are not significant, since \"A successful search engine requires more bandwidth to upload query result pages than its crawler needs to download pages...\".[6]"},
{"url": "https://en.wikipedia.org/wiki/Subject-oriented_programming", "title": "Subject-oriented programming - Wikipedia", "content": "In computing, subject-oriented programming is an object-oriented software paradigm in which the state (fields) and behavior (methods) of objects are not seen as intrinsic to the objects themselves, but are provided by various subjective perceptions (\"subjects\") of the objects. The term and concepts were first published in September 1993 in a conference paper[1] which was later recognized as being one of the three most influential papers to be presented at the conference between 1986 and 1996.[2] As illustrated in that paper, an analogy is made with the contrast between the philosophical views of Plato and Kant with respect to the characteristics of \"real\" objects, but applied to software ones. For example, while we may all perceive a tree as having a measurable height, weight, leaf-mass, etc., from the point of view of a bird, a tree may also have measures of relative value for food or nesting purposes, or from the point of view of a tax-assessor, it may have a certain taxable value in a given year. Neither the bird's nor the tax-assessor's additional state information need be seen as intrinsic to the tree, but are added by the perceptions of the bird and tax-assessor, and from Kant's analysis, the same may be true even of characteristics we think of as intrinsic.\n Subject-oriented programming advocates the organization of the classes that describe objects into \"subjects\", which may be composed to form larger subjects. At points of access to fields or methods, several subjects' contributions may be composed. These points were characterized as the join-points[3] of the subjects. For example, if a tree is cut down, the methods involved may need to join behavior in the bird and tax-assessor's subjects with that of the tree's own. It is therefore fundamentally a view of the compositional nature of software development, as opposed to the algorithmic (procedural) or representation-hiding (object) nature.\n The introduction of aspect-oriented programming in 1997[4] raised questions about its relationship to subject-oriented programming, and about the difference between subjects and aspects. These questions were unanswered for some time, but were addressed in the patent on Aspect-oriented programming filed in 1999[5] in which two points emerge as characteristic differences from earlier art:\n In the subject-oriented view, the cross-cut may be placed separately from the aspect (subject) and the behavior is not forced by the aspect, but governed by rules of composition. Hindsight[6] makes it also possible to distinguish aspect-oriented programming by its introduction and exploitation of the concept of a query-like pointcut to externally impose the join-points used by aspects in general ways.\n In the presentation of subject-oriented programming, the join-points were deliberately restricted to field access and method call on the grounds that those were the points at which well-designed frameworks were designed to admit functional extension. The use of externally imposed pointcuts is an important linguistic capability, but remains one of the most controversial features of aspect-oriented programming.[7]\n By the turn of the millennium, it was clear that a number of research groups were pursuing different technologies that employed the composition or attachment of separately packaged state and function to form objects.[8] To distinguish the common field of interest from Aspect-Oriented Programming with its particular patent definitions and to emphasize that the compositional technology deals with more than just the coding phase of software development, these technologies were organized together under the term Aspect-Oriented Software Development,[9] and an organization and series on international conferences begun on the subject. Like aspect-oriented programming, subject-oriented programming, composition filters, feature-oriented programming and adaptive methods are considered to be aspect-oriented software development approaches.\n The original formulation of subject-oriented programming deliberately envisioned it as a packaging technology – allowing the space of functions and data types to be extended in either dimension.  The first implementations had been for C++,[10] and Smalltalk.[11] These implementations exploited the concepts of software labels and composition rules to describe the joining of subjects.\n To address the concern that a better foundation should be provided for the analysis and composition of software not just in terms of its packaging but in terms of the various concerns these packages addressed,  an explicit organization of the material was developed in terms of a multi-dimensional \"matrix\" in which concerns are related to the software units that implement them. This organization is called multi-dimensional separation of concerns, and the paper describing it[12] has been recognized as the most influential paper of the ICSE 1999 Conference.[13]\n This new concept was implemented for composing Java software, using the name Hyper/J for the tool.[14]\n Composition and the concept of subject can be applied to software artifacts that have no executable semantics, like requirement specifications or documentation. A research vehicle for Eclipse, called the Concern Manipulation Environment (CME), has been described[15] in which tools for query, analysis, modelling,[16] and composition are applied to artifacts in any language or representation, through the use of appropriate plug-in adapters to manipulate the representation.\n A successor to the Hyper/J composition engine[17] was developed as part of CME which uses a general approach for the several elements of a composition engine:\n Both Hyper/J and CME are available, from alphaWorks[18] or sourceforge,[19] respectively, but neither is actively supported.\n Method dispatch in object oriented programming can be thought of as \"two dimensional\" in the sense that the code executed depends on both the method name and the object in question. This can be contrasted[20] with procedural programming, where a procedure name resolves directly, or one dimensionally, onto a subroutine, and also to subject oriented programming, where the sender or subject is also relevant to dispatch, constituting a third dimension."}
]